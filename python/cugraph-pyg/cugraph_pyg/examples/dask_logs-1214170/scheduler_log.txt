RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/python/cugraph-pyg/cugraph_pyg/examples/scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-04-04 04:39:21,011 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-04 04:39:21,684 - distributed.scheduler - INFO - State start
2023-04-04 04:39:21,698 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-04 04:39:21,699 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-04-04 04:39:21,699 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-04-04 04:39:30,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43253', status: init, memory: 0, processing: 0>
2023-04-04 04:39:30,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43253
2023-04-04 04:39:30,832 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46964
2023-04-04 04:39:30,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:46295', status: init, memory: 0, processing: 0>
2023-04-04 04:39:30,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:46295
2023-04-04 04:39:30,833 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:46980
2023-04-04 04:42:23,058 - distributed._signals - INFO - Received signal SIGINT (2)
2023-04-04 04:42:23,058 - distributed.scheduler - INFO - Scheduler closing...
2023-04-04 04:42:23,059 - distributed.scheduler - INFO - Scheduler closing all comms
2023-04-04 04:42:23,059 - distributed.core - INFO - Connection to tcp://10.120.104.11:46980 has been closed.
2023-04-04 04:42:23,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:46295', status: running, memory: 0, processing: 0>
2023-04-04 04:42:23,060 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:46295
2023-04-04 04:42:23,060 - distributed.core - INFO - Connection to tcp://10.120.104.11:46964 has been closed.
2023-04-04 04:42:23,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43253', status: running, memory: 0, processing: 0>
2023-04-04 04:42:23,060 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43253
2023-04-04 04:42:23,060 - distributed.scheduler - INFO - Lost all workers
2023-04-04 04:42:23,061 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46964>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46964>: Stream is closed
2023-04-04 04:42:23,062 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46980>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 317, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 328, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:46980>: Stream is closed
2023-04-04 04:42:23,063 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-04-04 04:42:23,063 - distributed.scheduler - INFO - End scheduler
