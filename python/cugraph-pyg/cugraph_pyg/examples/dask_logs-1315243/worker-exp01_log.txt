RUNNING: "python -m dask_cuda.cli.dask_cuda_worker --rmm-pool-size=28G
             --local-directory=/tmp/
             --scheduler-file=/root/cugraph/python/cugraph-pyg/cugraph_pyg/examples/scheduler.json
             --memory-limit=auto
             --device-memory-limit=auto
            "
2023-04-04 14:33:52,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:36601'
2023-04-04 14:33:52,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.104.11:37813'
2023-04-04 14:33:53,494 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-4ughzg6l', purging
2023-04-04 14:33:53,494 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-rge1m3y0', purging
2023-04-04 14:33:53,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-04-04 14:33:53,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-04-04 14:33:53,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-04-04 14:33:53,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-04-04 14:33:53,743 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-04-04 14:33:53,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-04-04 14:33:55,012 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:45185
2023-04-04 14:33:55,012 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:45185
2023-04-04 14:33:55,012 - distributed.worker - INFO -          dashboard at:        10.120.104.11:46319
2023-04-04 14:33:55,012 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-04-04 14:33:55,012 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,013 - distributed.worker - INFO -               Threads:                          1
2023-04-04 14:33:55,013 - distributed.worker - INFO -                Memory:                 755.28 GiB
2023-04-04 14:33:55,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w7u47ny_
2023-04-04 14:33:55,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e127262a-f235-4a5c-8d82-e94ad16d1d8d
2023-04-04 14:33:55,015 - distributed.worker - INFO -       Start worker at:  tcp://10.120.104.11:38339
2023-04-04 14:33:55,015 - distributed.worker - INFO -          Listening to:  tcp://10.120.104.11:38339
2023-04-04 14:33:55,015 - distributed.worker - INFO -          dashboard at:        10.120.104.11:45391
2023-04-04 14:33:55,015 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.104.11:8786
2023-04-04 14:33:55,015 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,015 - distributed.worker - INFO -               Threads:                          1
2023-04-04 14:33:55,015 - distributed.worker - INFO -                Memory:                 755.28 GiB
2023-04-04 14:33:55,015 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-07p1l68n
2023-04-04 14:33:55,016 - distributed.worker - INFO - Starting Worker plugin RMMSetup-175eb4e5-5371-4b5a-a7b5-dd703c021b1e
2023-04-04 14:33:55,179 - distributed.worker - INFO - Starting Worker plugin PreImport-0cfac085-4e20-4772-8974-e4b40f5a1a96
2023-04-04 14:33:55,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a5be3ff-4313-4ff4-8401-2c692713008e
2023-04-04 14:33:55,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a07c8b7-f0af-4564-b3e8-ba1776a4308e
2023-04-04 14:33:55,179 - distributed.worker - INFO - Starting Worker plugin PreImport-8fe47aa8-f17b-4e05-af6d-48114514703c
2023-04-04 14:33:55,181 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,182 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,193 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-04-04 14:33:55,193 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,194 - distributed.worker - INFO -         Registered to:   tcp://10.120.104.11:8786
2023-04-04 14:33:55,194 - distributed.worker - INFO - -------------------------------------------------
2023-04-04 14:33:55,195 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-04-04 14:33:55,196 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:8786
2023-04-04 14:35:36,875 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:38339. Reason: worker-handle-scheduler-connection-broken
2023-04-04 14:35:36,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:36601'. Reason: nanny-close
2023-04-04 14:35:36,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-04-04 14:35:36,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.120.104.11:37813'. Reason: nanny-close
2023-04-04 14:35:36,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
Process Dask Worker process (from Nanny):
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/envs/rapids/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/process.py", line 202, in _run
    target(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 990, in _run
    asyncio.run(run())
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 1906, in _run_once
    handle._run()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 687, in <lambda>
    lambda f: self._run_callback(functools.partial(callback, future))
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 927, in _run
    self._schedule_next()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 932, in _schedule_next
    self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py", line 577, in add_timeout
    if isinstance(deadline, numbers.Real):
  File "/opt/conda/envs/rapids/lib/python3.10/abc.py", line 117, in __instancecheck__
    def __instancecheck__(cls, instance):
KeyboardInterrupt
2023-04-04 14:35:37,044 - distributed.worker - INFO - Stopping worker at tcp://10.120.104.11:45185. Reason: worker-close
2023-04-04 14:35:37,152 - distributed.core - ERROR - Event loop is closed
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 946, in run
    await worker.finished()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1549, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1014, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1025, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1151, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Exception ignored in: <coroutine object WorkerProcess._run.<locals>.run at 0x7fcb9ca91770>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 553, in __aexit__
    await self.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1549, in close
    self.status = Status.closing
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1014, in status
    self._send_worker_status_change(stimulus_id)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1025, in _send_worker_status_change
    self.batched_send(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1151, in batched_send
    self.batched_stream.send(msg)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 162, in send
    self.waker.set()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/locks.py", line 222, in set
    fut.set_result(None)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 753, in call_soon
    self._check_closed()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/base_events.py", line 515, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed
Task was destroyed but it is pending!
task: <Task pending name='Task-11' coro=<Worker.handle_scheduler() running at /opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py:204> wait_for=<Future cancelled> cb=[IOLoop.add_future.<locals>.<lambda>() at /opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/ioloop.py:687, gather.<locals>._done_callback() at /opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py:720]>
2023-04-04 14:35:37,154 - distributed.worker - ERROR - <asyncio.locks.Event object at 0x7fcb9caf4910 [unset]> is bound to a different event loop
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 925, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 671, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1525, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 502, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fcb9caf4910 [unset]> is bound to a different event loop
2023-04-04 14:35:37,155 - distributed.worker - CRITICAL - Error trying close worker in response to broken internal state. Forcibly exiting worker NOW
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
GeneratorExit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 925, in handle_stream
    await comm.close()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 210, in wrapper
    future = _create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 147, in _create_future
    future = Future()  # type: Future
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/events.py", line 671, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'MainThread'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1292, in handle_scheduler
    await self.close(reason="worker-handle-scheduler-connection-broken")
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 1525, in close
    await self.finished()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/core.py", line 502, in finished
    await self._event_finished.wait()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/locks.py", line 211, in wait
    fut = self._get_loop().create_future()
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/mixins.py", line 30, in _get_loop
    raise RuntimeError(f'{self!r} is bound to a different event loop')
RuntimeError: <asyncio.locks.Event object at 0x7fcb9caf4910 [unset]> is bound to a different event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/worker.py", line 236, in _force_close
    await wait_for(
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/rapids/lib/python3.10/asyncio/tasks.py", line 405, in wait_for
    loop = events.get_running_loop()
RuntimeError: no running event loop
2023-04-04 14:35:37,283 - distributed.nanny - INFO - Worker process 1315429 exited with status 1
2023-04-04 14:35:40,083 - distributed.nanny - WARNING - Worker process still alive after 3.199983825683594 seconds, killing
2023-04-04 14:35:40,270 - distributed.nanny - INFO - Worker process 1315426 was killed by signal 9
