RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/python/cugraph-pyg/cugraph_pyg/examples/scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-04-04 17:51:43,733 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-04 17:51:44,416 - distributed.scheduler - INFO - State start
2023-04-04 17:51:44,430 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-04 17:51:44,431 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-04-04 17:51:44,431 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-04-04 17:51:53,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:32769', status: init, memory: 0, processing: 0>
2023-04-04 17:51:53,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:32769
2023-04-04 17:51:53,698 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60702
2023-04-04 17:51:53,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:43551', status: init, memory: 0, processing: 0>
2023-04-04 17:51:53,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:43551
2023-04-04 17:51:53,699 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:60704
2023-04-04 17:53:19,811 - distributed._signals - INFO - Received signal SIGINT (2)
2023-04-04 17:53:19,811 - distributed.scheduler - INFO - Scheduler closing...
2023-04-04 17:53:19,811 - distributed.core - INFO - Connection to tcp://10.120.104.11:60702 has been closed.
2023-04-04 17:53:19,812 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:32769', status: running, memory: 0, processing: 0>
2023-04-04 17:53:19,812 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:32769
2023-04-04 17:53:19,813 - distributed.scheduler - INFO - Scheduler closing all comms
2023-04-04 17:53:19,813 - distributed.core - INFO - Connection to tcp://10.120.104.11:60704 has been closed.
2023-04-04 17:53:19,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:43551', status: running, memory: 0, processing: 0>
2023-04-04 17:53:19,813 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:43551
2023-04-04 17:53:19,813 - distributed.scheduler - INFO - Lost all workers
2023-04-04 17:53:19,813 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:60704>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-04-04 17:53:19,815 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-04-04 17:53:19,816 - distributed.scheduler - INFO - End scheduler
