RUNNING: "python -m distributed.cli.dask_scheduler --protocol=tcp
                    --scheduler-file /root/cugraph/python/cugraph-pyg/cugraph_pyg/examples/scheduler.json
                "
/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead
  warnings.warn(
2023-04-05 13:40:59,021 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-05 13:40:59,709 - distributed.scheduler - INFO - State start
2023-04-05 13:40:59,723 - distributed.scheduler - INFO - -----------------------------------------------
2023-04-05 13:40:59,724 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.120.104.11:8786
2023-04-05 13:40:59,724 - distributed.scheduler - INFO -   dashboard at:  http://10.120.104.11:8787/status
2023-04-05 13:41:08,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:33907', status: init, memory: 0, processing: 0>
2023-04-05 13:41:08,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:33907
2023-04-05 13:41:08,861 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58512
2023-04-05 13:41:08,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.120.104.11:44805', status: init, memory: 0, processing: 0>
2023-04-05 13:41:08,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.120.104.11:44805
2023-04-05 13:41:08,868 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:58524
2023-04-05 13:43:38,003 - distributed.scheduler - INFO - Receive client connection: Client-de17316f-d3b7-11ed-839d-5cff35c1a711
2023-04-05 13:43:38,003 - distributed.core - INFO - Starting established connection to tcp://10.120.104.11:52830
2023-04-05 13:50:46,218 - distributed._signals - INFO - Received signal SIGINT (2)
2023-04-05 13:50:46,219 - distributed.core - INFO - Connection to tcp://10.120.104.11:58524 has been closed.
2023-04-05 13:50:46,219 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:44805', status: running, memory: 0, processing: 0>
2023-04-05 13:50:46,219 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:44805
2023-04-05 13:50:46,220 - distributed.scheduler - INFO - Scheduler closing...
2023-04-05 13:50:46,220 - distributed.core - INFO - Connection to tcp://10.120.104.11:58512 has been closed.
2023-04-05 13:50:46,220 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.120.104.11:33907', status: running, memory: 0, processing: 0>
2023-04-05 13:50:46,220 - distributed.core - INFO - Removing comms to tcp://10.120.104.11:33907
2023-04-05 13:50:46,220 - distributed.scheduler - INFO - Lost all workers
2023-04-05 13:50:46,220 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://10.120.104.11:8786 remote=tcp://10.120.104.11:58512>
Traceback (most recent call last):
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/opt/conda/envs/rapids/lib/python3.10/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-04-05 13:50:46,222 - distributed.scheduler - INFO - Scheduler closing all comms
2023-04-05 13:50:46,223 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.120.104.11:8786'
2023-04-05 13:50:46,223 - distributed.scheduler - INFO - End scheduler
