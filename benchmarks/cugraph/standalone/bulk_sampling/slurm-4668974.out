Node IP: 10.248.8.200
Num Nodes: 2
Num GPUs Per Node: 8
no change     /opt/conda/condabin/conda
no change     /opt/conda/bin/conda
no change     /opt/conda/bin/conda-env
no change     /opt/conda/bin/activate
no change     /opt/conda/bin/deactivate
no change     /opt/conda/etc/profile.d/conda.sh
no change     /opt/conda/etc/fish/conf.d/conda.fish
no change     /opt/conda/shell/condabin/Conda.psm1
no change     /opt/conda/shell/condabin/conda-hook.ps1
no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/conda/etc/profile.d/conda.csh
no change     /root/.bashrc
No action taken.

EnvironmentNameNotFound: Could not find conda environment: rapids
You can list all discoverable environments with `conda info --envs`.


properly waiting for workers to connect
>>>> Using cluster configurtion for TCP
>>>> Logs written to: /logs
wait_for_workers.py - initializing client...done.
wait_for_workers.py expected 16 but got 0, waiting...
no change     /opt/conda/condabin/conda
no change     /opt/conda/bin/conda
no change     /opt/conda/bin/conda-env
no change     /opt/conda/bin/activate
no change     /opt/conda/bin/deactivate
no change     /opt/conda/etc/profile.d/conda.sh
no change     /opt/conda/etc/fish/conf.d/conda.fish
no change     /opt/conda/shell/condabin/Conda.psm1
no change     /opt/conda/shell/condabin/conda-hook.ps1
no change     /opt/conda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /opt/conda/etc/profile.d/conda.csh
no change     /root/.bashrc
No action taken.

EnvironmentNameNotFound: Could not find conda environment: rapids
You can list all discoverable environments with `conda info --envs`.


properly waiting for workers to connect
>>>> Using cluster configurtion for TCP
>>>> Logs written to: /logs
worker(s) started.
waiting for worker pid 267773 to finish before exiting script...
wait_for_workers.py - initializing client...done.
wait_for_workers.py expected 16 but got 0, waiting...
scheduler started.
worker(s) started.
waiting for worker pid 2450595 to finish before exiting script...
wait_for_workers.py expected 16 but got 0, waiting...
wait_for_workers.py expected 16 but got 8, waiting...
wait_for_workers.py got 16 workers, done.
0
Launching Python Script
wait_for_workers.py got 16 workers, done.
1
INFO:__main__:starting dask client
INFO:__main__:dask client started
INFO:__main__:dataset: ogbn_papers100M
INFO:__main__:batch size: 512
INFO:__main__:fanout: [10, 10, 10]
INFO:__main__:seeds_per_call: 1048576
INFO:__main__:num epochs: 1
INFO:__main__:ogbn_papers100M
INFO:__main__:Number of input edges = 3,231,371,744
INFO:__main__:constructed graph
/opt/conda/lib/python3.10/site-packages/cudf/core/index.py:3329: FutureWarning: cudf.StringIndex is deprecated and will be removed from cudf in a future version. Use cudf.Index with the appropriate dtype instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/cudf/core/index.py:3329: FutureWarning: cudf.StringIndex is deprecated and will be removed from cudf in a future version. Use cudf.Index with the appropriate dtype instead.
  warnings.warn(
INFO:__main__:input memory: 51701947904
/scripts/cugraph_bulk_sampling.py:840: UserWarning: An Exception Occurred!
  warnings.warn("An Exception Occurred!")
Traceback (most recent call last):
  File "/scripts/cugraph_bulk_sampling.py", line 812, in <module>
    ) = benchmark_cugraph_bulk_sampling(
  File "/scripts/cugraph_bulk_sampling.py", line 566, in benchmark_cugraph_bulk_sampling
    os.makedirs(output_subdir)
  File "/opt/conda/lib/python3.10/os.py", line 225, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/samples/ogbn_papers100M[2]_b512_f[10, 10, 10]'

Dask client created using /scripts/mg_utils/dask_scheduler.json
Loading edge index for edge type paper__cites__paper
Loading node labels for node type paper (offset=0)
[Errno 17] File exists: '/samples/ogbn_papers100M[2]_b512_f[10, 10, 10]'
----------------------------------------dataset = ogbn_papers100M completed----------------------------------------

Dask client closed.
[1706477676.388666] [luna-0327:2450731:0]          parser.c:2036 UCX  WARN  unused environment variable: UCX_MEMTYPE_CACHE (maybe: UCX_MEMTYPE_CACHE?)
[1706477676.388666] [luna-0327:2450731:0]          parser.c:2036 UCX  WARN  (set UCX_WARN_UNUSED_ENV_VARS=n to suppress this warning)
2450538 /bin/bash /scripts/mg_utils/run-dask-process.sh scheduler workers
2450583 /opt/conda/bin/python3.10 /opt/conda/bin/dask-scheduler --protocol=tcp --scheduler-file /scripts/mg_utils/dask_scheduler.json
2450595 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
2450599 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(46)
2450602 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=53) --multiprocessing-fork
2450606 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=62) --multiprocessing-fork
2450610 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=69) --multiprocessing-fork
2450614 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=76) --multiprocessing-fork
2450619 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=83) --multiprocessing-fork
2450623 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=92) --multiprocessing-fork
2450627 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=97) --multiprocessing-fork
2450630 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=100) --multiprocessing-fork
267735 /bin/bash /scripts/mg_utils/run-dask-process.sh workers
267773 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
267781 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(46)
267784 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=53) --multiprocessing-fork
267788 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=60) --multiprocessing-fork
267792 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=69) --multiprocessing-fork
267797 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=78) --multiprocessing-fork
267800 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=85) --multiprocessing-fork
267804 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=90) --multiprocessing-fork
267808 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=97) --multiprocessing-fork
267812 /opt/conda/bin/python -c from multiprocessing.spawn import spawn_main; spawn_main(tracker_fd=47, pipe_handle=102) --multiprocessing-fork
2450599 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(46)
2450602 python
2450606 python
2450610 python
2450614 python
2450619 python
2450623 python
2450627 python
2450630 python
2450583 /opt/conda/bin/python3.10 /opt/conda/bin/dask-scheduler --protocol=tcp --scheduler-file /scripts/mg_utils/dask_scheduler.json
2450595 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
267781 /opt/conda/bin/python -c from multiprocessing.resource_tracker import main;main(46)
267784 python
267788 python
267792 python
267797 python
267800 python
267804 python
267808 python
267812 python
267773 /opt/conda/bin/python /opt/conda/bin/dask-cuda-worker --rmm-pool-size=28G --rmm-async --local-directory=/tmp/abarghi --scheduler-file=/scripts/mg_utils/dask_scheduler.json --memory-limit=auto --device-memory-limit=auto
srun: Job 4668974 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 4668974
[2024-01-28 13:35:47,386] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-01-28 13:35:47,386] torch.distributed.run: [WARNING] 
[2024-01-28 13:35:47,386] torch.distributed.run: [WARNING] *****************************************
[2024-01-28 13:35:47,386] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-01-28 13:35:47,386] torch.distributed.run: [WARNING] *****************************************
[2024-01-28 13:35:48,398] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-01-28 13:35:48,398] torch.distributed.run: [WARNING] 
[2024-01-28 13:35:48,398] torch.distributed.run: [WARNING] *****************************************
[2024-01-28 13:35:48,398] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-01-28 13:35:48,398] torch.distributed.run: [WARNING] *****************************************
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:rank 9 successfully initialized WG comms
INFO:bench_cugraph_training:rank 10 successfully initialized WG comms
INFO:bench_cugraph_training:rank 12 successfully initialized WG comms
INFO:bench_cugraph_training:rank 14 successfully initialized WG comms
INFO:bench_cugraph_training:rank 13 successfully initialized WG comms
INFO:bench_cugraph_training:rank 11 successfully initialized WG comms
INFO:bench_cugraph_training:rank 8 successfully initialized WG comms
INFO:bench_cugraph_training:rank 15 successfully initialized WG comms
INFO:bench_cugraph_training:rank 2 successfully initialized WG comms
INFO:bench_cugraph_training:rank 7 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1 successfully initialized WG comms
INFO:bench_cugraph_training:rank 0 successfully initialized WG comms
INFO:bench_cugraph_training:rank 6 successfully initialized WG comms
INFO:bench_cugraph_training:rank 4 successfully initialized WG comms
INFO:bench_cugraph_training:rank 3 successfully initialized WG comms
INFO:bench_cugraph_training:rank 5 successfully initialized WG comms
INFO:OGBNPapers100MDataset:Processing dataset...
INFO:OGBNPapers100MDataset:Processing node features...
INFO:OGBNPapers100MDataset:Processing edge index...
INFO:OGBNPapers100MDataset:Processing labels...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
Rank=8 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=2048.
Rank=8 done reading total 7107837440 bytes from needed files.
Rank=14 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=42647026688.
Rank=14 done reading total 7107837440 bytes from needed files.
Rank=6 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=42647024640.
Rank=6 done reading total 7107837440 bytes from needed files.
Rank=4 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=28431349760.
Rank=4 done reading total 7107837440 bytes from needed files.
Rank=7 done Reading 7107835392 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=49754862080.
Rank=5 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=35539187200.
Rank=5 done reading total 7107837440 bytes from needed files.
Rank=7 done Reading 2048 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=0.
Rank=7 done reading total 7107837440 bytes from needed files.
Rank=1 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=7107837440.
Rank=1 done reading total 7107837440 bytes from needed files.
Rank=0 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=0.
Rank=0 done reading total 7107837440 bytes from needed files.
Rank=3 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=21323512320.
Rank=3 done reading total 7107837440 bytes from needed files.
Rank=2 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/1.bin size=56862697472, starting from offset=14215674880.
Rank=2 done reading total 7107837440 bytes from needed files.
Rank=13 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=35539189248.
Rank=13 done reading total 7107837440 bytes from needed files.
Rank=9 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=7107839488.
Rank=9 done reading total 7107837440 bytes from needed files.
Rank=15 done Reading 7107833344 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=49754864128.
Rank=15 done reading total 7107833344 bytes from needed files.
Rank=11 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=21323514368.
Rank=11 done reading total 7107837440 bytes from needed files.
Rank=12 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=28431351808.
Rank=12 done reading total 7107837440 bytes from needed files.
Rank=10 done Reading 7107837440 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_2x.d/0.bin size=56862697472, starting from offset=14215676928.
Rank=10 done reading total 7107837440 bytes from needed files.
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
done creating modeldone creating model

done creating model
done creating model
done creating model
INFO:bench_cugraph_training:Trainer ready on rank 5
INFO:bench_cugraph_training:Trainer ready on rank 7
INFO:bench_cugraph_training:Trainer ready on rank 4
done creating model
done creating model
done creating modeldone creating model
done creating model

INFO:bench_cugraph_training:Trainer ready on rank 2
INFO:bench_cugraph_training:Trainer ready on rank 3
done creating model
done creating model
INFO:bench_cugraph_training:Trainer ready on rank 1
INFO:bench_cugraph_training:Trainer ready on rank 6
done creating model
done creating model
INFO:bench_cugraph_training:Trainer ready on rank 14
INFO:bench_cugraph_training:Trainer ready on rank 13
INFO:bench_cugraph_training:Trainer ready on rank 10
INFO:bench_cugraph_training:Trainer ready on rank 12
done creating model
INFO:bench_cugraph_training:Trainer ready on rank 15
INFO:bench_cugraph_training:Trainer ready on rank 8
INFO:bench_cugraph_training:Trainer ready on rank 0
INFO:bench_cugraph_training:Trainer ready on rank 11
done creating model
INFO:bench_cugraph_training:Trainer ready on rank 9
Creating dataloader
Creating dataloader
Creating dataloader
Creating dataloaderCreating dataloader

Time to create dataloader = 0.00 secondsCreating dataloader
Time to create dataloader = 0.00 seconds

Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Creating dataloader
Creating dataloader
Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Time to create dataloader = 0.00 seconds
Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Time to create dataloader = 0.00 seconds
Creating dataloader
Creating dataloader
Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Creating dataloader
Time to create dataloader = 0.00 seconds
Time to create dataloader = 0.00 seconds
Time to create dataloader = 0.00 seconds
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
INFO:DGLCuGraphTrainer:getting data
INFO:DGLCuGraphTrainer:using wholegraph backend
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 227, in __getattr__
    return self[key]
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1314, in __getitem__
    return self._get_columns_by_label(arg, downcast=True)
  File "/opt/conda/lib/python3.10/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/dataframe.py", line 1973, in _get_columns_by_label
    ca = self._data.select_by_label(labels)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 397, in select_by_label
    return self._select_by_label_grouped(key)
  File "/opt/conda/lib/python3.10/site-packages/cudf/core/column_accessor.py", line 552, in _select_by_label_grouped
    result = self._grouped_data[key]
KeyError: 'major_offsets'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scripts/bench_cugraph_training.py", line 309, in <module>
    main(args)
  File "/scripts/bench_cugraph_training.py", line 300, in main
    stats = trainer.train()
  File "/scripts/trainers/dgl/trainers_dgl.py", line 290, in train
    num_batches = train_epoch(
  File "/scripts/trainers/dgl/trainers_dgl.py", line 128, in train_epoch
    for iter_i, (input_nodes, output_nodes, blocks) in enumerate(loader):
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 53, in fetch
    data = self.dataset[possibly_batched_index]
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/dataset.py", line 73, in __getitem__
    create_homogeneous_sampled_graphs_from_dataframe_csc(df)
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 556, in create_homogeneous_sampled_graphs_from_dataframe_csc
    *(_process_sampled_df_csc(sampled_df))
  File "/opt/conda/lib/python3.10/site-packages/cugraph_dgl/dataloading/utils/sampling_helpers.py", line 446, in _process_sampled_df_csc
    major_offsets = cast_to_tensor(df.major_offsets.dropna())
  File "/opt/conda/lib/python3.10/site-packages/cudf/utils/utils.py", line 229, in __getattr__
    raise AttributeError(
AttributeError: DataFrame object has no attribute major_offsets
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268902 closing signal SIGTERM
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268903 closing signal SIGTERM
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268904 closing signal SIGTERM
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268905 closing signal SIGTERM
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268906 closing signal SIGTERM
[2024-01-28 13:37:34,625] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268907 closing signal SIGTERM
[2024-01-28 13:37:34,627] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451690 closing signal SIGTERM
[2024-01-28 13:37:34,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451691 closing signal SIGTERM
[2024-01-28 13:37:34,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451692 closing signal SIGTERM
[2024-01-28 13:37:34,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451693 closing signal SIGTERM
[2024-01-28 13:37:34,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451694 closing signal SIGTERM
[2024-01-28 13:37:34,628] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2451696 closing signal SIGTERM
[2024-01-28 13:37:36,921] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2451689) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.2', 'console_scripts', 'torchrun')())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/scripts/bench_cugraph_training.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-01-28_13:37:34
  host      : luna-0327.selene.nvidia.com
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 2451695)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-01-28_13:37:34
  host      : luna-0327.selene.nvidia.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2451689)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: luna-0327: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=4668974.2
[2024-01-28 13:37:37,391] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-01-28 13:37:37,391] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268903 closing signal SIGTERM
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.2', 'console_scripts', 'torchrun')())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 268275 got signal: 15
srun: error: luna-0331: task 1: Exited with exit code 1
