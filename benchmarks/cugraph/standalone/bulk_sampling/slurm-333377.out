Node IP: 10.52.51.98
Num Nodes: 128
Num GPUs Per Node: 8
[2024-02-18 20:42:30,489] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:30,489] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:30,489] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:30,489] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:30,489] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:30,567] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:30,567] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:30,567] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:30,567] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:30,567] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,028] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,029] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,029] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,029] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,029] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,116] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,116] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,116] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,116] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,116] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,279] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,279] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,279] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,279] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,279] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,671] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,671] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,671] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,671] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,671] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,746] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,747] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,747] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,747] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,747] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,985] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:31,985] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:31,985] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:31,985] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:31,985] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,068] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:32,068] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:32,068] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,068] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:32,068] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,190] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:32,190] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:32,190] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,190] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:32,190] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,342] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:32,342] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:32,342] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,342] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:32,342] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,965] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:32,965] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:32,965] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:32,965] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:32,965] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:33,283] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:33,283] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:33,283] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:33,283] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:33,283] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:33,798] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:33,798] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:33,798] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:33,798] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:33,798] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,528] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:34,528] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:34,528] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,528] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:34,528] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,537] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:34,538] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:34,538] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,538] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:34,538] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,545] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:34,545] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:34,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,545] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:34,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,926] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:34,927] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:34,927] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:34,927] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:34,927] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,157] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,157] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,157] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,157] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,157] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,204] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,204] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,204] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,204] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,204] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,600] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,600] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,600] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,600] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,600] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,663] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,663] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,663] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,663] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,663] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,799] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,799] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,799] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,799] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,799] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,869] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:35,869] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:35,869] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:35,869] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:35,869] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,049] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:36,049] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:36,049] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,049] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:36,049] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,197] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:36,197] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:36,197] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,197] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:36,197] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,593] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:36,593] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:36,593] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,593] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:36,593] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,770] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:36,770] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:36,770] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:36,770] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:36,770] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,238] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:37,238] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:37,238] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,238] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:37,238] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,512] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:37,512] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:37,512] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,512] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:37,512] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,521] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:37,522] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:37,522] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,522] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:37,522] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,964] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:37,964] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:37,964] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,964] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:37,964] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,968] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:37,968] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:37,968] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:37,968] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:37,968] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,113] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,113] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,113] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,113] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,113] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,220] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,220] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,220] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,220] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,220] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,492] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,492] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,492] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,492] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,492] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,509] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,510] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,510] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,510] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,510] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,512] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,512] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,512] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,512] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,512] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,553] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,554] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,554] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,554] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,554] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,579] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,579] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,579] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,579] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,579] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,817] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,817] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,817] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,817] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,817] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,860] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:38,860] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:38,860] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:38,860] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:38,860] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,017] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,017] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,017] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,017] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,017] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,070] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,070] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,070] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,070] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,070] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,204] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,204] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,204] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,204] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,204] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,280] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,280] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,280] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,280] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,280] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,674] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,674] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,674] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,674] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,674] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,873] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:39,873] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:39,873] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:39,873] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:39,873] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,133] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,133] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,133] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,133] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,133] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,202] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,202] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,202] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,202] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,202] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,234] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,234] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,234] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,234] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,234] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,238] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,238] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,238] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,238] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,238] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,306] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,306] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,306] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,306] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,306] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,332] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,332] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,332] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,332] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,332] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,372] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,372] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,372] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,372] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,372] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,382] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,382] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,382] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,382] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,382] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,401] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,401] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,401] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,401] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,401] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,419] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,419] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,419] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,419] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,419] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,436] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,436] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,436] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,436] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,436] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,440] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,440] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,440] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,440] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,440] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,463] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,463] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,463] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,463] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,463] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,479] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,479] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,479] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,479] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,479] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,501] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,501] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,501] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,501] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,501] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,523] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,523] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,523] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,523] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,523] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,545] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,548] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,548] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,548] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,548] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,548] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,556] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,556] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,556] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,556] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,556] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,568] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,568] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,568] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,568] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,568] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,588] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,588] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,588] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,588] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,588] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,593] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,593] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,593] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,593] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,593] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,602] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,602] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,602] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,602] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,602] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,623] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,623] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,623] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,623] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,623] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,633] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,633] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,633] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,633] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,633] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,639] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,639] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,639] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,639] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,639] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,643] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,643] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,643] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,643] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,643] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,665] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,665] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,665] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,665] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,665] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,682] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,682] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,682] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,682] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,682] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,695] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,696] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,696] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,696] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,696] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,698] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,698] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,698] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,698] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,698] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,708] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,708] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,708] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,708] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,708] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,717] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,717] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,717] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,717] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,717] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,718] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,718] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,718] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,718] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,718] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,720] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,720] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,720] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,720] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,720] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,725] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,725] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,725] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,725] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,725] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,754] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,754] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,754] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,754] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,754] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,770] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,770] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,770] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,770] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,770] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,775] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,775] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,775] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,775] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,775] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,793] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,793] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,793] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,793] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,793] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,799] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,799] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,799] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,799] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,799] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,814] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,814] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,814] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,814] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,814] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,823] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,823] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,823] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,823] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,823] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,835] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,835] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,835] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,835] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,835] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,868] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,868] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,868] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,868] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,868] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,875] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,875] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,875] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,875] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,875] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,890] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,890] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,890] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,890] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,890] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,912] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,912] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,912] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,912] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,912] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,920] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,920] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,920] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,920] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,920] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,977] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,977] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,977] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,977] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,977] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,981] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,981] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,981] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,981] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,981] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,986] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,986] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,986] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,986] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,986] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,989] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:40,989] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:40,989] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:40,989] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:40,989] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,006] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,006] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,006] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,006] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,006] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,009] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,017] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,017] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,017] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,017] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,017] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,019] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,019] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,019] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,019] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,019] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,026] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,026] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,026] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,026] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,026] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,030] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,031] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,010] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,010] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,010] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,010] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,063] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,063] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,063] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,063] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,063] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,080] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,080] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,080] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,080] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,080] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,084] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,084] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,084] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,084] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,084] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,101] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,101] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,101] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,101] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,101] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,102] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,102] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,102] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,102] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,102] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,111] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,111] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,111] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,111] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,111] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,115] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,115] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,115] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,115] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,115] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,122] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,122] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,122] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,122] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,122] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,127] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,127] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,127] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,127] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,127] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,146] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,146] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,146] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,146] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,146] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,149] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,149] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,149] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,149] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,149] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,151] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,151] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,151] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,151] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,151] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,192] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,192] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,192] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,192] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,192] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,298] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,298] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,298] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,298] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,298] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,337] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,337] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,337] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,337] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,337] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,409] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,410] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,410] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,410] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,410] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,437] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,437] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,437] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,437] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,437] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,438] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,438] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,438] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,438] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,438] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,659] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-02-18 20:42:41,659] torch.distributed.run: [WARNING] 
[2024-02-18 20:42:41,659] torch.distributed.run: [WARNING] *****************************************
[2024-02-18 20:42:41,659] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-18 20:42:41,659] torch.distributed.run: [WARNING] *****************************************
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
worker initialized
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:initializing WG comms...
INFO:bench_cugraph_training:rank 137 successfully initialized WG comms
INFO:bench_cugraph_training:rank 136 successfully initialized WG comms
INFO:bench_cugraph_training:rank 139 successfully initialized WG comms
INFO:bench_cugraph_training:rank 138 successfully initialized WG comms
INFO:bench_cugraph_training:rank 142 successfully initialized WG comms
INFO:bench_cugraph_training:rank 143 successfully initialized WG comms
INFO:bench_cugraph_training:rank 141 successfully initialized WG comms
INFO:bench_cugraph_training:rank 140 successfully initialized WG comms
INFO:bench_cugraph_training:rank 240 successfully initialized WG comms
INFO:bench_cugraph_training:rank 243 successfully initialized WG comms
INFO:bench_cugraph_training:rank 241 successfully initialized WG comms
INFO:bench_cugraph_training:rank 242 successfully initialized WG comms
INFO:bench_cugraph_training:rank 246 successfully initialized WG comms
INFO:bench_cugraph_training:rank 245 successfully initialized WG comms
INFO:bench_cugraph_training:rank 247 successfully initialized WG comms
INFO:bench_cugraph_training:rank 244 successfully initialized WG comms
INFO:bench_cugraph_training:rank 888 successfully initialized WG comms
INFO:bench_cugraph_training:rank 891 successfully initialized WG comms
INFO:bench_cugraph_training:rank 889 successfully initialized WG comms
INFO:bench_cugraph_training:rank 890 successfully initialized WG comms
INFO:bench_cugraph_training:rank 894 successfully initialized WG comms
INFO:bench_cugraph_training:rank 895 successfully initialized WG comms
INFO:bench_cugraph_training:rank 892 successfully initialized WG comms
INFO:bench_cugraph_training:rank 893 successfully initialized WG comms
INFO:bench_cugraph_training:rank 736 successfully initialized WG comms
INFO:bench_cugraph_training:rank 738 successfully initialized WG comms
INFO:bench_cugraph_training:rank 739 successfully initialized WG comms
INFO:bench_cugraph_training:rank 737 successfully initialized WG comms
INFO:bench_cugraph_training:rank 741 successfully initialized WG comms
INFO:bench_cugraph_training:rank 742 successfully initialized WG comms
INFO:bench_cugraph_training:rank 740 successfully initialized WG comms
INFO:bench_cugraph_training:rank 743 successfully initialized WG comms
INFO:bench_cugraph_training:rank 669 successfully initialized WG comms
INFO:bench_cugraph_training:rank 668 successfully initialized WG comms
INFO:bench_cugraph_training:rank 671 successfully initialized WG comms
INFO:bench_cugraph_training:rank 670 successfully initialized WG comms
INFO:bench_cugraph_training:rank 665 successfully initialized WG comms
INFO:bench_cugraph_training:rank 666 successfully initialized WG comms
INFO:bench_cugraph_training:rank 667 successfully initialized WG comms
INFO:bench_cugraph_training:rank 664 successfully initialized WG comms
INFO:bench_cugraph_training:rank 294 successfully initialized WG comms
INFO:bench_cugraph_training:rank 292 successfully initialized WG comms
INFO:bench_cugraph_training:rank 293 successfully initialized WG comms
INFO:bench_cugraph_training:rank 295 successfully initialized WG comms
INFO:bench_cugraph_training:rank 288 successfully initialized WG comms
INFO:bench_cugraph_training:rank 290 successfully initialized WG comms
INFO:bench_cugraph_training:rank 291 successfully initialized WG comms
INFO:bench_cugraph_training:rank 289 successfully initialized WG comms
INFO:bench_cugraph_training:rank 454 successfully initialized WG comms
INFO:bench_cugraph_training:rank 452 successfully initialized WG comms
INFO:bench_cugraph_training:rank 455 successfully initialized WG comms
INFO:bench_cugraph_training:rank 453 successfully initialized WG comms
INFO:bench_cugraph_training:rank 449 successfully initialized WG comms
INFO:bench_cugraph_training:rank 450 successfully initialized WG comms
INFO:bench_cugraph_training:rank 451 successfully initialized WG comms
INFO:bench_cugraph_training:rank 448 successfully initialized WG comms
INFO:bench_cugraph_training:rank 338 successfully initialized WG comms
INFO:bench_cugraph_training:rank 337 successfully initialized WG comms
INFO:bench_cugraph_training:rank 336 successfully initialized WG comms
INFO:bench_cugraph_training:rank 339 successfully initialized WG comms
INFO:bench_cugraph_training:rank 342 successfully initialized WG comms
INFO:bench_cugraph_training:rank 343 successfully initialized WG comms
INFO:bench_cugraph_training:rank 341 successfully initialized WG comms
INFO:bench_cugraph_training:rank 340 successfully initialized WG comms
INFO:bench_cugraph_training:rank 802 successfully initialized WG comms
INFO:bench_cugraph_training:rank 800 successfully initialized WG comms
INFO:bench_cugraph_training:rank 803 successfully initialized WG comms
INFO:bench_cugraph_training:rank 801 successfully initialized WG comms
INFO:bench_cugraph_training:rank 804 successfully initialized WG comms
INFO:bench_cugraph_training:rank 806 successfully initialized WG comms
INFO:bench_cugraph_training:rank 805 successfully initialized WG comms
INFO:bench_cugraph_training:rank 807 successfully initialized WG comms
INFO:bench_cugraph_training:rank 830 successfully initialized WG comms
INFO:bench_cugraph_training:rank 829 successfully initialized WG comms
INFO:bench_cugraph_training:rank 831 successfully initialized WG comms
INFO:bench_cugraph_training:rank 828 successfully initialized WG comms
INFO:bench_cugraph_training:rank 825 successfully initialized WG comms
INFO:bench_cugraph_training:rank 824 successfully initialized WG comms
INFO:bench_cugraph_training:rank 826 successfully initialized WG comms
INFO:bench_cugraph_training:rank 827 successfully initialized WG comms
INFO:bench_cugraph_training:rank 263 successfully initialized WG comms
INFO:bench_cugraph_training:rank 260 successfully initialized WG comms
INFO:bench_cugraph_training:rank 262 successfully initialized WG comms
INFO:bench_cugraph_training:rank 261 successfully initialized WG comms
INFO:bench_cugraph_training:rank 258 successfully initialized WG comms
INFO:bench_cugraph_training:rank 257 successfully initialized WG comms
INFO:bench_cugraph_training:rank 259 successfully initialized WG comms
INFO:bench_cugraph_training:rank 256 successfully initialized WG comms
INFO:bench_cugraph_training:rank 844 successfully initialized WG comms
INFO:bench_cugraph_training:rank 845 successfully initialized WG comms
INFO:bench_cugraph_training:rank 846 successfully initialized WG comms
INFO:bench_cugraph_training:rank 847 successfully initialized WG comms
INFO:bench_cugraph_training:rank 840 successfully initialized WG comms
INFO:bench_cugraph_training:rank 842 successfully initialized WG comms
INFO:bench_cugraph_training:rank 841 successfully initialized WG comms
INFO:bench_cugraph_training:rank 843 successfully initialized WG comms
INFO:bench_cugraph_training:rank 551 successfully initialized WG comms
INFO:bench_cugraph_training:rank 549 successfully initialized WG comms
INFO:bench_cugraph_training:rank 548 successfully initialized WG comms
INFO:bench_cugraph_training:rank 550 successfully initialized WG comms
INFO:bench_cugraph_training:rank 545 successfully initialized WG comms
INFO:bench_cugraph_training:rank 547 successfully initialized WG comms
INFO:bench_cugraph_training:rank 546 successfully initialized WG comms
INFO:bench_cugraph_training:rank 544 successfully initialized WG comms
INFO:bench_cugraph_training:rank 974 successfully initialized WG comms
INFO:bench_cugraph_training:rank 972 successfully initialized WG comms
INFO:bench_cugraph_training:rank 975 successfully initialized WG comms
INFO:bench_cugraph_training:rank 973 successfully initialized WG comms
INFO:bench_cugraph_training:rank 970 successfully initialized WG comms
INFO:bench_cugraph_training:rank 969 successfully initialized WG comms
INFO:bench_cugraph_training:rank 971 successfully initialized WG comms
INFO:bench_cugraph_training:rank 968 successfully initialized WG comms
INFO:bench_cugraph_training:rank 717 successfully initialized WG comms
INFO:bench_cugraph_training:rank 718 successfully initialized WG comms
INFO:bench_cugraph_training:rank 716 successfully initialized WG comms
INFO:bench_cugraph_training:rank 719 successfully initialized WG comms
INFO:bench_cugraph_training:rank 33 successfully initialized WG comms
INFO:bench_cugraph_training:rank 35 successfully initialized WG comms
INFO:bench_cugraph_training:rank 34 successfully initialized WG comms
INFO:bench_cugraph_training:rank 32 successfully initialized WG comms
INFO:bench_cugraph_training:rank 38 successfully initialized WG comms
INFO:bench_cugraph_training:rank 36 successfully initialized WG comms
INFO:bench_cugraph_training:rank 39 successfully initialized WG comms
INFO:bench_cugraph_training:rank 37 successfully initialized WG comms
INFO:bench_cugraph_training:rank 715 successfully initialized WG comms
INFO:bench_cugraph_training:rank 714 successfully initialized WG comms
INFO:bench_cugraph_training:rank 713 successfully initialized WG comms
INFO:bench_cugraph_training:rank 712 successfully initialized WG comms
INFO:bench_cugraph_training:rank 145 successfully initialized WG comms
INFO:bench_cugraph_training:rank 146 successfully initialized WG comms
INFO:bench_cugraph_training:rank 147 successfully initialized WG comms
INFO:bench_cugraph_training:rank 144 successfully initialized WG comms
INFO:bench_cugraph_training:rank 151 successfully initialized WG comms
INFO:bench_cugraph_training:rank 149 successfully initialized WG comms
INFO:bench_cugraph_training:rank 150 successfully initialized WG comms
INFO:bench_cugraph_training:rank 148 successfully initialized WG comms
INFO:bench_cugraph_training:rank 120 successfully initialized WG comms
INFO:bench_cugraph_training:rank 122 successfully initialized WG comms
INFO:bench_cugraph_training:rank 123 successfully initialized WG comms
INFO:bench_cugraph_training:rank 121 successfully initialized WG comms
INFO:bench_cugraph_training:rank 127 successfully initialized WG comms
INFO:bench_cugraph_training:rank 124 successfully initialized WG comms
INFO:bench_cugraph_training:rank 125 successfully initialized WG comms
INFO:bench_cugraph_training:rank 126 successfully initialized WG comms
INFO:bench_cugraph_training:rank 867 successfully initialized WG comms
INFO:bench_cugraph_training:rank 866 successfully initialized WG comms
INFO:bench_cugraph_training:rank 865 successfully initialized WG comms
INFO:bench_cugraph_training:rank 864 successfully initialized WG comms
INFO:bench_cugraph_training:rank 871 successfully initialized WG comms
INFO:bench_cugraph_training:rank 869 successfully initialized WG comms
INFO:bench_cugraph_training:rank 870 successfully initialized WG comms
INFO:bench_cugraph_training:rank 868 successfully initialized WG comms
INFO:bench_cugraph_training:rank 413 successfully initialized WG comms
INFO:bench_cugraph_training:rank 414 successfully initialized WG comms
INFO:bench_cugraph_training:rank 415 successfully initialized WG comms
INFO:bench_cugraph_training:rank 412 successfully initialized WG comms
INFO:bench_cugraph_training:rank 408 successfully initialized WG comms
INFO:bench_cugraph_training:rank 409 successfully initialized WG comms
INFO:bench_cugraph_training:rank 410 successfully initialized WG comms
INFO:bench_cugraph_training:rank 411 successfully initialized WG comms
INFO:bench_cugraph_training:rank 795 successfully initialized WG comms
INFO:bench_cugraph_training:rank 793 successfully initialized WG comms
INFO:bench_cugraph_training:rank 792 successfully initialized WG comms
INFO:bench_cugraph_training:rank 794 successfully initialized WG comms
INFO:bench_cugraph_training:rank 797 successfully initialized WG comms
INFO:bench_cugraph_training:rank 796 successfully initialized WG comms
INFO:bench_cugraph_training:rank 799 successfully initialized WG comms
INFO:bench_cugraph_training:rank 798 successfully initialized WG comms
INFO:bench_cugraph_training:rank 436 successfully initialized WG comms
INFO:bench_cugraph_training:rank 438 successfully initialized WG comms
INFO:bench_cugraph_training:rank 437 successfully initialized WG comms
INFO:bench_cugraph_training:rank 439 successfully initialized WG comms
INFO:bench_cugraph_training:rank 432 successfully initialized WG comms
INFO:bench_cugraph_training:rank 433 successfully initialized WG comms
INFO:bench_cugraph_training:rank 435 successfully initialized WG comms
INFO:bench_cugraph_training:rank 434 successfully initialized WG comms
INFO:bench_cugraph_training:rank 400 successfully initialized WG comms
INFO:bench_cugraph_training:rank 402 successfully initialized WG comms
INFO:bench_cugraph_training:rank 403 successfully initialized WG comms
INFO:bench_cugraph_training:rank 401 successfully initialized WG comms
INFO:bench_cugraph_training:rank 406 successfully initialized WG comms
INFO:bench_cugraph_training:rank 404 successfully initialized WG comms
INFO:bench_cugraph_training:rank 405 successfully initialized WG comms
INFO:bench_cugraph_training:rank 407 successfully initialized WG comms
INFO:bench_cugraph_training:rank 950 successfully initialized WG comms
INFO:bench_cugraph_training:rank 948 successfully initialized WG comms
INFO:bench_cugraph_training:rank 949 successfully initialized WG comms
INFO:bench_cugraph_training:rank 951 successfully initialized WG comms
INFO:bench_cugraph_training:rank 944 successfully initialized WG comms
INFO:bench_cugraph_training:rank 945 successfully initialized WG comms
INFO:bench_cugraph_training:rank 946 successfully initialized WG comms
INFO:bench_cugraph_training:rank 947 successfully initialized WG comms
INFO:bench_cugraph_training:rank 850 successfully initialized WG comms
INFO:bench_cugraph_training:rank 848 successfully initialized WG comms
INFO:bench_cugraph_training:rank 851 successfully initialized WG comms
INFO:bench_cugraph_training:rank 849 successfully initialized WG comms
INFO:bench_cugraph_training:rank 853 successfully initialized WG comms
INFO:bench_cugraph_training:rank 854 successfully initialized WG comms
INFO:bench_cugraph_training:rank 855 successfully initialized WG comms
INFO:bench_cugraph_training:rank 852 successfully initialized WG comms
INFO:bench_cugraph_training:rank 363 successfully initialized WG comms
INFO:bench_cugraph_training:rank 360 successfully initialized WG comms
INFO:bench_cugraph_training:rank 361 successfully initialized WG comms
INFO:bench_cugraph_training:rank 362 successfully initialized WG comms
INFO:bench_cugraph_training:rank 365 successfully initialized WG comms
INFO:bench_cugraph_training:rank 364 successfully initialized WG comms
INFO:bench_cugraph_training:rank 367 successfully initialized WG comms
INFO:bench_cugraph_training:rank 366 successfully initialized WG comms
INFO:bench_cugraph_training:rank 783 successfully initialized WG comms
INFO:bench_cugraph_training:rank 781 successfully initialized WG comms
INFO:bench_cugraph_training:rank 780 successfully initialized WG comms
INFO:bench_cugraph_training:rank 782 successfully initialized WG comms
INFO:bench_cugraph_training:rank 776 successfully initialized WG comms
INFO:bench_cugraph_training:rank 778 successfully initialized WG comms
INFO:bench_cugraph_training:rank 777 successfully initialized WG comms
INFO:bench_cugraph_training:rank 779 successfully initialized WG comms
INFO:bench_cugraph_training:rank 603 successfully initialized WG comms
INFO:bench_cugraph_training:rank 602 successfully initialized WG comms
INFO:bench_cugraph_training:rank 601 successfully initialized WG comms
INFO:bench_cugraph_training:rank 600 successfully initialized WG comms
INFO:bench_cugraph_training:rank 606 successfully initialized WG comms
INFO:bench_cugraph_training:rank 607 successfully initialized WG comms
INFO:bench_cugraph_training:rank 605 successfully initialized WG comms
INFO:bench_cugraph_training:rank 604 successfully initialized WG comms
INFO:bench_cugraph_training:rank 378 successfully initialized WG comms
INFO:bench_cugraph_training:rank 376 successfully initialized WG comms
INFO:bench_cugraph_training:rank 379 successfully initialized WG comms
INFO:bench_cugraph_training:rank 377 successfully initialized WG comms
INFO:bench_cugraph_training:rank 382 successfully initialized WG comms
INFO:bench_cugraph_training:rank 383 successfully initialized WG comms
INFO:bench_cugraph_training:rank 381 successfully initialized WG comms
INFO:bench_cugraph_training:rank 380 successfully initialized WG comms
INFO:bench_cugraph_training:rank 112 successfully initialized WG comms
INFO:bench_cugraph_training:rank 114 successfully initialized WG comms
INFO:bench_cugraph_training:rank 113 successfully initialized WG comms
INFO:bench_cugraph_training:rank 115 successfully initialized WG comms
INFO:bench_cugraph_training:rank 119 successfully initialized WG comms
INFO:bench_cugraph_training:rank 118 successfully initialized WG comms
INFO:bench_cugraph_training:rank 117 successfully initialized WG comms
INFO:bench_cugraph_training:rank 116 successfully initialized WG comms
INFO:bench_cugraph_training:rank 790 successfully initialized WG comms
INFO:bench_cugraph_training:rank 788 successfully initialized WG comms
INFO:bench_cugraph_training:rank 791 successfully initialized WG comms
INFO:bench_cugraph_training:rank 789 successfully initialized WG comms
INFO:bench_cugraph_training:rank 787 successfully initialized WG comms
INFO:bench_cugraph_training:rank 786 successfully initialized WG comms
INFO:bench_cugraph_training:rank 785 successfully initialized WG comms
INFO:bench_cugraph_training:rank 784 successfully initialized WG comms
INFO:bench_cugraph_training:rank 160 successfully initialized WG comms
INFO:bench_cugraph_training:rank 163 successfully initialized WG comms
INFO:bench_cugraph_training:rank 161 successfully initialized WG comms
INFO:bench_cugraph_training:rank 162 successfully initialized WG comms
INFO:bench_cugraph_training:rank 165 successfully initialized WG comms
INFO:bench_cugraph_training:rank 166 successfully initialized WG comms
INFO:bench_cugraph_training:rank 164 successfully initialized WG comms
INFO:bench_cugraph_training:rank 167 successfully initialized WG comms
INFO:bench_cugraph_training:rank 368 successfully initialized WG comms
INFO:bench_cugraph_training:rank 371 successfully initialized WG comms
INFO:bench_cugraph_training:rank 370 successfully initialized WG comms
INFO:bench_cugraph_training:rank 369 successfully initialized WG comms
INFO:bench_cugraph_training:rank 372 successfully initialized WG comms
INFO:bench_cugraph_training:rank 375 successfully initialized WG comms
INFO:bench_cugraph_training:rank 374 successfully initialized WG comms
INFO:bench_cugraph_training:rank 373 successfully initialized WG comms
INFO:bench_cugraph_training:rank 523 successfully initialized WG comms
INFO:bench_cugraph_training:rank 520 successfully initialized WG comms
INFO:bench_cugraph_training:rank 521 successfully initialized WG comms
INFO:bench_cugraph_training:rank 522 successfully initialized WG comms
INFO:bench_cugraph_training:rank 526 successfully initialized WG comms
INFO:bench_cugraph_training:rank 525 successfully initialized WG comms
INFO:bench_cugraph_training:rank 527 successfully initialized WG comms
INFO:bench_cugraph_training:rank 524 successfully initialized WG comms
INFO:bench_cugraph_training:rank 485 successfully initialized WG comms
INFO:bench_cugraph_training:rank 486 successfully initialized WG comms
INFO:bench_cugraph_training:rank 484 successfully initialized WG comms
INFO:bench_cugraph_training:rank 487 successfully initialized WG comms
INFO:bench_cugraph_training:rank 483 successfully initialized WG comms
INFO:bench_cugraph_training:rank 480 successfully initialized WG comms
INFO:bench_cugraph_training:rank 482 successfully initialized WG comms
INFO:bench_cugraph_training:rank 481 successfully initialized WG comms
INFO:bench_cugraph_training:rank 698 successfully initialized WG comms
INFO:bench_cugraph_training:rank 699 successfully initialized WG comms
INFO:bench_cugraph_training:rank 697 successfully initialized WG comms
INFO:bench_cugraph_training:rank 696 successfully initialized WG comms
INFO:bench_cugraph_training:rank 700 successfully initialized WG comms
INFO:bench_cugraph_training:rank 701 successfully initialized WG comms
INFO:bench_cugraph_training:rank 703 successfully initialized WG comms
INFO:bench_cugraph_training:rank 702 successfully initialized WG comms
INFO:bench_cugraph_training:rank 615 successfully initialized WG comms
INFO:bench_cugraph_training:rank 614 successfully initialized WG comms
INFO:bench_cugraph_training:rank 612 successfully initialized WG comms
INFO:bench_cugraph_training:rank 613 successfully initialized WG comms
INFO:bench_cugraph_training:rank 608 successfully initialized WG comms
INFO:bench_cugraph_training:rank 609 successfully initialized WG comms
INFO:bench_cugraph_training:rank 610 successfully initialized WG comms
INFO:bench_cugraph_training:rank 611 successfully initialized WG comms
INFO:bench_cugraph_training:rank 250 successfully initialized WG comms
INFO:bench_cugraph_training:rank 251 successfully initialized WG comms
INFO:bench_cugraph_training:rank 248 successfully initialized WG comms
INFO:bench_cugraph_training:rank 249 successfully initialized WG comms
INFO:bench_cugraph_training:rank 253 successfully initialized WG comms
INFO:bench_cugraph_training:rank 254 successfully initialized WG comms
INFO:bench_cugraph_training:rank 252 successfully initialized WG comms
INFO:bench_cugraph_training:rank 255 successfully initialized WG comms
INFO:bench_cugraph_training:rank 566 successfully initialized WG comms
INFO:bench_cugraph_training:rank 567 successfully initialized WG comms
INFO:bench_cugraph_training:rank 565 successfully initialized WG comms
INFO:bench_cugraph_training:rank 564 successfully initialized WG comms
INFO:bench_cugraph_training:rank 560 successfully initialized WG comms
INFO:bench_cugraph_training:rank 563 successfully initialized WG comms
INFO:bench_cugraph_training:rank 562 successfully initialized WG comms
INFO:bench_cugraph_training:rank 561 successfully initialized WG comms
INFO:bench_cugraph_training:rank 833 successfully initialized WG comms
INFO:bench_cugraph_training:rank 835 successfully initialized WG comms
INFO:bench_cugraph_training:rank 832 successfully initialized WG comms
INFO:bench_cugraph_training:rank 834 successfully initialized WG comms
INFO:bench_cugraph_training:rank 838 successfully initialized WG comms
INFO:bench_cugraph_training:rank 836 successfully initialized WG comms
INFO:bench_cugraph_training:rank 837 successfully initialized WG comms
INFO:bench_cugraph_training:rank 839 successfully initialized WG comms
INFO:bench_cugraph_training:rank 886 successfully initialized WG comms
INFO:bench_cugraph_training:rank 884 successfully initialized WG comms
INFO:bench_cugraph_training:rank 885 successfully initialized WG comms
INFO:bench_cugraph_training:rank 887 successfully initialized WG comms
INFO:bench_cugraph_training:rank 883 successfully initialized WG comms
INFO:bench_cugraph_training:rank 880 successfully initialized WG comms
INFO:bench_cugraph_training:rank 882 successfully initialized WG comms
INFO:bench_cugraph_training:rank 881 successfully initialized WG comms
INFO:bench_cugraph_training:rank 74 successfully initialized WG comms
INFO:bench_cugraph_training:rank 75 successfully initialized WG comms
INFO:bench_cugraph_training:rank 73 successfully initialized WG comms
INFO:bench_cugraph_training:rank 72 successfully initialized WG comms
INFO:bench_cugraph_training:rank 78 successfully initialized WG comms
INFO:bench_cugraph_training:rank 79 successfully initialized WG comms
INFO:bench_cugraph_training:rank 76 successfully initialized WG comms
INFO:bench_cugraph_training:rank 77 successfully initialized WG comms
INFO:bench_cugraph_training:rank 275 successfully initialized WG comms
INFO:bench_cugraph_training:rank 273 successfully initialized WG comms
INFO:bench_cugraph_training:rank 274 successfully initialized WG comms
INFO:bench_cugraph_training:rank 272 successfully initialized WG comms
INFO:bench_cugraph_training:rank 279 successfully initialized WG comms
INFO:bench_cugraph_training:rank 278 successfully initialized WG comms
INFO:bench_cugraph_training:rank 277 successfully initialized WG comms
INFO:bench_cugraph_training:rank 276 successfully initialized WG comms
INFO:bench_cugraph_training:rank 817 successfully initialized WG comms
INFO:bench_cugraph_training:rank 816 successfully initialized WG comms
INFO:bench_cugraph_training:rank 819 successfully initialized WG comms
INFO:bench_cugraph_training:rank 818 successfully initialized WG comms
INFO:bench_cugraph_training:rank 822 successfully initialized WG comms
INFO:bench_cugraph_training:rank 820 successfully initialized WG comms
INFO:bench_cugraph_training:rank 823 successfully initialized WG comms
INFO:bench_cugraph_training:rank 821 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1011 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1010 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1008 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1009 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1014 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1015 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1012 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1013 successfully initialized WG comms
INFO:bench_cugraph_training:rank 496 successfully initialized WG comms
INFO:bench_cugraph_training:rank 499 successfully initialized WG comms
INFO:bench_cugraph_training:rank 498 successfully initialized WG comms
INFO:bench_cugraph_training:rank 497 successfully initialized WG comms
INFO:bench_cugraph_training:rank 502 successfully initialized WG comms
INFO:bench_cugraph_training:rank 501 successfully initialized WG comms
INFO:bench_cugraph_training:rank 503 successfully initialized WG comms
INFO:bench_cugraph_training:rank 500 successfully initialized WG comms
INFO:bench_cugraph_training:rank 542 successfully initialized WG comms
INFO:bench_cugraph_training:rank 541 successfully initialized WG comms
INFO:bench_cugraph_training:rank 540 successfully initialized WG comms
INFO:bench_cugraph_training:rank 543 successfully initialized WG comms
INFO:bench_cugraph_training:rank 538 successfully initialized WG comms
INFO:bench_cugraph_training:rank 539 successfully initialized WG comms
INFO:bench_cugraph_training:rank 537 successfully initialized WG comms
INFO:bench_cugraph_training:rank 536 successfully initialized WG comms
INFO:bench_cugraph_training:rank 731 successfully initialized WG comms
INFO:bench_cugraph_training:rank 729 successfully initialized WG comms
INFO:bench_cugraph_training:rank 728 successfully initialized WG comms
INFO:bench_cugraph_training:rank 730 successfully initialized WG comms
INFO:bench_cugraph_training:rank 733 successfully initialized WG comms
INFO:bench_cugraph_training:rank 735 successfully initialized WG comms
INFO:bench_cugraph_training:rank 732 successfully initialized WG comms
INFO:bench_cugraph_training:rank 734 successfully initialized WG comms
INFO:bench_cugraph_training:rank 85 successfully initialized WG comms
INFO:bench_cugraph_training:rank 87 successfully initialized WG comms
INFO:bench_cugraph_training:rank 86 successfully initialized WG comms
INFO:bench_cugraph_training:rank 84 successfully initialized WG comms
INFO:bench_cugraph_training:rank 81 successfully initialized WG comms
INFO:bench_cugraph_training:rank 83 successfully initialized WG comms
INFO:bench_cugraph_training:rank 80 successfully initialized WG comms
INFO:bench_cugraph_training:rank 82 successfully initialized WG comms
INFO:bench_cugraph_training:rank 179 successfully initialized WG comms
INFO:bench_cugraph_training:rank 177 successfully initialized WG comms
INFO:bench_cugraph_training:rank 176 successfully initialized WG comms
INFO:bench_cugraph_training:rank 178 successfully initialized WG comms
INFO:bench_cugraph_training:rank 181 successfully initialized WG comms
INFO:bench_cugraph_training:rank 183 successfully initialized WG comms
INFO:bench_cugraph_training:rank 182 successfully initialized WG comms
INFO:bench_cugraph_training:rank 180 successfully initialized WG comms
INFO:bench_cugraph_training:rank 281 successfully initialized WG comms
INFO:bench_cugraph_training:rank 280 successfully initialized WG comms
INFO:bench_cugraph_training:rank 282 successfully initialized WG comms
INFO:bench_cugraph_training:rank 283 successfully initialized WG comms
INFO:bench_cugraph_training:rank 285 successfully initialized WG comms
INFO:bench_cugraph_training:rank 287 successfully initialized WG comms
INFO:bench_cugraph_training:rank 284 successfully initialized WG comms
INFO:bench_cugraph_training:rank 286 successfully initialized WG comms
INFO:bench_cugraph_training:rank 42 successfully initialized WG comms
INFO:bench_cugraph_training:rank 43 successfully initialized WG comms
INFO:bench_cugraph_training:rank 40 successfully initialized WG comms
INFO:bench_cugraph_training:rank 41 successfully initialized WG comms
INFO:bench_cugraph_training:rank 45 successfully initialized WG comms
INFO:bench_cugraph_training:rank 47 successfully initialized WG comms
INFO:bench_cugraph_training:rank 46 successfully initialized WG comms
INFO:bench_cugraph_training:rank 44 successfully initialized WG comms
INFO:bench_cugraph_training:rank 309 successfully initialized WG comms
INFO:bench_cugraph_training:rank 308 successfully initialized WG comms
INFO:bench_cugraph_training:rank 310 successfully initialized WG comms
INFO:bench_cugraph_training:rank 311 successfully initialized WG comms
INFO:bench_cugraph_training:rank 304 successfully initialized WG comms
INFO:bench_cugraph_training:rank 307 successfully initialized WG comms
INFO:bench_cugraph_training:rank 305 successfully initialized WG comms
INFO:bench_cugraph_training:rank 306 successfully initialized WG comms
INFO:bench_cugraph_training:rank 102 successfully initialized WG comms
INFO:bench_cugraph_training:rank 103 successfully initialized WG comms
INFO:bench_cugraph_training:rank 101 successfully initialized WG comms
INFO:bench_cugraph_training:rank 100 successfully initialized WG comms
INFO:bench_cugraph_training:rank 97 successfully initialized WG comms
INFO:bench_cugraph_training:rank 98 successfully initialized WG comms
INFO:bench_cugraph_training:rank 99 successfully initialized WG comms
INFO:bench_cugraph_training:rank 96 successfully initialized WG comms
INFO:bench_cugraph_training:rank 70 successfully initialized WG comms
INFO:bench_cugraph_training:rank 71 successfully initialized WG comms
INFO:bench_cugraph_training:rank 68 successfully initialized WG comms
INFO:bench_cugraph_training:rank 69 successfully initialized WG comms
INFO:bench_cugraph_training:rank 64 successfully initialized WG comms
INFO:bench_cugraph_training:rank 65 successfully initialized WG comms
INFO:bench_cugraph_training:rank 66 successfully initialized WG comms
INFO:bench_cugraph_training:rank 67 successfully initialized WG comms
INFO:bench_cugraph_training:rank 771 successfully initialized WG comms
INFO:bench_cugraph_training:rank 769 successfully initialized WG comms
INFO:bench_cugraph_training:rank 768 successfully initialized WG comms
INFO:bench_cugraph_training:rank 770 successfully initialized WG comms
INFO:bench_cugraph_training:rank 775 successfully initialized WG comms
INFO:bench_cugraph_training:rank 774 successfully initialized WG comms
INFO:bench_cugraph_training:rank 772 successfully initialized WG comms
INFO:bench_cugraph_training:rank 773 successfully initialized WG comms
INFO:bench_cugraph_training:rank 392 successfully initialized WG comms
INFO:bench_cugraph_training:rank 394 successfully initialized WG comms
INFO:bench_cugraph_training:rank 395 successfully initialized WG comms
INFO:bench_cugraph_training:rank 393 successfully initialized WG comms
INFO:bench_cugraph_training:rank 398 successfully initialized WG comms
INFO:bench_cugraph_training:rank 396 successfully initialized WG comms
INFO:bench_cugraph_training:rank 397 successfully initialized WG comms
INFO:bench_cugraph_training:rank 399 successfully initialized WG comms
INFO:bench_cugraph_training:rank 390 successfully initialized WG comms
INFO:bench_cugraph_training:rank 388 successfully initialized WG comms
INFO:bench_cugraph_training:rank 391 successfully initialized WG comms
INFO:bench_cugraph_training:rank 389 successfully initialized WG comms
INFO:bench_cugraph_training:rank 92 successfully initialized WG comms
INFO:bench_cugraph_training:rank 93 successfully initialized WG comms
INFO:bench_cugraph_training:rank 94 successfully initialized WG comms
INFO:bench_cugraph_training:rank 95 successfully initialized WG comms
INFO:bench_cugraph_training:rank 385 successfully initialized WG comms
INFO:bench_cugraph_training:rank 384 successfully initialized WG comms
INFO:bench_cugraph_training:rank 387 successfully initialized WG comms
INFO:bench_cugraph_training:rank 386 successfully initialized WG comms
INFO:bench_cugraph_training:rank 91 successfully initialized WG comms
INFO:bench_cugraph_training:rank 88 successfully initialized WG comms
INFO:bench_cugraph_training:rank 89 successfully initialized WG comms
INFO:bench_cugraph_training:rank 90 successfully initialized WG comms
INFO:bench_cugraph_training:rank 925 successfully initialized WG comms
INFO:bench_cugraph_training:rank 927 successfully initialized WG comms
INFO:bench_cugraph_training:rank 924 successfully initialized WG comms
INFO:bench_cugraph_training:rank 926 successfully initialized WG comms
INFO:bench_cugraph_training:rank 923 successfully initialized WG comms
INFO:bench_cugraph_training:rank 921 successfully initialized WG comms
INFO:bench_cugraph_training:rank 922 successfully initialized WG comms
INFO:bench_cugraph_training:rank 920 successfully initialized WG comms
INFO:bench_cugraph_training:rank 679 successfully initialized WG comms
INFO:bench_cugraph_training:rank 677 successfully initialized WG comms
INFO:bench_cugraph_training:rank 678 successfully initialized WG comms
INFO:bench_cugraph_training:rank 676 successfully initialized WG comms
INFO:bench_cugraph_training:rank 675 successfully initialized WG comms
INFO:bench_cugraph_training:rank 674 successfully initialized WG comms
INFO:bench_cugraph_training:rank 673 successfully initialized WG comms
INFO:bench_cugraph_training:rank 672 successfully initialized WG comms
INFO:bench_cugraph_training:rank 931 successfully initialized WG comms
INFO:bench_cugraph_training:rank 929 successfully initialized WG comms
INFO:bench_cugraph_training:rank 930 successfully initialized WG comms
INFO:bench_cugraph_training:rank 928 successfully initialized WG comms
INFO:bench_cugraph_training:rank 932 successfully initialized WG comms
INFO:bench_cugraph_training:rank 934 successfully initialized WG comms
INFO:bench_cugraph_training:rank 935 successfully initialized WG comms
INFO:bench_cugraph_training:rank 933 successfully initialized WG comms
INFO:bench_cugraph_training:rank 271 successfully initialized WG comms
INFO:bench_cugraph_training:rank 269 successfully initialized WG comms
INFO:bench_cugraph_training:rank 270 successfully initialized WG comms
INFO:bench_cugraph_training:rank 268 successfully initialized WG comms
INFO:bench_cugraph_training:rank 756 successfully initialized WG comms
INFO:bench_cugraph_training:rank 759 successfully initialized WG comms
INFO:bench_cugraph_training:rank 757 successfully initialized WG comms
INFO:bench_cugraph_training:rank 758 successfully initialized WG comms
INFO:bench_cugraph_training:rank 752 successfully initialized WG comms
INFO:bench_cugraph_training:rank 754 successfully initialized WG comms
INFO:bench_cugraph_training:rank 753 successfully initialized WG comms
INFO:bench_cugraph_training:rank 755 successfully initialized WG comms
INFO:bench_cugraph_training:rank 265 successfully initialized WG comms
INFO:bench_cugraph_training:rank 267 successfully initialized WG comms
INFO:bench_cugraph_training:rank 266 successfully initialized WG comms
INFO:bench_cugraph_training:rank 264 successfully initialized WG comms
INFO:bench_cugraph_training:rank 328 successfully initialized WG comms
INFO:bench_cugraph_training:rank 330 successfully initialized WG comms
INFO:bench_cugraph_training:rank 329 successfully initialized WG comms
INFO:bench_cugraph_training:rank 331 successfully initialized WG comms
INFO:bench_cugraph_training:rank 333 successfully initialized WG comms
INFO:bench_cugraph_training:rank 334 successfully initialized WG comms
INFO:bench_cugraph_training:rank 335 successfully initialized WG comms
INFO:bench_cugraph_training:rank 332 successfully initialized WG comms
INFO:bench_cugraph_training:rank 352 successfully initialized WG comms
INFO:bench_cugraph_training:rank 353 successfully initialized WG comms
INFO:bench_cugraph_training:rank 355 successfully initialized WG comms
INFO:bench_cugraph_training:rank 354 successfully initialized WG comms
INFO:bench_cugraph_training:rank 358 successfully initialized WG comms
INFO:bench_cugraph_training:rank 359 successfully initialized WG comms
INFO:bench_cugraph_training:rank 357 successfully initialized WG comms
INFO:bench_cugraph_training:rank 356 successfully initialized WG comms
INFO:bench_cugraph_training:rank 900 successfully initialized WG comms
INFO:bench_cugraph_training:rank 901 successfully initialized WG comms
INFO:bench_cugraph_training:rank 902 successfully initialized WG comms
INFO:bench_cugraph_training:rank 903 successfully initialized WG comms
INFO:bench_cugraph_training:rank 899 successfully initialized WG comms
INFO:bench_cugraph_training:rank 896 successfully initialized WG comms
INFO:bench_cugraph_training:rank 898 successfully initialized WG comms
INFO:bench_cugraph_training:rank 897 successfully initialized WG comms
INFO:bench_cugraph_training:rank 999 successfully initialized WG comms
INFO:bench_cugraph_training:rank 997 successfully initialized WG comms
INFO:bench_cugraph_training:rank 996 successfully initialized WG comms
INFO:bench_cugraph_training:rank 998 successfully initialized WG comms
INFO:bench_cugraph_training:rank 992 successfully initialized WG comms
INFO:bench_cugraph_training:rank 993 successfully initialized WG comms
INFO:bench_cugraph_training:rank 994 successfully initialized WG comms
INFO:bench_cugraph_training:rank 995 successfully initialized WG comms
INFO:bench_cugraph_training:rank 351 successfully initialized WG comms
INFO:bench_cugraph_training:rank 348 successfully initialized WG comms
INFO:bench_cugraph_training:rank 349 successfully initialized WG comms
INFO:bench_cugraph_training:rank 350 successfully initialized WG comms
INFO:bench_cugraph_training:rank 346 successfully initialized WG comms
INFO:bench_cugraph_training:rank 345 successfully initialized WG comms
INFO:bench_cugraph_training:rank 344 successfully initialized WG comms
INFO:bench_cugraph_training:rank 347 successfully initialized WG comms
INFO:bench_cugraph_training:rank 460 successfully initialized WG comms
INFO:bench_cugraph_training:rank 463 successfully initialized WG comms
INFO:bench_cugraph_training:rank 461 successfully initialized WG comms
INFO:bench_cugraph_training:rank 462 successfully initialized WG comms
INFO:bench_cugraph_training:rank 459 successfully initialized WG comms
INFO:bench_cugraph_training:rank 457 successfully initialized WG comms
INFO:bench_cugraph_training:rank 456 successfully initialized WG comms
INFO:bench_cugraph_training:rank 458 successfully initialized WG comms
INFO:bench_cugraph_training:rank 873 successfully initialized WG comms
INFO:bench_cugraph_training:rank 872 successfully initialized WG comms
INFO:bench_cugraph_training:rank 875 successfully initialized WG comms
INFO:bench_cugraph_training:rank 874 successfully initialized WG comms
INFO:bench_cugraph_training:rank 878 successfully initialized WG comms
INFO:bench_cugraph_training:rank 877 successfully initialized WG comms
INFO:bench_cugraph_training:rank 879 successfully initialized WG comms
INFO:bench_cugraph_training:rank 876 successfully initialized WG comms
INFO:bench_cugraph_training:rank 943 successfully initialized WG comms
INFO:bench_cugraph_training:rank 942 successfully initialized WG comms
INFO:bench_cugraph_training:rank 941 successfully initialized WG comms
INFO:bench_cugraph_training:rank 940 successfully initialized WG comms
INFO:bench_cugraph_training:rank 936 successfully initialized WG comms
INFO:bench_cugraph_training:rank 939 successfully initialized WG comms
INFO:bench_cugraph_training:rank 937 successfully initialized WG comms
INFO:bench_cugraph_training:rank 938 successfully initialized WG comms
INFO:bench_cugraph_training:rank 694 successfully initialized WG comms
INFO:bench_cugraph_training:rank 695 successfully initialized WG comms
INFO:bench_cugraph_training:rank 692 successfully initialized WG comms
INFO:bench_cugraph_training:rank 693 successfully initialized WG comms
INFO:bench_cugraph_training:rank 689 successfully initialized WG comms
INFO:bench_cugraph_training:rank 691 successfully initialized WG comms
INFO:bench_cugraph_training:rank 690 successfully initialized WG comms
INFO:bench_cugraph_training:rank 688 successfully initialized WG comms
INFO:bench_cugraph_training:rank 59 successfully initialized WG comms
INFO:bench_cugraph_training:rank 58 successfully initialized WG comms
INFO:bench_cugraph_training:rank 56 successfully initialized WG comms
INFO:bench_cugraph_training:rank 57 successfully initialized WG comms
INFO:bench_cugraph_training:rank 62 successfully initialized WG comms
INFO:bench_cugraph_training:rank 60 successfully initialized WG comms
INFO:bench_cugraph_training:rank 61 successfully initialized WG comms
INFO:bench_cugraph_training:rank 63 successfully initialized WG comms
INFO:bench_cugraph_training:rank 763 successfully initialized WG comms
INFO:bench_cugraph_training:rank 761 successfully initialized WG comms
INFO:bench_cugraph_training:rank 760 successfully initialized WG comms
INFO:bench_cugraph_training:rank 762 successfully initialized WG comms
INFO:bench_cugraph_training:rank 765 successfully initialized WG comms
INFO:bench_cugraph_training:rank 764 successfully initialized WG comms
INFO:bench_cugraph_training:rank 767 successfully initialized WG comms
INFO:bench_cugraph_training:rank 766 successfully initialized WG comms
INFO:bench_cugraph_training:rank 153 successfully initialized WG comms
INFO:bench_cugraph_training:rank 155 successfully initialized WG comms
INFO:bench_cugraph_training:rank 152 successfully initialized WG comms
INFO:bench_cugraph_training:rank 154 successfully initialized WG comms
INFO:bench_cugraph_training:rank 156 successfully initialized WG comms
INFO:bench_cugraph_training:rank 159 successfully initialized WG comms
INFO:bench_cugraph_training:rank 157 successfully initialized WG comms
INFO:bench_cugraph_training:rank 158 successfully initialized WG comms
INFO:bench_cugraph_training:rank 648 successfully initialized WG comms
INFO:bench_cugraph_training:rank 649 successfully initialized WG comms
INFO:bench_cugraph_training:rank 650 successfully initialized WG comms
INFO:bench_cugraph_training:rank 651 successfully initialized WG comms
INFO:bench_cugraph_training:rank 653 successfully initialized WG comms
INFO:bench_cugraph_training:rank 654 successfully initialized WG comms
INFO:bench_cugraph_training:rank 652 successfully initialized WG comms
INFO:bench_cugraph_training:rank 655 successfully initialized WG comms
INFO:bench_cugraph_training:rank 205 successfully initialized WG comms
INFO:bench_cugraph_training:rank 207 successfully initialized WG comms
INFO:bench_cugraph_training:rank 206 successfully initialized WG comms
INFO:bench_cugraph_training:rank 204 successfully initialized WG comms
INFO:bench_cugraph_training:rank 200 successfully initialized WG comms
INFO:bench_cugraph_training:rank 203 successfully initialized WG comms
INFO:bench_cugraph_training:rank 201 successfully initialized WG comms
INFO:bench_cugraph_training:rank 202 successfully initialized WG comms
INFO:bench_cugraph_training:rank 132 successfully initialized WG comms
INFO:bench_cugraph_training:rank 134 successfully initialized WG comms
INFO:bench_cugraph_training:rank 135 successfully initialized WG comms
INFO:bench_cugraph_training:rank 133 successfully initialized WG comms
INFO:bench_cugraph_training:rank 131 successfully initialized WG comms
INFO:bench_cugraph_training:rank 129 successfully initialized WG comms
INFO:bench_cugraph_training:rank 128 successfully initialized WG comms
INFO:bench_cugraph_training:rank 130 successfully initialized WG comms
INFO:bench_cugraph_training:rank 687 successfully initialized WG comms
INFO:bench_cugraph_training:rank 684 successfully initialized WG comms
INFO:bench_cugraph_training:rank 685 successfully initialized WG comms
INFO:bench_cugraph_training:rank 686 successfully initialized WG comms
INFO:bench_cugraph_training:rank 681 successfully initialized WG comms
INFO:bench_cugraph_training:rank 680 successfully initialized WG comms
INFO:bench_cugraph_training:rank 683 successfully initialized WG comms
INFO:bench_cugraph_training:rank 682 successfully initialized WG comms
INFO:bench_cugraph_training:rank 169 successfully initialized WG comms
INFO:bench_cugraph_training:rank 168 successfully initialized WG comms
INFO:bench_cugraph_training:rank 171 successfully initialized WG comms
INFO:bench_cugraph_training:rank 170 successfully initialized WG comms
INFO:bench_cugraph_training:rank 174 successfully initialized WG comms
INFO:bench_cugraph_training:rank 172 successfully initialized WG comms
INFO:bench_cugraph_training:rank 175 successfully initialized WG comms
INFO:bench_cugraph_training:rank 173 successfully initialized WG comms
INFO:bench_cugraph_training:rank 510 successfully initialized WG comms
INFO:bench_cugraph_training:rank 511 successfully initialized WG comms
INFO:bench_cugraph_training:rank 508 successfully initialized WG comms
INFO:bench_cugraph_training:rank 509 successfully initialized WG comms
INFO:bench_cugraph_training:rank 507 successfully initialized WG comms
INFO:bench_cugraph_training:rank 504 successfully initialized WG comms
INFO:bench_cugraph_training:rank 505 successfully initialized WG comms
INFO:bench_cugraph_training:rank 506 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1003 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1001 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1002 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1000 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1006 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1005 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1007 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1004 successfully initialized WG comms
INFO:bench_cugraph_training:rank 443 successfully initialized WG comms
INFO:bench_cugraph_training:rank 441 successfully initialized WG comms
INFO:bench_cugraph_training:rank 440 successfully initialized WG comms
INFO:bench_cugraph_training:rank 442 successfully initialized WG comms
INFO:bench_cugraph_training:rank 444 successfully initialized WG comms
INFO:bench_cugraph_training:rank 446 successfully initialized WG comms
INFO:bench_cugraph_training:rank 447 successfully initialized WG comms
INFO:bench_cugraph_training:rank 445 successfully initialized WG comms
INFO:bench_cugraph_training:rank 623 successfully initialized WG comms
INFO:bench_cugraph_training:rank 622 successfully initialized WG comms
INFO:bench_cugraph_training:rank 620 successfully initialized WG comms
INFO:bench_cugraph_training:rank 621 successfully initialized WG comms
INFO:bench_cugraph_training:rank 616 successfully initialized WG comms
INFO:bench_cugraph_training:rank 617 successfully initialized WG comms
INFO:bench_cugraph_training:rank 619 successfully initialized WG comms
INFO:bench_cugraph_training:rank 618 successfully initialized WG comms
INFO:bench_cugraph_training:rank 14 successfully initialized WG comms
INFO:bench_cugraph_training:rank 12 successfully initialized WG comms
INFO:bench_cugraph_training:rank 13 successfully initialized WG comms
INFO:bench_cugraph_training:rank 15 successfully initialized WG comms
INFO:bench_cugraph_training:rank 9 successfully initialized WG comms
INFO:bench_cugraph_training:rank 8 successfully initialized WG comms
INFO:bench_cugraph_training:rank 10 successfully initialized WG comms
INFO:bench_cugraph_training:rank 11 successfully initialized WG comms
INFO:bench_cugraph_training:rank 965 successfully initialized WG comms
INFO:bench_cugraph_training:rank 966 successfully initialized WG comms
INFO:bench_cugraph_training:rank 967 successfully initialized WG comms
INFO:bench_cugraph_training:rank 964 successfully initialized WG comms
INFO:bench_cugraph_training:rank 962 successfully initialized WG comms
INFO:bench_cugraph_training:rank 961 successfully initialized WG comms
INFO:bench_cugraph_training:rank 960 successfully initialized WG comms
INFO:bench_cugraph_training:rank 963 successfully initialized WG comms
INFO:bench_cugraph_training:rank 552 successfully initialized WG comms
INFO:bench_cugraph_training:rank 555 successfully initialized WG comms
INFO:bench_cugraph_training:rank 554 successfully initialized WG comms
INFO:bench_cugraph_training:rank 553 successfully initialized WG comms
INFO:bench_cugraph_training:rank 556 successfully initialized WG comms
INFO:bench_cugraph_training:rank 557 successfully initialized WG comms
INFO:bench_cugraph_training:rank 558 successfully initialized WG comms
INFO:bench_cugraph_training:rank 559 successfully initialized WG comms
INFO:bench_cugraph_training:rank 862 successfully initialized WG comms
INFO:bench_cugraph_training:rank 860 successfully initialized WG comms
INFO:bench_cugraph_training:rank 863 successfully initialized WG comms
INFO:bench_cugraph_training:rank 861 successfully initialized WG comms
INFO:bench_cugraph_training:rank 858 successfully initialized WG comms
INFO:bench_cugraph_training:rank 859 successfully initialized WG comms
INFO:bench_cugraph_training:rank 857 successfully initialized WG comms
INFO:bench_cugraph_training:rank 856 successfully initialized WG comms
INFO:bench_cugraph_training:rank 578 successfully initialized WG comms
INFO:bench_cugraph_training:rank 577 successfully initialized WG comms
INFO:bench_cugraph_training:rank 576 successfully initialized WG comms
INFO:bench_cugraph_training:rank 579 successfully initialized WG comms
INFO:bench_cugraph_training:rank 580 successfully initialized WG comms
INFO:bench_cugraph_training:rank 582 successfully initialized WG comms
INFO:bench_cugraph_training:rank 581 successfully initialized WG comms
INFO:bench_cugraph_training:rank 583 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1 successfully initialized WG comms
INFO:bench_cugraph_training:rank 3 successfully initialized WG comms
INFO:bench_cugraph_training:rank 2 successfully initialized WG comms
INFO:bench_cugraph_training:rank 0 successfully initialized WG comms
INFO:bench_cugraph_training:rank 4 successfully initialized WG comms
INFO:bench_cugraph_training:rank 7 successfully initialized WG comms
INFO:bench_cugraph_training:rank 5 successfully initialized WG comms
INFO:bench_cugraph_training:rank 6 successfully initialized WG comms
INFO:bench_cugraph_training:rank 228 successfully initialized WG comms
INFO:bench_cugraph_training:rank 230 successfully initialized WG comms
INFO:bench_cugraph_training:rank 231 successfully initialized WG comms
INFO:bench_cugraph_training:rank 229 successfully initialized WG comms
INFO:bench_cugraph_training:rank 226 successfully initialized WG comms
INFO:bench_cugraph_training:rank 227 successfully initialized WG comms
INFO:bench_cugraph_training:rank 225 successfully initialized WG comms
INFO:bench_cugraph_training:rank 224 successfully initialized WG comms
INFO:bench_cugraph_training:rank 212 successfully initialized WG comms
INFO:bench_cugraph_training:rank 214 successfully initialized WG comms
INFO:bench_cugraph_training:rank 215 successfully initialized WG comms
INFO:bench_cugraph_training:rank 213 successfully initialized WG comms
INFO:bench_cugraph_training:rank 211 successfully initialized WG comms
INFO:bench_cugraph_training:rank 210 successfully initialized WG comms
INFO:bench_cugraph_training:rank 209 successfully initialized WG comms
INFO:bench_cugraph_training:rank 208 successfully initialized WG comms
INFO:bench_cugraph_training:rank 586 successfully initialized WG comms
INFO:bench_cugraph_training:rank 587 successfully initialized WG comms
INFO:bench_cugraph_training:rank 585 successfully initialized WG comms
INFO:bench_cugraph_training:rank 584 successfully initialized WG comms
INFO:bench_cugraph_training:rank 590 successfully initialized WG comms
INFO:bench_cugraph_training:rank 589 successfully initialized WG comms
INFO:bench_cugraph_training:rank 591 successfully initialized WG comms
INFO:bench_cugraph_training:rank 588 successfully initialized WG comms
INFO:bench_cugraph_training:rank 660 successfully initialized WG comms
INFO:bench_cugraph_training:rank 663 successfully initialized WG comms
INFO:bench_cugraph_training:rank 662 successfully initialized WG comms
INFO:bench_cugraph_training:rank 661 successfully initialized WG comms
INFO:bench_cugraph_training:rank 656 successfully initialized WG comms
INFO:bench_cugraph_training:rank 658 successfully initialized WG comms
INFO:bench_cugraph_training:rank 657 successfully initialized WG comms
INFO:bench_cugraph_training:rank 659 successfully initialized WG comms
INFO:bench_cugraph_training:rank 707 successfully initialized WG comms
INFO:bench_cugraph_training:rank 706 successfully initialized WG comms
INFO:bench_cugraph_training:rank 705 successfully initialized WG comms
INFO:bench_cugraph_training:rank 704 successfully initialized WG comms
INFO:bench_cugraph_training:rank 709 successfully initialized WG comms
INFO:bench_cugraph_training:rank 711 successfully initialized WG comms
INFO:bench_cugraph_training:rank 710 successfully initialized WG comms
INFO:bench_cugraph_training:rank 708 successfully initialized WG comms
INFO:bench_cugraph_training:rank 747 successfully initialized WG comms
INFO:bench_cugraph_training:rank 745 successfully initialized WG comms
INFO:bench_cugraph_training:rank 744 successfully initialized WG comms
INFO:bench_cugraph_training:rank 746 successfully initialized WG comms
INFO:bench_cugraph_training:rank 750 successfully initialized WG comms
INFO:bench_cugraph_training:rank 748 successfully initialized WG comms
INFO:bench_cugraph_training:rank 751 successfully initialized WG comms
INFO:bench_cugraph_training:rank 749 successfully initialized WG comms
INFO:bench_cugraph_training:rank 302 successfully initialized WG comms
INFO:bench_cugraph_training:rank 303 successfully initialized WG comms
INFO:bench_cugraph_training:rank 301 successfully initialized WG comms
INFO:bench_cugraph_training:rank 300 successfully initialized WG comms
INFO:bench_cugraph_training:rank 298 successfully initialized WG comms
INFO:bench_cugraph_training:rank 299 successfully initialized WG comms
INFO:bench_cugraph_training:rank 297 successfully initialized WG comms
INFO:bench_cugraph_training:rank 296 successfully initialized WG comms
INFO:bench_cugraph_training:rank 192 successfully initialized WG comms
INFO:bench_cugraph_training:rank 194 successfully initialized WG comms
INFO:bench_cugraph_training:rank 193 successfully initialized WG comms
INFO:bench_cugraph_training:rank 195 successfully initialized WG comms
INFO:bench_cugraph_training:rank 197 successfully initialized WG comms
INFO:bench_cugraph_training:rank 199 successfully initialized WG comms
INFO:bench_cugraph_training:rank 198 successfully initialized WG comms
INFO:bench_cugraph_training:rank 196 successfully initialized WG comms
INFO:bench_cugraph_training:rank 51 successfully initialized WG comms
INFO:bench_cugraph_training:rank 50 successfully initialized WG comms
INFO:bench_cugraph_training:rank 48 successfully initialized WG comms
INFO:bench_cugraph_training:rank 49 successfully initialized WG comms
INFO:bench_cugraph_training:rank 55 successfully initialized WG comms
INFO:bench_cugraph_training:rank 53 successfully initialized WG comms
INFO:bench_cugraph_training:rank 52 successfully initialized WG comms
INFO:bench_cugraph_training:rank 54 successfully initialized WG comms
INFO:bench_cugraph_training:rank 628 successfully initialized WG comms
INFO:bench_cugraph_training:rank 631 successfully initialized WG comms
INFO:bench_cugraph_training:rank 629 successfully initialized WG comms
INFO:bench_cugraph_training:rank 630 successfully initialized WG comms
INFO:bench_cugraph_training:rank 627 successfully initialized WG comms
INFO:bench_cugraph_training:rank 626 successfully initialized WG comms
INFO:bench_cugraph_training:rank 625 successfully initialized WG comms
INFO:bench_cugraph_training:rank 624 successfully initialized WG comms
INFO:bench_cugraph_training:rank 987 successfully initialized WG comms
INFO:bench_cugraph_training:rank 986 successfully initialized WG comms
INFO:bench_cugraph_training:rank 985 successfully initialized WG comms
INFO:bench_cugraph_training:rank 984 successfully initialized WG comms
INFO:bench_cugraph_training:rank 989 successfully initialized WG comms
INFO:bench_cugraph_training:rank 991 successfully initialized WG comms
INFO:bench_cugraph_training:rank 988 successfully initialized WG comms
INFO:bench_cugraph_training:rank 990 successfully initialized WG comms
INFO:bench_cugraph_training:rank 979 successfully initialized WG comms
INFO:bench_cugraph_training:rank 978 successfully initialized WG comms
INFO:bench_cugraph_training:rank 976 successfully initialized WG comms
INFO:bench_cugraph_training:rank 977 successfully initialized WG comms
INFO:bench_cugraph_training:rank 983 successfully initialized WG comms
INFO:bench_cugraph_training:rank 981 successfully initialized WG comms
INFO:bench_cugraph_training:rank 982 successfully initialized WG comms
INFO:bench_cugraph_training:rank 980 successfully initialized WG comms
INFO:bench_cugraph_training:rank 422 successfully initialized WG comms
INFO:bench_cugraph_training:rank 420 successfully initialized WG comms
INFO:bench_cugraph_training:rank 423 successfully initialized WG comms
INFO:bench_cugraph_training:rank 421 successfully initialized WG comms
INFO:bench_cugraph_training:rank 418 successfully initialized WG comms
INFO:bench_cugraph_training:rank 419 successfully initialized WG comms
INFO:bench_cugraph_training:rank 417 successfully initialized WG comms
INFO:bench_cugraph_training:rank 416 successfully initialized WG comms
INFO:bench_cugraph_training:rank 810 successfully initialized WG comms
INFO:bench_cugraph_training:rank 809 successfully initialized WG comms
INFO:bench_cugraph_training:rank 808 successfully initialized WG comms
INFO:bench_cugraph_training:rank 811 successfully initialized WG comms
INFO:bench_cugraph_training:rank 812 successfully initialized WG comms
INFO:bench_cugraph_training:rank 813 successfully initialized WG comms
INFO:bench_cugraph_training:rank 814 successfully initialized WG comms
INFO:bench_cugraph_training:rank 815 successfully initialized WG comms
INFO:bench_cugraph_training:rank 189 successfully initialized WG comms
INFO:bench_cugraph_training:rank 191 successfully initialized WG comms
INFO:bench_cugraph_training:rank 188 successfully initialized WG comms
INFO:bench_cugraph_training:rank 190 successfully initialized WG comms
INFO:bench_cugraph_training:rank 185 successfully initialized WG comms
INFO:bench_cugraph_training:rank 187 successfully initialized WG comms
INFO:bench_cugraph_training:rank 186 successfully initialized WG comms
INFO:bench_cugraph_training:rank 184 successfully initialized WG comms
INFO:bench_cugraph_training:rank 473 successfully initialized WG comms
INFO:bench_cugraph_training:rank 475 successfully initialized WG comms
INFO:bench_cugraph_training:rank 472 successfully initialized WG comms
INFO:bench_cugraph_training:rank 474 successfully initialized WG comms
INFO:bench_cugraph_training:rank 477 successfully initialized WG comms
INFO:bench_cugraph_training:rank 478 successfully initialized WG comms
INFO:bench_cugraph_training:rank 479 successfully initialized WG comms
INFO:bench_cugraph_training:rank 476 successfully initialized WG comms
INFO:bench_cugraph_training:rank 532 successfully initialized WG comms
INFO:bench_cugraph_training:rank 534 successfully initialized WG comms
INFO:bench_cugraph_training:rank 535 successfully initialized WG comms
INFO:bench_cugraph_training:rank 533 successfully initialized WG comms
INFO:bench_cugraph_training:rank 530 successfully initialized WG comms
INFO:bench_cugraph_training:rank 528 successfully initialized WG comms
INFO:bench_cugraph_training:rank 529 successfully initialized WG comms
INFO:bench_cugraph_training:rank 531 successfully initialized WG comms
INFO:bench_cugraph_training:rank 642 successfully initialized WG comms
INFO:bench_cugraph_training:rank 641 successfully initialized WG comms
INFO:bench_cugraph_training:rank 643 successfully initialized WG comms
INFO:bench_cugraph_training:rank 640 successfully initialized WG comms
INFO:bench_cugraph_training:rank 647 successfully initialized WG comms
INFO:bench_cugraph_training:rank 644 successfully initialized WG comms
INFO:bench_cugraph_training:rank 645 successfully initialized WG comms
INFO:bench_cugraph_training:rank 646 successfully initialized WG comms
INFO:bench_cugraph_training:rank 912 successfully initialized WG comms
INFO:bench_cugraph_training:rank 915 successfully initialized WG comms
INFO:bench_cugraph_training:rank 913 successfully initialized WG comms
INFO:bench_cugraph_training:rank 914 successfully initialized WG comms
INFO:bench_cugraph_training:rank 916 successfully initialized WG comms
INFO:bench_cugraph_training:rank 917 successfully initialized WG comms
INFO:bench_cugraph_training:rank 918 successfully initialized WG comms
INFO:bench_cugraph_training:rank 919 successfully initialized WG comms
INFO:bench_cugraph_training:rank 633 successfully initialized WG comms
INFO:bench_cugraph_training:rank 635 successfully initialized WG comms
INFO:bench_cugraph_training:rank 632 successfully initialized WG comms
INFO:bench_cugraph_training:rank 634 successfully initialized WG comms
INFO:bench_cugraph_training:rank 639 successfully initialized WG comms
INFO:bench_cugraph_training:rank 636 successfully initialized WG comms
INFO:bench_cugraph_training:rank 638 successfully initialized WG comms
INFO:bench_cugraph_training:rank 637 successfully initialized WG comms
INFO:bench_cugraph_training:rank 595 successfully initialized WG comms
INFO:bench_cugraph_training:rank 592 successfully initialized WG comms
INFO:bench_cugraph_training:rank 593 successfully initialized WG comms
INFO:bench_cugraph_training:rank 594 successfully initialized WG comms
INFO:bench_cugraph_training:rank 599 successfully initialized WG comms
INFO:bench_cugraph_training:rank 597 successfully initialized WG comms
INFO:bench_cugraph_training:rank 598 successfully initialized WG comms
INFO:bench_cugraph_training:rank 596 successfully initialized WG comms
INFO:bench_cugraph_training:rank 217 successfully initialized WG comms
INFO:bench_cugraph_training:rank 218 successfully initialized WG comms
INFO:bench_cugraph_training:rank 216 successfully initialized WG comms
INFO:bench_cugraph_training:rank 219 successfully initialized WG comms
INFO:bench_cugraph_training:rank 222 successfully initialized WG comms
INFO:bench_cugraph_training:rank 220 successfully initialized WG comms
INFO:bench_cugraph_training:rank 221 successfully initialized WG comms
INFO:bench_cugraph_training:rank 223 successfully initialized WG comms
INFO:bench_cugraph_training:rank 512 successfully initialized WG comms
INFO:bench_cugraph_training:rank 514 successfully initialized WG comms
INFO:bench_cugraph_training:rank 515 successfully initialized WG comms
INFO:bench_cugraph_training:rank 513 successfully initialized WG comms
INFO:bench_cugraph_training:rank 516 successfully initialized WG comms
INFO:bench_cugraph_training:rank 517 successfully initialized WG comms
INFO:bench_cugraph_training:rank 519 successfully initialized WG comms
INFO:bench_cugraph_training:rank 518 successfully initialized WG comms
INFO:bench_cugraph_training:rank 491 successfully initialized WG comms
INFO:bench_cugraph_training:rank 488 successfully initialized WG comms
INFO:bench_cugraph_training:rank 490 successfully initialized WG comms
INFO:bench_cugraph_training:rank 489 successfully initialized WG comms
INFO:bench_cugraph_training:rank 494 successfully initialized WG comms
INFO:bench_cugraph_training:rank 493 successfully initialized WG comms
INFO:bench_cugraph_training:rank 492 successfully initialized WG comms
INFO:bench_cugraph_training:rank 495 successfully initialized WG comms
INFO:bench_cugraph_training:rank 469 successfully initialized WG comms
INFO:bench_cugraph_training:rank 471 successfully initialized WG comms
INFO:bench_cugraph_training:rank 468 successfully initialized WG comms
INFO:bench_cugraph_training:rank 470 successfully initialized WG comms
INFO:bench_cugraph_training:rank 466 successfully initialized WG comms
INFO:bench_cugraph_training:rank 465 successfully initialized WG comms
INFO:bench_cugraph_training:rank 464 successfully initialized WG comms
INFO:bench_cugraph_training:rank 467 successfully initialized WG comms
INFO:bench_cugraph_training:rank 327 successfully initialized WG comms
INFO:bench_cugraph_training:rank 324 successfully initialized WG comms
INFO:bench_cugraph_training:rank 326 successfully initialized WG comms
INFO:bench_cugraph_training:rank 325 successfully initialized WG comms
INFO:bench_cugraph_training:rank 322 successfully initialized WG comms
INFO:bench_cugraph_training:rank 321 successfully initialized WG comms
INFO:bench_cugraph_training:rank 323 successfully initialized WG comms
INFO:bench_cugraph_training:rank 320 successfully initialized WG comms
INFO:bench_cugraph_training:rank 724 successfully initialized WG comms
INFO:bench_cugraph_training:rank 726 successfully initialized WG comms
INFO:bench_cugraph_training:rank 725 successfully initialized WG comms
INFO:bench_cugraph_training:rank 727 successfully initialized WG comms
INFO:bench_cugraph_training:rank 721 successfully initialized WG comms
INFO:bench_cugraph_training:rank 723 successfully initialized WG comms
INFO:bench_cugraph_training:rank 722 successfully initialized WG comms
INFO:bench_cugraph_training:rank 720 successfully initialized WG comms
INFO:bench_cugraph_training:rank 574 successfully initialized WG comms
INFO:bench_cugraph_training:rank 573 successfully initialized WG comms
INFO:bench_cugraph_training:rank 575 successfully initialized WG comms
INFO:bench_cugraph_training:rank 572 successfully initialized WG comms
INFO:bench_cugraph_training:rank 571 successfully initialized WG comms
INFO:bench_cugraph_training:rank 569 successfully initialized WG comms
INFO:bench_cugraph_training:rank 568 successfully initialized WG comms
INFO:bench_cugraph_training:rank 570 successfully initialized WG comms
INFO:bench_cugraph_training:rank 104 successfully initialized WG comms
INFO:bench_cugraph_training:rank 107 successfully initialized WG comms
INFO:bench_cugraph_training:rank 106 successfully initialized WG comms
INFO:bench_cugraph_training:rank 105 successfully initialized WG comms
INFO:bench_cugraph_training:rank 108 successfully initialized WG comms
INFO:bench_cugraph_training:rank 111 successfully initialized WG comms
INFO:bench_cugraph_training:rank 110 successfully initialized WG comms
INFO:bench_cugraph_training:rank 109 successfully initialized WG comms
INFO:bench_cugraph_training:rank 427 successfully initialized WG comms
INFO:bench_cugraph_training:rank 424 successfully initialized WG comms
INFO:bench_cugraph_training:rank 426 successfully initialized WG comms
INFO:bench_cugraph_training:rank 425 successfully initialized WG comms
INFO:bench_cugraph_training:rank 430 successfully initialized WG comms
INFO:bench_cugraph_training:rank 431 successfully initialized WG comms
INFO:bench_cugraph_training:rank 429 successfully initialized WG comms
INFO:bench_cugraph_training:rank 428 successfully initialized WG comms
INFO:bench_cugraph_training:rank 21 successfully initialized WG comms
INFO:bench_cugraph_training:rank 22 successfully initialized WG comms
INFO:bench_cugraph_training:rank 20 successfully initialized WG comms
INFO:bench_cugraph_training:rank 23 successfully initialized WG comms
INFO:bench_cugraph_training:rank 18 successfully initialized WG comms
INFO:bench_cugraph_training:rank 19 successfully initialized WG comms
INFO:bench_cugraph_training:rank 17 successfully initialized WG comms
INFO:bench_cugraph_training:rank 16 successfully initialized WG comms
INFO:bench_cugraph_training:rank 31 successfully initialized WG comms
INFO:bench_cugraph_training:rank 28 successfully initialized WG comms
INFO:bench_cugraph_training:rank 29 successfully initialized WG comms
INFO:bench_cugraph_training:rank 30 successfully initialized WG comms
INFO:bench_cugraph_training:rank 24 successfully initialized WG comms
INFO:bench_cugraph_training:rank 27 successfully initialized WG comms
INFO:bench_cugraph_training:rank 25 successfully initialized WG comms
INFO:bench_cugraph_training:rank 26 successfully initialized WG comms
INFO:bench_cugraph_training:rank 239 successfully initialized WG comms
INFO:bench_cugraph_training:rank 236 successfully initialized WG comms
INFO:bench_cugraph_training:rank 237 successfully initialized WG comms
INFO:bench_cugraph_training:rank 238 successfully initialized WG comms
INFO:bench_cugraph_training:rank 233 successfully initialized WG comms
INFO:bench_cugraph_training:rank 235 successfully initialized WG comms
INFO:bench_cugraph_training:rank 232 successfully initialized WG comms
INFO:bench_cugraph_training:rank 234 successfully initialized WG comms
INFO:bench_cugraph_training:rank 910 successfully initialized WG comms
INFO:bench_cugraph_training:rank 908 successfully initialized WG comms
INFO:bench_cugraph_training:rank 909 successfully initialized WG comms
INFO:bench_cugraph_training:rank 911 successfully initialized WG comms
INFO:bench_cugraph_training:rank 905 successfully initialized WG comms
INFO:bench_cugraph_training:rank 907 successfully initialized WG comms
INFO:bench_cugraph_training:rank 906 successfully initialized WG comms
INFO:bench_cugraph_training:rank 904 successfully initialized WG comms
INFO:bench_cugraph_training:rank 316 successfully initialized WG comms
INFO:bench_cugraph_training:rank 317 successfully initialized WG comms
INFO:bench_cugraph_training:rank 318 successfully initialized WG comms
INFO:bench_cugraph_training:rank 319 successfully initialized WG comms
INFO:bench_cugraph_training:rank 313 successfully initialized WG comms
INFO:bench_cugraph_training:rank 314 successfully initialized WG comms
INFO:bench_cugraph_training:rank 315 successfully initialized WG comms
INFO:bench_cugraph_training:rank 312 successfully initialized WG comms
INFO:bench_cugraph_training:rank 957 successfully initialized WG comms
INFO:bench_cugraph_training:rank 956 successfully initialized WG comms
INFO:bench_cugraph_training:rank 958 successfully initialized WG comms
INFO:bench_cugraph_training:rank 959 successfully initialized WG comms
INFO:bench_cugraph_training:rank 954 successfully initialized WG comms
INFO:bench_cugraph_training:rank 953 successfully initialized WG comms
INFO:bench_cugraph_training:rank 952 successfully initialized WG comms
INFO:bench_cugraph_training:rank 955 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1017 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1019 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1016 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1018 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1020 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1022 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1023 successfully initialized WG comms
INFO:bench_cugraph_training:rank 1021 successfully initialized WG comms
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:Creating model...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:creating trainer
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
INFO:PyGCuGraphTrainer:creating trainer
INFO:PyGCuGraphTrainer:Creating model...
INFO:PyGCuGraphTrainer:getting input features...
INFO:OGBNPapers100MDataset:Loading x into WG embedding...
Rank=732 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/105.bin size=56862697472, starting from offset=0.
Rank=732 done reading total 56862697472 bytes from needed files.
Rank=19 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/98.bin size=56862697472, starting from offset=0.
Rank=19 done reading total 56862697472 bytes from needed files.
Rank=876 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/543.bin size=56862697472, starting from offset=0.
Rank=876 done reading total 56862697472 bytes from needed files.
Rank=819 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/533.bin size=56862697472, starting from offset=0.
Rank=819 done reading total 56862697472 bytes from needed files.
Rank=315 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/860.bin size=56862697472, starting from offset=0.
Rank=315 done reading total 56862697472 bytes from needed files.
Rank=16 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/203.bin size=56862697472, starting from offset=0.
Rank=16 done reading total 56862697472 bytes from needed files.
Rank=995 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/786.bin size=56862697472, starting from offset=0.
Rank=995 done reading total 56862697472 bytes from needed files.
Rank=905 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/723.bin size=56862697472, starting from offset=0.
Rank=905 done reading total 56862697472 bytes from needed files.
Rank=878 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/843.bin size=56862697472, starting from offset=0.
Rank=878 done reading total 56862697472 bytes from needed files.
Rank=143 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/127.bin size=56862697472, starting from offset=0.
Rank=143 done reading total 56862697472 bytes from needed files.
Rank=404 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/724.bin size=56862697472, starting from offset=0.
Rank=404 done reading total 56862697472 bytes from needed files.
Rank=835 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/331.bin size=56862697472, starting from offset=0.
Rank=835 done reading total 56862697472 bytes from needed files.
Rank=879 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/940.bin size=56862697472, starting from offset=0.
Rank=879 done reading total 56862697472 bytes from needed files.
Rank=651 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/184.bin size=56862697472, starting from offset=0.
Rank=651 done reading total 56862697472 bytes from needed files.
Rank=818 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/701.bin size=56862697472, starting from offset=0.
Rank=818 done reading total 56862697472 bytes from needed files.
Rank=314 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/787.bin size=56862697472, starting from offset=0.
Rank=314 done reading total 56862697472 bytes from needed files.
Rank=735 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/443.bin size=56862697472, starting from offset=0.
Rank=735 done reading total 56862697472 bytes from needed files.
Rank=88 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/776.bin size=56862697472, starting from offset=0.
Rank=88 done reading total 56862697472 bytes from needed files.
Rank=42 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/919.bin size=56862697472, starting from offset=0.
Rank=42 done reading total 56862697472 bytes from needed files.
Rank=639 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/737.bin size=56862697472, starting from offset=0.
Rank=639 done reading total 56862697472 bytes from needed files.
Rank=637 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/339.bin size=56862697472, starting from offset=0.
Rank=637 done reading total 56862697472 bytes from needed files.
Rank=442 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/439.bin size=56862697472, starting from offset=0.
Rank=442 done reading total 56862697472 bytes from needed files.
Rank=412 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/792.bin size=56862697472, starting from offset=0.
Rank=412 done reading total 56862697472 bytes from needed files.
Rank=375 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/270.bin size=56862697472, starting from offset=0.
Rank=375 done reading total 56862697472 bytes from needed files.
Rank=363 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/541.bin size=56862697472, starting from offset=0.
Rank=363 done reading total 56862697472 bytes from needed files.
Rank=576 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/112.bin size=56862697472, starting from offset=0.
Rank=576 done reading total 56862697472 bytes from needed files.
Rank=142 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/121.bin size=56862697472, starting from offset=0.
Rank=142 done reading total 56862697472 bytes from needed files.
Rank=290 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/79.bin size=56862697472, starting from offset=0.
Rank=290 done reading total 56862697472 bytes from needed files.
Rank=443 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/790.bin size=56862697472, starting from offset=0.
Rank=443 done reading total 56862697472 bytes from needed files.
Rank=723 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/407.bin size=56862697472, starting from offset=0.
Rank=723 done reading total 56862697472 bytes from needed files.
Rank=994 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/239.bin size=56862697472, starting from offset=0.
Rank=994 done reading total 56862697472 bytes from needed files.
Rank=721 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/978.bin size=56862697472, starting from offset=0.
Rank=721 done reading total 56862697472 bytes from needed files.
Rank=509 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/849.bin size=56862697472, starting from offset=0.
Rank=509 done reading total 56862697472 bytes from needed files.
Rank=405 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/688.bin size=56862697472, starting from offset=0.
Rank=405 done reading total 56862697472 bytes from needed files.
Rank=413 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/891.bin size=56862697472, starting from offset=0.
Rank=413 done reading total 56862697472 bytes from needed files.
Rank=406 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/988.bin size=56862697472, starting from offset=0.
Rank=406 done reading total 56862697472 bytes from needed files.
Rank=468 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/167.bin size=56862697472, starting from offset=0.
Rank=468 done reading total 56862697472 bytes from needed files.
Rank=753 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/73.bin size=56862697472, starting from offset=0.
Rank=753 done reading total 56862697472 bytes from needed files.
Rank=101 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/769.bin size=56862697472, starting from offset=0.
Rank=101 done reading total 56862697472 bytes from needed files.
Rank=360 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/559.bin size=56862697472, starting from offset=0.
Rank=360 done reading total 56862697472 bytes from needed files.
Rank=40 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/879.bin size=56862697472, starting from offset=0.
Rank=40 done reading total 56862697472 bytes from needed files.
Rank=384 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/136.bin size=56862697472, starting from offset=0.
Rank=384 done reading total 56862697472 bytes from needed files.
Rank=362 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/865.bin size=56862697472, starting from offset=0.
Rank=362 done reading total 56862697472 bytes from needed files.
Rank=727 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/971.bin size=56862697472, starting from offset=0.
Rank=727 done reading total 56862697472 bytes from needed files.
Rank=697 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/376.bin size=56862697472, starting from offset=0.
Rank=697 done reading total 56862697472 bytes from needed files.
Rank=241 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/338.bin size=56862697472, starting from offset=0.
Rank=241 done reading total 56862697472 bytes from needed files.
Rank=816 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/255.bin size=56862697472, starting from offset=0.
Rank=816 done reading total 56862697472 bytes from needed files.
Rank=141 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/568.bin size=56862697472, starting from offset=0.
Rank=141 done reading total 56862697472 bytes from needed files.
Rank=983 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/760.bin size=56862697472, starting from offset=0.
Rank=983 done reading total 56862697472 bytes from needed files.
Rank=522 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/162.bin size=56862697472, starting from offset=0.
Rank=522 done reading total 56862697472 bytes from needed files.
Rank=777 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/348.bin size=56862697472, starting from offset=0.
Rank=777 done reading total 56862697472 bytes from needed files.
Rank=982 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/511.bin size=56862697472, starting from offset=0.
Rank=982 done reading total 56862697472 bytes from needed files.
Rank=470 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/567.bin size=56862697472, starting from offset=0.
Rank=470 done reading total 56862697472 bytes from needed files.
Rank=577 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/793.bin size=56862697472, starting from offset=0.
Rank=577 done reading total 56862697472 bytes from needed files.
Rank=535 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/955.bin size=56862697472, starting from offset=0.
Rank=535 done reading total 56862697472 bytes from needed files.
Rank=305 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/102.bin size=56862697472, starting from offset=0.
Rank=305 done reading total 56862697472 bytes from needed files.
Rank=313 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/880.bin size=56862697472, starting from offset=0.
Rank=313 done reading total 56862697472 bytes from needed files.
Rank=453 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/288.bin size=56862697472, starting from offset=0.
Rank=453 done reading total 56862697472 bytes from needed files.
Rank=414 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/386.bin size=56862697472, starting from offset=0.
Rank=414 done reading total 56862697472 bytes from needed files.
Rank=372 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/155.bin size=56862697472, starting from offset=0.
Rank=372 done reading total 56862697472 bytes from needed files.
Rank=256 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/557.bin size=56862697472, starting from offset=0.
Rank=256 done reading total 56862697472 bytes from needed files.
Rank=755 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/342.bin size=56862697472, starting from offset=0.
Rank=755 done reading total 56862697472 bytes from needed files.
Rank=992 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/878.bin size=56862697472, starting from offset=0.
Rank=992 done reading total 56862697472 bytes from needed files.
Rank=912 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/299.bin size=56862697472, starting from offset=0.
Rank=912 done reading total 56862697472 bytes from needed files.
Rank=38 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/834.bin size=56862697472, starting from offset=0.
Rank=38 done reading total 56862697472 bytes from needed files.
Rank=43 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/486.bin size=56862697472, starting from offset=0.
Rank=43 done reading total 56862697472 bytes from needed files.
Rank=671 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/838.bin size=56862697472, starting from offset=0.
Rank=671 done reading total 56862697472 bytes from needed files.
Rank=471 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/753.bin size=56862697472, starting from offset=0.
Rank=471 done reading total 56862697472 bytes from needed files.
Rank=395 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/935.bin size=56862697472, starting from offset=0.
Rank=395 done reading total 56862697472 bytes from needed files.
Rank=9 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/37.bin size=56862697472, starting from offset=0.
Rank=9 done reading total 56862697472 bytes from needed files.
Rank=455 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/404.bin size=56862697472, starting from offset=0.
Rank=455 done reading total 56862697472 bytes from needed files.
Rank=41 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/853.bin size=56862697472, starting from offset=0.
Rank=41 done reading total 56862697472 bytes from needed files.
Rank=953 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/884.bin size=56862697472, starting from offset=0.
Rank=953 done reading total 56862697472 bytes from needed files.
Rank=909 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/492.bin size=56862697472, starting from offset=0.
Rank=909 done reading total 56862697472 bytes from needed files.
Rank=636 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/418.bin size=56862697472, starting from offset=0.
Rank=636 done reading total 56862697472 bytes from needed files.
Rank=469 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/785.bin size=56862697472, starting from offset=0.
Rank=469 done reading total 56862697472 bytes from needed files.
Rank=440 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/609.bin size=56862697472, starting from offset=0.
Rank=440 done reading total 56862697472 bytes from needed files.
Rank=960 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/36.bin size=56862697472, starting from offset=0.
Rank=960 done reading total 56862697472 bytes from needed files.
Rank=72 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/237.bin size=56862697472, starting from offset=0.
Rank=72 done reading total 56862697472 bytes from needed files.
Rank=374 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/742.bin size=56862697472, starting from offset=0.
Rank=374 done reading total 56862697472 bytes from needed files.
Rank=387 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/561.bin size=56862697472, starting from offset=0.
Rank=387 done reading total 56862697472 bytes from needed files.
Rank=886 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/778.bin size=56862697472, starting from offset=0.
Rank=886 done reading total 56862697472 bytes from needed files.
Rank=325 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/356.bin size=56862697472, starting from offset=0.
Rank=325 done reading total 56862697472 bytes from needed files.
Rank=285 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/700.bin size=56862697472, starting from offset=0.
Rank=285 done reading total 56862697472 bytes from needed files.
Rank=667 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/357.bin size=56862697472, starting from offset=0.
Rank=667 done reading total 56862697472 bytes from needed files.
Rank=980 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/858.bin size=56862697472, starting from offset=0.
Rank=980 done reading total 56862697472 bytes from needed files.
Rank=452 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/327.bin size=56862697472, starting from offset=0.
Rank=452 done reading total 56862697472 bytes from needed files.
Rank=743 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/643.bin size=56862697472, starting from offset=0.
Rank=743 done reading total 56862697472 bytes from needed files.
Rank=831 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/553.bin size=56862697472, starting from offset=0.
Rank=831 done reading total 56862697472 bytes from needed files.
Rank=80 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/552.bin size=56862697472, starting from offset=0.
Rank=80 done reading total 56862697472 bytes from needed files.
Rank=832 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/687.bin size=56862697472, starting from offset=0.
Rank=832 done reading total 56862697472 bytes from needed files.
Rank=62 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/985.bin size=56862697472, starting from offset=0.
Rank=62 done reading total 56862697472 bytes from needed files.
Rank=776 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/362.bin size=56862697472, starting from offset=0.
Rank=776 done reading total 56862697472 bytes from needed files.
Rank=189 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/312.bin size=56862697472, starting from offset=0.
Rank=189 done reading total 56862697472 bytes from needed files.
Rank=619 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/154.bin size=56862697472, starting from offset=0.
Rank=619 done reading total 56862697472 bytes from needed files.
Rank=415 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/219.bin size=56862697472, starting from offset=0.
Rank=415 done reading total 56862697472 bytes from needed files.
Rank=240 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/885.bin size=56862697472, starting from offset=0.
Rank=240 done reading total 56862697472 bytes from needed files.
Rank=900 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/956.bin size=56862697472, starting from offset=0.
Rank=900 done reading total 56862697472 bytes from needed files.
Rank=890 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/179.bin size=56862697472, starting from offset=0.
Rank=890 done reading total 56862697472 bytes from needed files.
Rank=155 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/714.bin size=56862697472, starting from offset=0.
Rank=155 done reading total 56862697472 bytes from needed files.
Rank=906 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/974.bin size=56862697472, starting from offset=0.
Rank=906 done reading total 56862697472 bytes from needed files.
Rank=885 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/133.bin size=56862697472, starting from offset=0.
Rank=885 done reading total 56862697472 bytes from needed files.
Rank=954 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/76.bin size=56862697472, starting from offset=0.
Rank=954 done reading total 56862697472 bytes from needed files.
Rank=841 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/15.bin size=56862697472, starting from offset=0.
Rank=841 done reading total 56862697472 bytes from needed files.
Rank=394 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/201.bin size=56862697472, starting from offset=0.
Rank=394 done reading total 56862697472 bytes from needed files.
Rank=699 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/212.bin size=56862697472, starting from offset=0.
Rank=699 done reading total 56862697472 bytes from needed files.
Rank=692 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/788.bin size=56862697472, starting from offset=0.
Rank=692 done reading total 56862697472 bytes from needed files.
Rank=971 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/547.bin size=56862697472, starting from offset=0.
Rank=971 done reading total 56862697472 bytes from needed files.
Rank=523 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/346.bin size=56862697472, starting from offset=0.
Rank=523 done reading total 56862697472 bytes from needed files.
Rank=666 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/770.bin size=56862697472, starting from offset=0.
Rank=666 done reading total 56862697472 bytes from needed files.
Rank=833 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/378.bin size=56862697472, starting from offset=0.
Rank=833 done reading total 56862697472 bytes from needed files.
Rank=952 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/296.bin size=56862697472, starting from offset=0.
Rank=952 done reading total 56862697472 bytes from needed files.
Rank=521 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/524.bin size=56862697472, starting from offset=0.
Rank=521 done reading total 56862697472 bytes from needed files.
Rank=698 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/928.bin size=56862697472, starting from offset=0.
Rank=698 done reading total 56862697472 bytes from needed files.
Rank=144 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/812.bin size=56862697472, starting from offset=0.
Rank=144 done reading total 56862697472 bytes from needed files.
Rank=220 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/19.bin size=56862697472, starting from offset=0.
Rank=220 done reading total 56862697472 bytes from needed files.
Rank=904 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/344.bin size=56862697472, starting from offset=0.
Rank=904 done reading total 56862697472 bytes from needed files.
Rank=90 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/703.bin size=56862697472, starting from offset=0.
Rank=90 done reading total 56862697472 bytes from needed files.
Rank=742 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/772.bin size=56862697472, starting from offset=0.
Rank=742 done reading total 56862697472 bytes from needed files.
Rank=145 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/146.bin size=56862697472, starting from offset=0.
Rank=145 done reading total 56862697472 bytes from needed files.
Rank=631 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/285.bin size=56862697472, starting from offset=0.
Rank=631 done reading total 56862697472 bytes from needed files.
Rank=32 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/783.bin size=56862697472, starting from offset=0.
Rank=32 done reading total 56862697472 bytes from needed files.
Rank=500 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/638.bin size=56862697472, starting from offset=0.
Rank=500 done reading total 56862697472 bytes from needed files.
Rank=386 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/927.bin size=56862697472, starting from offset=0.
Rank=386 done reading total 56862697472 bytes from needed files.
Rank=779 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/62.bin size=56862697472, starting from offset=0.
Rank=779 done reading total 56862697472 bytes from needed files.
Rank=836 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/463.bin size=56862697472, starting from offset=0.
Rank=836 done reading total 56862697472 bytes from needed files.
Rank=180 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/275.bin size=56862697472, starting from offset=0.
Rank=180 done reading total 56862697472 bytes from needed files.
Rank=89 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/175.bin size=56862697472, starting from offset=0.
Rank=89 done reading total 56862697472 bytes from needed files.
Rank=242 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/280.bin size=56862697472, starting from offset=0.
Rank=242 done reading total 56862697472 bytes from needed files.
Rank=265 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/947.bin size=56862697472, starting from offset=0.
Rank=265 done reading total 56862697472 bytes from needed files.
Rank=588 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/320.bin size=56862697472, starting from offset=0.
Rank=588 done reading total 56862697472 bytes from needed files.
Rank=740 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/719.bin size=56862697472, starting from offset=0.
Rank=740 done reading total 56862697472 bytes from needed files.
Rank=955 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/276.bin size=56862697472, starting from offset=0.
Rank=955 done reading total 56862697472 bytes from needed files.
Rank=483 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/194.bin size=56862697472, starting from offset=0.
Rank=483 done reading total 56862697472 bytes from needed files.
Rank=937 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/368.bin size=56862697472, starting from offset=0.
Rank=937 done reading total 56862697472 bytes from needed files.
Rank=73 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/904.bin size=56862697472, starting from offset=0.
Rank=73 done reading total 56862697472 bytes from needed files.
Rank=339 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/837.bin size=56862697472, starting from offset=0.
Rank=339 done reading total 56862697472 bytes from needed files.
Rank=56 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/387.bin size=56862697472, starting from offset=0.
Rank=56 done reading total 56862697472 bytes from needed files.
Rank=704 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/926.bin size=56862697472, starting from offset=0.
Rank=704 done reading total 56862697472 bytes from needed files.
Rank=545 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1021.bin size=56862697472, starting from offset=0.
Rank=545 done reading total 56862697472 bytes from needed files.
Rank=581 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/647.bin size=56862697472, starting from offset=0.
Rank=581 done reading total 56862697472 bytes from needed files.
Rank=981 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1008.bin size=56862697472, starting from offset=0.
Rank=981 done reading total 56862697472 bytes from needed files.
Rank=291 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/968.bin size=56862697472, starting from offset=0.
Rank=291 done reading total 56862697472 bytes from needed files.
Rank=696 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/453.bin size=56862697472, starting from offset=0.
Rank=696 done reading total 56862697472 bytes from needed files.
Rank=708 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/942.bin size=56862697472, starting from offset=0.
Rank=708 done reading total 56862697472 bytes from needed files.
Rank=100 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/423.bin size=56862697472, starting from offset=0.
Rank=100 done reading total 56862697472 bytes from needed files.
Rank=122 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/725.bin size=56862697472, starting from offset=0.
Rank=122 done reading total 56862697472 bytes from needed files.
Rank=94 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/527.bin size=56862697472, starting from offset=0.
Rank=94 done reading total 56862697472 bytes from needed files.
Rank=911 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/496.bin size=56862697472, starting from offset=0.
Rank=911 done reading total 56862697472 bytes from needed files.
Rank=63 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/841.bin size=56862697472, starting from offset=0.
Rank=63 done reading total 56862697472 bytes from needed files.
Rank=533 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/998.bin size=56862697472, starting from offset=0.
Rank=533 done reading total 56862697472 bytes from needed files.
Rank=705 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/535.bin size=56862697472, starting from offset=0.
Rank=705 done reading total 56862697472 bytes from needed files.
Rank=617 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/836.bin size=56862697472, starting from offset=0.
Rank=617 done reading total 56862697472 bytes from needed files.
Rank=839 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/75.bin size=56862697472, starting from offset=0.
Rank=839 done reading total 56862697472 bytes from needed files.
Rank=693 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/848.bin size=56862697472, starting from offset=0.
Rank=693 done reading total 56862697472 bytes from needed files.
Rank=903 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/991.bin size=56862697472, starting from offset=0.
Rank=903 done reading total 56862697472 bytes from needed files.
Rank=629 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/295.bin size=56862697472, starting from offset=0.
Rank=629 done reading total 56862697472 bytes from needed files.
Rank=346 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/512.bin size=56862697472, starting from offset=0.
Rank=346 done reading total 56862697472 bytes from needed files.
Rank=259 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/420.bin size=56862697472, starting from offset=0.
Rank=259 done reading total 56862697472 bytes from needed files.
Rank=287 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/637.bin size=56862697472, starting from offset=0.
Rank=287 done reading total 56862697472 bytes from needed files.
Rank=36 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/243.bin size=56862697472, starting from offset=0.
Rank=36 done reading total 56862697472 bytes from needed files.
Rank=193 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/126.bin size=56862697472, starting from offset=0.
Rank=193 done reading total 56862697472 bytes from needed files.
Rank=252 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/992.bin size=56862697472, starting from offset=0.
Rank=252 done reading total 56862697472 bytes from needed files.
Rank=711 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/21.bin size=56862697472, starting from offset=0.
Rank=711 done reading total 56862697472 bytes from needed files.
Rank=648 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/429.bin size=56862697472, starting from offset=0.
Rank=648 done reading total 56862697472 bytes from needed files.
Rank=74 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/269.bin size=56862697472, starting from offset=0.
Rank=74 done reading total 56862697472 bytes from needed files.
Rank=748 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/954.bin size=56862697472, starting from offset=0.
Rank=748 done reading total 56862697472 bytes from needed files.
Rank=30 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/350.bin size=56862697472, starting from offset=0.
Rank=30 done reading total 56862697472 bytes from needed files.
Rank=152 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/830.bin size=56862697472, starting from offset=0.
Rank=152 done reading total 56862697472 bytes from needed files.
Rank=670 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/116.bin size=56862697472, starting from offset=0.
Rank=670 done reading total 56862697472 bytes from needed files.
Rank=224 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/355.bin size=56862697472, starting from offset=0.
Rank=224 done reading total 56862697472 bytes from needed files.
Rank=253 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/850.bin size=56862697472, starting from offset=0.
Rank=253 done reading total 56862697472 bytes from needed files.
Rank=706 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/361.bin size=56862697472, starting from offset=0.
Rank=706 done reading total 56862697472 bytes from needed files.
Rank=949 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/746.bin size=56862697472, starting from offset=0.
Rank=949 done reading total 56862697472 bytes from needed files.
Rank=158 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/645.bin size=56862697472, starting from offset=0.
Rank=158 done reading total 56862697472 bytes from needed files.
Rank=226 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/916.bin size=56862697472, starting from offset=0.
Rank=345 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/481.bin size=56862697472, starting from offset=0.
Rank=226 done reading total 56862697472 bytes from needed files.
Rank=345 done reading total 56862697472 bytes from needed files.
Rank=78 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/180.bin size=56862697472, starting from offset=0.
Rank=78 done reading total 56862697472 bytes from needed files.
Rank=288 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/780.bin size=56862697472, starting from offset=0.
Rank=288 done reading total 56862697472 bytes from needed files.
Rank=913 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/820.bin size=56862697472, starting from offset=0.
Rank=913 done reading total 56862697472 bytes from needed files.
Rank=1003 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/74.bin size=56862697472, starting from offset=0.
Rank=1003 done reading total 56862697472 bytes from needed files.
Rank=103 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/977.bin size=56862697472, starting from offset=0.
Rank=103 done reading total 56862697472 bytes from needed files.
Rank=649 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/385.bin size=56862697472, starting from offset=0.
Rank=649 done reading total 56862697472 bytes from needed files.
Rank=258 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/536.bin size=56862697472, starting from offset=0.
Rank=258 done reading total 56862697472 bytes from needed files.
Rank=585 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/246.bin size=56862697472, starting from offset=0.
Rank=585 done reading total 56862697472 bytes from needed files.
Rank=1007 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/213.bin size=56862697472, starting from offset=0.
Rank=1007 done reading total 56862697472 bytes from needed files.
Rank=102 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/210.bin size=56862697472, starting from offset=0.
Rank=102 done reading total 56862697472 bytes from needed files.
Rank=902 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/271.bin size=56862697472, starting from offset=0.
Rank=902 done reading total 56862697472 bytes from needed files.
Rank=951 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/432.bin size=56862697472, starting from offset=0.
Rank=951 done reading total 56862697472 bytes from needed files.
Rank=267 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/415.bin size=56862697472, starting from offset=0.
Rank=267 done reading total 56862697472 bytes from needed files.
Rank=840 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/715.bin size=56862697472, starting from offset=0.
Rank=840 done reading total 56862697472 bytes from needed files.
Rank=499 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/708.bin size=56862697472, starting from offset=0.
Rank=499 done reading total 56862697472 bytes from needed files.
Rank=838 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/750.bin size=56862697472, starting from offset=0.
Rank=838 done reading total 56862697472 bytes from needed files.
Rank=351 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/90.bin size=56862697472, starting from offset=0.
Rank=351 done reading total 56862697472 bytes from needed files.
Rank=343 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/188.bin size=56862697472, starting from offset=0.
Rank=343 done reading total 56862697472 bytes from needed files.
Rank=741 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/738.bin size=56862697472, starting from offset=0.
Rank=741 done reading total 56862697472 bytes from needed files.
Rank=700 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/183.bin size=56862697472, starting from offset=0.
Rank=700 done reading total 56862697472 bytes from needed files.
Rank=709 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/804.bin size=56862697472, starting from offset=0.
Rank=709 done reading total 56862697472 bytes from needed files.
Rank=694 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/52.bin size=56862697472, starting from offset=0.
Rank=694 done reading total 56862697472 bytes from needed files.
Rank=650 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/99.bin size=56862697472, starting from offset=0.
Rank=650 done reading total 56862697472 bytes from needed files.
Rank=571 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/519.bin size=56862697472, starting from offset=0.
Rank=571 done reading total 56862697472 bytes from needed files.
Rank=908 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/983.bin size=56862697472, starting from offset=0.
Rank=908 done reading total 56862697472 bytes from needed files.
Rank=907 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/847.bin size=56862697472, starting from offset=0.
Rank=907 done reading total 56862697472 bytes from needed files.
Rank=55 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/461.bin size=56862697472, starting from offset=0.
Rank=55 done reading total 56862697472 bytes from needed files.
Rank=558 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/970.bin size=56862697472, starting from offset=0.
Rank=558 done reading total 56862697472 bytes from needed files.
Rank=710 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/520.bin size=56862697472, starting from offset=0.
Rank=710 done reading total 56862697472 bytes from needed files.
Rank=568 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/767.bin size=56862697472, starting from offset=0.
Rank=568 done reading total 56862697472 bytes from needed files.
Rank=514 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/250.bin size=56862697472, starting from offset=0.
Rank=514 done reading total 56862697472 bytes from needed files.
Rank=752 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/178.bin size=56862697472, starting from offset=0.
Rank=752 done reading total 56862697472 bytes from needed files.
Rank=901 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/762.bin size=56862697472, starting from offset=0.
Rank=901 done reading total 56862697472 bytes from needed files.
Rank=596 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/87.bin size=56862697472, starting from offset=0.
Rank=596 done reading total 56862697472 bytes from needed files.
Rank=628 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/132.bin size=56862697472, starting from offset=0.
Rank=628 done reading total 56862697472 bytes from needed files.
Rank=630 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/877.bin size=56862697472, starting from offset=0.
Rank=630 done reading total 56862697472 bytes from needed files.
Rank=510 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/963.bin size=56862697472, starting from offset=0.
Rank=510 done reading total 56862697472 bytes from needed files.
Rank=497 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/332.bin size=56862697472, starting from offset=0.
Rank=497 done reading total 56862697472 bytes from needed files.
Rank=284 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/911.bin size=56862697472, starting from offset=0.
Rank=284 done reading total 56862697472 bytes from needed files.
Rank=286 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/434.bin size=56862697472, starting from offset=0.
Rank=286 done reading total 56862697472 bytes from needed files.
Rank=502 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/549.bin size=56862697472, starting from offset=0.
Rank=502 done reading total 56862697472 bytes from needed files.
Rank=190 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/923.bin size=56862697472, starting from offset=0.
Rank=190 done reading total 56862697472 bytes from needed files.
Rank=79 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1023.bin size=56862697472, starting from offset=0.
Rank=79 done reading total 56862697472 bytes from needed files.
Rank=289 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/639.bin size=56862697472, starting from offset=0.
Rank=289 done reading total 56862697472 bytes from needed files.
Rank=616 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/2.bin size=56862697472, starting from offset=0.
Rank=616 done reading total 56862697472 bytes from needed files.
Rank=939 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/424.bin size=56862697472, starting from offset=0.
Rank=939 done reading total 56862697472 bytes from needed files.
Rank=943 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/773.bin size=56862697472, starting from offset=0.
Rank=943 done reading total 56862697472 bytes from needed files.
Rank=511 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/894.bin size=56862697472, starting from offset=0.
Rank=511 done reading total 56862697472 bytes from needed files.
Rank=969 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/662.bin size=56862697472, starting from offset=0.
Rank=969 done reading total 56862697472 bytes from needed files.
Rank=75 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/120.bin size=56862697472, starting from offset=0.
Rank=75 done reading total 56862697472 bytes from needed files.
Rank=669 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1004.bin size=56862697472, starting from offset=0.
Rank=669 done reading total 56862697472 bytes from needed files.
Rank=183 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/591.bin size=56862697472, starting from offset=0.
Rank=183 done reading total 56862697472 bytes from needed files.
Rank=484 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/690.bin size=56862697472, starting from offset=0.
Rank=484 done reading total 56862697472 bytes from needed files.
Rank=114 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/103.bin size=56862697472, starting from offset=0.
Rank=114 done reading total 56862697472 bytes from needed files.
Rank=225 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/100.bin size=56862697472, starting from offset=0.
Rank=225 done reading total 56862697472 bytes from needed files.
Rank=707 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/525.bin size=56862697472, starting from offset=0.
Rank=707 done reading total 56862697472 bytes from needed files.
Rank=257 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/696.bin size=56862697472, starting from offset=0.
Rank=257 done reading total 56862697472 bytes from needed files.
Rank=57 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/150.bin size=56862697472, starting from offset=0.
Rank=57 done reading total 56862697472 bytes from needed files.
Rank=239 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/869.bin size=56862697472, starting from offset=0.
Rank=239 done reading total 56862697472 bytes from needed files.
Rank=153 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/142.bin size=56862697472, starting from offset=0.
Rank=153 done reading total 56862697472 bytes from needed files.
Rank=837 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/431.bin size=56862697472, starting from offset=0.
Rank=837 done reading total 56862697472 bytes from needed files.
Rank=915 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/667.bin size=56862697472, starting from offset=0.
Rank=915 done reading total 56862697472 bytes from needed files.
Rank=809 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/88.bin size=56862697472, starting from offset=0.
Rank=809 done reading total 56862697472 bytes from needed files.
Rank=344 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/886.bin size=56862697472, starting from offset=0.
Rank=344 done reading total 56862697472 bytes from needed files.
Rank=164 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/135.bin size=56862697472, starting from offset=0.
Rank=164 done reading total 56862697472 bytes from needed files.
Rank=120 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/54.bin size=56862697472, starting from offset=0.
Rank=120 done reading total 56862697472 bytes from needed files.
Rank=486 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/826.bin size=56862697472, starting from offset=0.
Rank=486 done reading total 56862697472 bytes from needed files.
Rank=249 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/454.bin size=56862697472, starting from offset=0.
Rank=249 done reading total 56862697472 bytes from needed files.
Rank=98 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/336.bin size=56862697472, starting from offset=0.
Rank=98 done reading total 56862697472 bytes from needed files.
Rank=914 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/58.bin size=56862697472, starting from offset=0.
Rank=914 done reading total 56862697472 bytes from needed files.
Rank=188 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/619.bin size=56862697472, starting from offset=0.
Rank=188 done reading total 56862697472 bytes from needed files.
Rank=254 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/718.bin size=56862697472, starting from offset=0.
Rank=254 done reading total 56862697472 bytes from needed files.
Rank=503 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/592.bin size=56862697472, starting from offset=0.
Rank=503 done reading total 56862697472 bytes from needed files.
Rank=295 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/979.bin size=56862697472, starting from offset=0.
Rank=295 done reading total 56862697472 bytes from needed files.
Rank=888 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/674.bin size=56862697472, starting from offset=0.
Rank=888 done reading total 56862697472 bytes from needed files.
Rank=565 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/125.bin size=56862697472, starting from offset=0.
Rank=565 done reading total 56862697472 bytes from needed files.
Rank=968 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/483.bin size=56862697472, starting from offset=0.
Rank=968 done reading total 56862697472 bytes from needed files.
Rank=772 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/163.bin size=56862697472, starting from offset=0.
Rank=772 done reading total 56862697472 bytes from needed files.
Rank=481 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/648.bin size=56862697472, starting from offset=0.
Rank=481 done reading total 56862697472 bytes from needed files.
Rank=534 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/515.bin size=56862697472, starting from offset=0.
Rank=534 done reading total 56862697472 bytes from needed files.
Rank=850 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/414.bin size=56862697472, starting from offset=0.
Rank=850 done reading total 56862697472 bytes from needed files.
Rank=849 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/910.bin size=56862697472, starting from offset=0.
Rank=828 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/391.bin size=56862697472, starting from offset=0.
Rank=828 done reading total 56862697472 bytes from needed files.
Rank=849 done reading total 56862697472 bytes from needed files.
Rank=834 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/265.bin size=56862697472, starting from offset=0.
Rank=834 done reading total 56862697472 bytes from needed files.
Rank=794 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/930.bin size=56862697472, starting from offset=0.
Rank=794 done reading total 56862697472 bytes from needed files.
Rank=26 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/607.bin size=56862697472, starting from offset=0.
Rank=26 done reading total 56862697472 bytes from needed files.
Rank=625 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/729.bin size=56862697472, starting from offset=0.
Rank=625 done reading total 56862697472 bytes from needed files.
Rank=792 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/491.bin size=56862697472, starting from offset=0.
Rank=792 done reading total 56862697472 bytes from needed files.
Rank=910 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/419.bin size=56862697472, starting from offset=0.
Rank=910 done reading total 56862697472 bytes from needed files.
Rank=695 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/323.bin size=56862697472, starting from offset=0.
Rank=695 done reading total 56862697472 bytes from needed files.
Rank=465 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/936.bin size=56862697472, starting from offset=0.
Rank=465 done reading total 56862697472 bytes from needed files.
Rank=154 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/867.bin size=56862697472, starting from offset=0.
Rank=154 done reading total 56862697472 bytes from needed files.
Rank=668 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/284.bin size=56862697472, starting from offset=0.
Rank=668 done reading total 56862697472 bytes from needed files.
Rank=716 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/546.bin size=56862697472, starting from offset=0.
Rank=716 done reading total 56862697472 bytes from needed files.
Rank=987 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/782.bin size=56862697472, starting from offset=0.
Rank=987 done reading total 56862697472 bytes from needed files.
Rank=85 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/433.bin size=56862697472, starting from offset=0.
Rank=85 done reading total 56862697472 bytes from needed files.
Rank=420 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/438.bin size=56862697472, starting from offset=0.
Rank=420 done reading total 56862697472 bytes from needed files.
Rank=340 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/18.bin size=56862697472, starting from offset=0.
Rank=340 done reading total 56862697472 bytes from needed files.
Rank=423 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/732.bin size=56862697472, starting from offset=0.
Rank=423 done reading total 56862697472 bytes from needed files.
Rank=421 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1005.bin size=56862697472, starting from offset=0.
Rank=421 done reading total 56862697472 bytes from needed files.
Rank=1005 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/995.bin size=56862697472, starting from offset=0.
Rank=1005 done reading total 56862697472 bytes from needed files.
Rank=538 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1015.bin size=56862697472, starting from offset=0.
Rank=538 done reading total 56862697472 bytes from needed files.
Rank=664 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/205.bin size=56862697472, starting from offset=0.
Rank=664 done reading total 56862697472 bytes from needed files.
Rank=950 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/23.bin size=56862697472, starting from offset=0.
Rank=950 done reading total 56862697472 bytes from needed files.
Rank=99 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/625.bin size=56862697472, starting from offset=0.
Rank=99 done reading total 56862697472 bytes from needed files.
Rank=598 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/123.bin size=56862697472, starting from offset=0.
Rank=598 done reading total 56862697472 bytes from needed files.
Rank=1006 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/566.bin size=56862697472, starting from offset=0.
Rank=1006 done reading total 56862697472 bytes from needed files.
Rank=539 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1011.bin size=56862697472, starting from offset=0.
Rank=539 done reading total 56862697472 bytes from needed files.
Rank=306 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/494.bin size=56862697472, starting from offset=0.
Rank=306 done reading total 56862697472 bytes from needed files.
Rank=146 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/122.bin size=56862697472, starting from offset=0.
Rank=146 done reading total 56862697472 bytes from needed files.
Rank=569 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/677.bin size=56862697472, starting from offset=0.
Rank=569 done reading total 56862697472 bytes from needed files.
Rank=121 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/410.bin size=56862697472, starting from offset=0.
Rank=121 done reading total 56862697472 bytes from needed files.
Rank=96 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/582.bin size=56862697472, starting from offset=0.
Rank=96 done reading total 56862697472 bytes from needed files.
Rank=829 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/34.bin size=56862697472, starting from offset=0.
Rank=829 done reading total 56862697472 bytes from needed files.
Rank=50 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/313.bin size=56862697472, starting from offset=0.
Rank=50 done reading total 56862697472 bytes from needed files.
Rank=1017 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/473.bin size=56862697472, starting from offset=0.
Rank=1017 done reading total 56862697472 bytes from needed files.
Rank=464 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/576.bin size=56862697472, starting from offset=0.
Rank=464 done reading total 56862697472 bytes from needed files.
Rank=642 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/534.bin size=56862697472, starting from offset=0.
Rank=642 done reading total 56862697472 bytes from needed files.
Rank=1001 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/266.bin size=56862697472, starting from offset=0.
Rank=1001 done reading total 56862697472 bytes from needed files.
Rank=37 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/740.bin size=56862697472, starting from offset=0.
Rank=37 done reading total 56862697472 bytes from needed files.
Rank=554 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/987.bin size=56862697472, starting from offset=0.
Rank=554 done reading total 56862697472 bytes from needed files.
Rank=48 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/845.bin size=56862697472, starting from offset=0.
Rank=48 done reading total 56862697472 bytes from needed files.
Rank=867 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/113.bin size=56862697472, starting from offset=0.
Rank=867 done reading total 56862697472 bytes from needed files.
Rank=347 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/564.bin size=56862697472, starting from offset=0.
Rank=347 done reading total 56862697472 bytes from needed files.
Rank=97 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/39.bin size=56862697472, starting from offset=0.
Rank=97 done reading total 56862697472 bytes from needed files.
Rank=676 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/581.bin size=56862697472, starting from offset=0.
Rank=676 done reading total 56862697472 bytes from needed files.
Rank=678 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/578.bin size=56862697472, starting from offset=0.
Rank=678 done reading total 56862697472 bytes from needed files.
Rank=515 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/381.bin size=56862697472, starting from offset=0.
Rank=515 done reading total 56862697472 bytes from needed files.
Rank=294 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/759.bin size=56862697472, starting from offset=0.
Rank=294 done reading total 56862697472 bytes from needed files.
Rank=251 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/518.bin size=56862697472, starting from offset=0.
Rank=251 done reading total 56862697472 bytes from needed files.
Rank=131 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/509.bin size=56862697472, starting from offset=0.
Rank=131 done reading total 56862697472 bytes from needed files.
Rank=59 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/137.bin size=56862697472, starting from offset=0.
Rank=59 done reading total 56862697472 bytes from needed files.
Rank=485 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/931.bin size=56862697472, starting from offset=0.
Rank=485 done reading total 56862697472 bytes from needed files.
Rank=184 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/289.bin size=56862697472, starting from offset=0.
Rank=184 done reading total 56862697472 bytes from needed files.
Rank=324 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/24.bin size=56862697472, starting from offset=0.
Rank=324 done reading total 56862697472 bytes from needed files.
Rank=250 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/388.bin size=56862697472, starting from offset=0.
Rank=250 done reading total 56862697472 bytes from needed files.
Rank=940 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/287.bin size=56862697472, starting from offset=0.
Rank=940 done reading total 56862697472 bytes from needed files.
Rank=192 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/851.bin size=56862697472, starting from offset=0.
Rank=192 done reading total 56862697472 bytes from needed files.
Rank=83 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/545.bin size=56862697472, starting from offset=0.
Rank=83 done reading total 56862697472 bytes from needed files.
Rank=60 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/498.bin size=56862697472, starting from offset=0.
Rank=60 done reading total 56862697472 bytes from needed files.
Rank=61 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/319.bin size=56862697472, starting from offset=0.
Rank=61 done reading total 56862697472 bytes from needed files.
Rank=801 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/989.bin size=56862697472, starting from offset=0.
Rank=801 done reading total 56862697472 bytes from needed files.
Rank=843 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/953.bin size=56862697472, starting from offset=0.
Rank=843 done reading total 56862697472 bytes from needed files.
Rank=984 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/301.bin size=56862697472, starting from offset=0.
Rank=984 done reading total 56862697472 bytes from needed files.
Rank=702 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/458.bin size=56862697472, starting from offset=0.
Rank=702 done reading total 56862697472 bytes from needed files.
Rank=13 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/685.bin size=56862697472, starting from offset=0.
Rank=13 done reading total 56862697472 bytes from needed files.
Rank=53 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/597.bin size=56862697472, starting from offset=0.
Rank=53 done reading total 56862697472 bytes from needed files.
Rank=186 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/311.bin size=56862697472, starting from offset=0.
Rank=186 done reading total 56862697472 bytes from needed files.
Rank=701 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/252.bin size=56862697472, starting from offset=0.
Rank=701 done reading total 56862697472 bytes from needed files.
Rank=236 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/108.bin size=56862697472, starting from offset=0.
Rank=236 done reading total 56862697472 bytes from needed files.
Rank=717 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/656.bin size=56862697472, starting from offset=0.
Rank=717 done reading total 56862697472 bytes from needed files.
Rank=978 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/168.bin size=56862697472, starting from offset=0.
Rank=978 done reading total 56862697472 bytes from needed files.
Rank=842 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/374.bin size=56862697472, starting from offset=0.
Rank=842 done reading total 56862697472 bytes from needed files.
Rank=719 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/676.bin size=56862697472, starting from offset=0.
Rank=719 done reading total 56862697472 bytes from needed files.
Rank=237 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/821.bin size=56862697472, starting from offset=0.
Rank=237 done reading total 56862697472 bytes from needed files.
Rank=58 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/918.bin size=56862697472, starting from offset=0.
Rank=58 done reading total 56862697472 bytes from needed files.
Rank=221 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/791.bin size=56862697472, starting from offset=0.
Rank=221 done reading total 56862697472 bytes from needed files.
Rank=570 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/595.bin size=56862697472, starting from offset=0.
Rank=570 done reading total 56862697472 bytes from needed files.
Rank=795 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/615.bin size=56862697472, starting from offset=0.
Rank=795 done reading total 56862697472 bytes from needed files.
Rank=640 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/548.bin size=56862697472, starting from offset=0.
Rank=640 done reading total 56862697472 bytes from needed files.
Rank=891 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/25.bin size=56862697472, starting from offset=0.
Rank=891 done reading total 56862697472 bytes from needed files.
Rank=985 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/8.bin size=56862697472, starting from offset=0.
Rank=985 done reading total 56862697472 bytes from needed files.
Rank=243 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/470.bin size=56862697472, starting from offset=0.
Rank=243 done reading total 56862697472 bytes from needed files.
Rank=592 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/349.bin size=56862697472, starting from offset=0.
Rank=592 done reading total 56862697472 bytes from needed files.
Rank=555 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/279.bin size=56862697472, starting from offset=0.
Rank=555 done reading total 56862697472 bytes from needed files.
Rank=1004 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/241.bin size=56862697472, starting from offset=0.
Rank=1004 done reading total 56862697472 bytes from needed files.
Rank=147 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/451.bin size=56862697472, starting from offset=0.
Rank=147 done reading total 56862697472 bytes from needed files.
Rank=970 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/704.bin size=56862697472, starting from offset=0.
Rank=970 done reading total 56862697472 bytes from needed files.
Rank=508 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/456.bin size=56862697472, starting from offset=0.
Rank=508 done reading total 56862697472 bytes from needed files.
Rank=848 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/48.bin size=56862697472, starting from offset=0.
Rank=848 done reading total 56862697472 bytes from needed files.
Rank=830 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/134.bin size=56862697472, starting from offset=0.
Rank=830 done reading total 56862697472 bytes from needed files.
Rank=81 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/354.bin size=56862697472, starting from offset=0.
Rank=81 done reading total 56862697472 bytes from needed files.
Rank=255 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/153.bin size=56862697472, starting from offset=0.
Rank=255 done reading total 56862697472 bytes from needed files.
Rank=82 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/641.bin size=56862697472, starting from offset=0.
Rank=82 done reading total 56862697472 bytes from needed files.
Rank=754 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/274.bin size=56862697472, starting from offset=0.
Rank=754 done reading total 56862697472 bytes from needed files.
Rank=262 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/89.bin size=56862697472, starting from offset=0.
Rank=262 done reading total 56862697472 bytes from needed files.
Rank=356 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1010.bin size=56862697472, starting from offset=0.
Rank=356 done reading total 56862697472 bytes from needed files.
Rank=119 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/218.bin size=56862697472, starting from offset=0.
Rank=119 done reading total 56862697472 bytes from needed files.
Rank=376 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/111.bin size=56862697472, starting from offset=0.
Rank=376 done reading total 56862697472 bytes from needed files.
Rank=248 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/364.bin size=56862697472, starting from offset=0.
Rank=248 done reading total 56862697472 bytes from needed files.
Rank=802 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/707.bin size=56862697472, starting from offset=0.
Rank=802 done reading total 56862697472 bytes from needed files.
Rank=938 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/140.bin size=56862697472, starting from offset=0.
Rank=938 done reading total 56862697472 bytes from needed files.
Rank=855 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/353.bin size=56862697472, starting from offset=0.
Rank=855 done reading total 56862697472 bytes from needed files.
Rank=320 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/83.bin size=56862697472, starting from offset=0.
Rank=320 done reading total 56862697472 bytes from needed files.
Rank=358 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/445.bin size=56862697472, starting from offset=0.
Rank=358 done reading total 56862697472 bytes from needed files.
Rank=532 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/204.bin size=56862697472, starting from offset=0.
Rank=532 done reading total 56862697472 bytes from needed files.
Rank=238 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1017.bin size=56862697472, starting from offset=0.
Rank=238 done reading total 56862697472 bytes from needed files.
Rank=377 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/44.bin size=56862697472, starting from offset=0.
Rank=377 done reading total 56862697472 bytes from needed files.
Rank=875 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/300.bin size=56862697472, starting from offset=0.
Rank=875 done reading total 56862697472 bytes from needed files.
Rank=222 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/165.bin size=56862697472, starting from offset=0.
Rank=222 done reading total 56862697472 bytes from needed files.
Rank=350 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/64.bin size=56862697472, starting from offset=0.
Rank=350 done reading total 56862697472 bytes from needed files.
Rank=216 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/190.bin size=56862697472, starting from offset=0.
Rank=216 done reading total 56862697472 bytes from needed files.
Rank=263 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/633.bin size=56862697472, starting from offset=0.
Rank=263 done reading total 56862697472 bytes from needed files.
Rank=108 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/698.bin size=56862697472, starting from offset=0.
Rank=108 done reading total 56862697472 bytes from needed files.
Rank=187 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/369.bin size=56862697472, starting from offset=0.
Rank=187 done reading total 56862697472 bytes from needed files.
Rank=172 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/500.bin size=56862697472, starting from offset=0.
Rank=172 done reading total 56862697472 bytes from needed files.
Rank=31 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/281.bin size=56862697472, starting from offset=0.
Rank=31 done reading total 56862697472 bytes from needed files.
Rank=181 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/260.bin size=56862697472, starting from offset=0.
Rank=181 done reading total 56862697472 bytes from needed files.
Rank=627 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/442.bin size=56862697472, starting from offset=0.
Rank=627 done reading total 56862697472 bytes from needed files.
Rank=227 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/962.bin size=56862697472, starting from offset=0.
Rank=227 done reading total 56862697472 bytes from needed files.
Rank=175 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/808.bin size=56862697472, starting from offset=0.
Rank=175 done reading total 56862697472 bytes from needed files.
Rank=665 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/222.bin size=56862697472, starting from offset=0.
Rank=665 done reading total 56862697472 bytes from needed files.
Rank=109 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/462.bin size=56862697472, starting from offset=0.
Rank=109 done reading total 56862697472 bytes from needed files.
Rank=778 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/316.bin size=56862697472, starting from offset=0.
Rank=778 done reading total 56862697472 bytes from needed files.
Rank=703 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/450.bin size=56862697472, starting from offset=0.
Rank=703 done reading total 56862697472 bytes from needed files.
Rank=941 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/254.bin size=56862697472, starting from offset=0.
Rank=941 done reading total 56862697472 bytes from needed files.
Rank=166 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/272.bin size=56862697472, starting from offset=0.
Rank=166 done reading total 56862697472 bytes from needed files.
Rank=454 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/588.bin size=56862697472, starting from offset=0.
Rank=454 done reading total 56862697472 bytes from needed files.
Rank=223 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/683.bin size=56862697472, starting from offset=0.
Rank=223 done reading total 56862697472 bytes from needed files.
Rank=852 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/957.bin size=56862697472, starting from offset=0.
Rank=852 done reading total 56862697472 bytes from needed files.
Rank=546 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/601.bin size=56862697472, starting from offset=0.
Rank=546 done reading total 56862697472 bytes from needed files.
Rank=27 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/382.bin size=56862697472, starting from offset=0.
Rank=27 done reading total 56862697472 bytes from needed files.
Rank=773 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/490.bin size=56862697472, starting from offset=0.
Rank=773 done reading total 56862697472 bytes from needed files.
Rank=573 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/840.bin size=56862697472, starting from offset=0.
Rank=573 done reading total 56862697472 bytes from needed files.
Rank=873 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/881.bin size=56862697472, starting from offset=0.
Rank=873 done reading total 56862697472 bytes from needed files.
Rank=854 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/671.bin size=56862697472, starting from offset=0.
Rank=854 done reading total 56862697472 bytes from needed files.
Rank=931 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/981.bin size=56862697472, starting from offset=0.
Rank=931 done reading total 56862697472 bytes from needed files.
Rank=307 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/664.bin size=56862697472, starting from offset=0.
Rank=307 done reading total 56862697472 bytes from needed files.
Rank=304 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1003.bin size=56862697472, starting from offset=0.
Rank=304 done reading total 56862697472 bytes from needed files.
Rank=501 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/964.bin size=56862697472, starting from offset=0.
Rank=501 done reading total 56862697472 bytes from needed files.
Rank=140 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/82.bin size=56862697472, starting from offset=0.
Rank=140 done reading total 56862697472 bytes from needed files.
Rank=86 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/722.bin size=56862697472, starting from offset=0.
Rank=86 done reading total 56862697472 bytes from needed files.
Rank=851 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/182.bin size=56862697472, starting from offset=0.
Rank=851 done reading total 56862697472 bytes from needed files.
Rank=112 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/702.bin size=56862697472, starting from offset=0.
Rank=112 done reading total 56862697472 bytes from needed files.
Rank=113 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/636.bin size=56862697472, starting from offset=0.
Rank=113 done reading total 56862697472 bytes from needed files.
Rank=266 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/244.bin size=56862697472, starting from offset=0.
Rank=266 done reading total 56862697472 bytes from needed files.
Rank=15 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1002.bin size=56862697472, starting from offset=0.
Rank=15 done reading total 56862697472 bytes from needed files.
Rank=338 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/807.bin size=56862697472, starting from offset=0.
Rank=338 done reading total 56862697472 bytes from needed files.
Rank=301 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/903.bin size=56862697472, starting from offset=0.
Rank=301 done reading total 56862697472 bytes from needed files.
Rank=156 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/297.bin size=56862697472, starting from offset=0.
Rank=156 done reading total 56862697472 bytes from needed files.
Rank=219 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/232.bin size=56862697472, starting from offset=0.
Rank=219 done reading total 56862697472 bytes from needed files.
Rank=800 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/268.bin size=56862697472, starting from offset=0.
Rank=800 done reading total 56862697472 bytes from needed files.
Rank=774 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/411.bin size=56862697472, starting from offset=0.
Rank=774 done reading total 56862697472 bytes from needed files.
Rank=566 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/921.bin size=56862697472, starting from offset=0.
Rank=566 done reading total 56862697472 bytes from needed files.
Rank=928 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/149.bin size=56862697472, starting from offset=0.
Rank=928 done reading total 56862697472 bytes from needed files.
Rank=793 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/622.bin size=56862697472, starting from offset=0.
Rank=793 done reading total 56862697472 bytes from needed files.
Rank=148 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/128.bin size=56862697472, starting from offset=0.
Rank=148 done reading total 56862697472 bytes from needed files.
Rank=182 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/215.bin size=56862697472, starting from offset=0.
Rank=182 done reading total 56862697472 bytes from needed files.
Rank=512 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/584.bin size=56862697472, starting from offset=0.
Rank=512 done reading total 56862697472 bytes from needed files.
Rank=136 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/452.bin size=56862697472, starting from offset=0.
Rank=136 done reading total 56862697472 bytes from needed files.
Rank=138 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/544.bin size=56862697472, starting from offset=0.
Rank=138 done reading total 56862697472 bytes from needed files.
Rank=624 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/211.bin size=56862697472, starting from offset=0.
Rank=624 done reading total 56862697472 bytes from needed files.
Rank=962 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/421.bin size=56862697472, starting from offset=0.
Rank=962 done reading total 56862697472 bytes from needed files.
Rank=874 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/657.bin size=56862697472, starting from offset=0.
Rank=874 done reading total 56862697472 bytes from needed files.
Rank=438 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/562.bin size=56862697472, starting from offset=0.
Rank=438 done reading total 56862697472 bytes from needed files.
Rank=14 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/822.bin size=56862697472, starting from offset=0.
Rank=14 done reading total 56862697472 bytes from needed files.
Rank=936 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/892.bin size=56862697472, starting from offset=0.
Rank=936 done reading total 56862697472 bytes from needed files.
Rank=84 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/602.bin size=56862697472, starting from offset=0.
Rank=84 done reading total 56862697472 bytes from needed files.
Rank=137 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/56.bin size=56862697472, starting from offset=0.
Rank=137 done reading total 56862697472 bytes from needed files.
Rank=796 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/624.bin size=56862697472, starting from offset=0.
Rank=796 done reading total 56862697472 bytes from needed files.
Rank=24 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/248.bin size=56862697472, starting from offset=0.
Rank=24 done reading total 56862697472 bytes from needed files.
Rank=233 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/405.bin size=56862697472, starting from offset=0.
Rank=233 done reading total 56862697472 bytes from needed files.
Rank=602 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/290.bin size=56862697472, starting from offset=0.
Rank=602 done reading total 56862697472 bytes from needed files.
Rank=115 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/65.bin size=56862697472, starting from offset=0.
Rank=115 done reading total 56862697472 bytes from needed files.
Rank=341 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/644.bin size=56862697472, starting from offset=0.
Rank=341 done reading total 56862697472 bytes from needed files.
Rank=783 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/915.bin size=56862697472, starting from offset=0.
Rank=783 done reading total 56862697472 bytes from needed files.
Rank=643 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/758.bin size=56862697472, starting from offset=0.
Rank=643 done reading total 56862697472 bytes from needed files.
Rank=319 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/367.bin size=56862697472, starting from offset=0.
Rank=319 done reading total 56862697472 bytes from needed files.
Rank=337 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/61.bin size=56862697472, starting from offset=0.
Rank=337 done reading total 56862697472 bytes from needed files.
Rank=466 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/435.bin size=56862697472, starting from offset=0.
Rank=466 done reading total 56862697472 bytes from needed files.
Rank=715 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/754.bin size=56862697472, starting from offset=0.
Rank=715 done reading total 56862697472 bytes from needed files.
Rank=771 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/4.bin size=56862697472, starting from offset=0.
Rank=771 done reading total 56862697472 bytes from needed files.
Rank=173 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/401.bin size=56862697472, starting from offset=0.
Rank=173 done reading total 56862697472 bytes from needed files.
Rank=977 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/859.bin size=56862697472, starting from offset=0.
Rank=977 done reading total 56862697472 bytes from needed files.
Rank=745 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/951.bin size=56862697472, starting from offset=0.
Rank=745 done reading total 56862697472 bytes from needed files.
Rank=327 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/513.bin size=56862697472, starting from offset=0.
Rank=327 done reading total 56862697472 bytes from needed files.
Rank=353 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/294.bin size=56862697472, starting from offset=0.
Rank=353 done reading total 56862697472 bytes from needed files.
Rank=65 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/990.bin size=56862697472, starting from offset=0.
Rank=65 done reading total 56862697472 bytes from needed files.
Rank=110 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/242.bin size=56862697472, starting from offset=0.
Rank=110 done reading total 56862697472 bytes from needed files.
Rank=480 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/199.bin size=56862697472, starting from offset=0.
Rank=480 done reading total 56862697472 bytes from needed files.
Rank=191 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/400.bin size=56862697472, starting from offset=0.
Rank=191 done reading total 56862697472 bytes from needed files.
Rank=553 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/172.bin size=56862697472, starting from offset=0.
Rank=553 done reading total 56862697472 bytes from needed files.
Rank=487 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/236.bin size=56862697472, starting from offset=0.
Rank=487 done reading total 56862697472 bytes from needed files.
Rank=932 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/107.bin size=56862697472, starting from offset=0.
Rank=932 done reading total 56862697472 bytes from needed files.
Rank=594 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/397.bin size=56862697472, starting from offset=0.
Rank=594 done reading total 56862697472 bytes from needed files.
Rank=467 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/757.bin size=56862697472, starting from offset=0.
Rank=467 done reading total 56862697472 bytes from needed files.
Rank=872 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/993.bin size=56862697472, starting from offset=0.
Rank=872 done reading total 56862697472 bytes from needed files.
Rank=217 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/159.bin size=56862697472, starting from offset=0.
Rank=217 done reading total 56862697472 bytes from needed files.
Rank=986 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/623.bin size=56862697472, starting from offset=0.
Rank=986 done reading total 56862697472 bytes from needed files.
Rank=170 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/569.bin size=56862697472, starting from offset=0.
Rank=170 done reading total 56862697472 bytes from needed files.
Rank=273 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/514.bin size=56862697472, starting from offset=0.
Rank=273 done reading total 56862697472 bytes from needed files.
Rank=149 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/932.bin size=56862697472, starting from offset=0.
Rank=149 done reading total 56862697472 bytes from needed files.
Rank=896 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/528.bin size=56862697472, starting from offset=0.
Rank=896 done reading total 56862697472 bytes from needed files.
Rank=361 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/538.bin size=56862697472, starting from offset=0.
Rank=361 done reading total 56862697472 bytes from needed files.
Rank=844 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/22.bin size=56862697472, starting from offset=0.
Rank=844 done reading total 56862697472 bytes from needed files.
Rank=541 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/77.bin size=56862697472, starting from offset=0.
Rank=541 done reading total 56862697472 bytes from needed files.
Rank=956 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/13.bin size=56862697472, starting from offset=0.
Rank=956 done reading total 56862697472 bytes from needed files.
Rank=562 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/181.bin size=56862697472, starting from offset=0.
Rank=562 done reading total 56862697472 bytes from needed files.
Rank=139 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/186.bin size=56862697472, starting from offset=0.
Rank=139 done reading total 56862697472 bytes from needed files.
Rank=744 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/227.bin size=56862697472, starting from offset=0.
Rank=744 done reading total 56862697472 bytes from needed files.
Rank=118 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/110.bin size=56862697472, starting from offset=0.
Rank=118 done reading total 56862697472 bytes from needed files.
Rank=10 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/144.bin size=56862697472, starting from offset=0.
Rank=10 done reading total 56862697472 bytes from needed files.
Rank=274 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/42.bin size=56862697472, starting from offset=0.
Rank=274 done reading total 56862697472 bytes from needed files.
Rank=803 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/341.bin size=56862697472, starting from offset=0.
Rank=803 done reading total 56862697472 bytes from needed files.
Rank=69 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/876.bin size=56862697472, starting from offset=0.
Rank=69 done reading total 56862697472 bytes from needed files.
Rank=877 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/777.bin size=56862697472, starting from offset=0.
Rank=877 done reading total 56862697472 bytes from needed files.
Rank=87 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/109.bin size=56862697472, starting from offset=0.
Rank=87 done reading total 56862697472 bytes from needed files.
Rank=593 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/303.bin size=56862697472, starting from offset=0.
Rank=593 done reading total 56862697472 bytes from needed files.
Rank=317 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/734.bin size=56862697472, starting from offset=0.
Rank=317 done reading total 56862697472 bytes from needed files.
Rank=318 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/145.bin size=56862697472, starting from offset=0.
Rank=318 done reading total 56862697472 bytes from needed files.
Rank=714 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/686.bin size=56862697472, starting from offset=0.
Rank=714 done reading total 56862697472 bytes from needed files.
Rank=129 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/943.bin size=56862697472, starting from offset=0.
Rank=129 done reading total 56862697472 bytes from needed files.
Rank=422 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/959.bin size=56862697472, starting from offset=0.
Rank=422 done reading total 56862697472 bytes from needed files.
Rank=690 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/870.bin size=56862697472, starting from offset=0.
Rank=690 done reading total 56862697472 bytes from needed files.
Rank=25 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/846.bin size=56862697472, starting from offset=0.
Rank=25 done reading total 56862697472 bytes from needed files.
Rank=942 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/440.bin size=56862697472, starting from offset=0.
Rank=942 done reading total 56862697472 bytes from needed files.
Rank=691 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/675.bin size=56862697472, starting from offset=0.
Rank=691 done reading total 56862697472 bytes from needed files.
Rank=1016 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/532.bin size=56862697472, starting from offset=0.
Rank=1016 done reading total 56862697472 bytes from needed files.
Rank=29 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/225.bin size=56862697472, starting from offset=0.
Rank=29 done reading total 56862697472 bytes from needed files.
Rank=808 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/784.bin size=56862697472, starting from offset=0.
Rank=808 done reading total 56862697472 bytes from needed files.
Rank=770 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/97.bin size=56862697472, starting from offset=0.
Rank=770 done reading total 56862697472 bytes from needed files.
Rank=177 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/6.bin size=56862697472, starting from offset=0.
Rank=177 done reading total 56862697472 bytes from needed files.
Rank=457 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/173.bin size=56862697472, starting from offset=0.
Rank=457 done reading total 56862697472 bytes from needed files.
Rank=300 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/53.bin size=56862697472, starting from offset=0.
Rank=300 done reading total 56862697472 bytes from needed files.
Rank=18 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1007.bin size=56862697472, starting from offset=0.
Rank=18 done reading total 56862697472 bytes from needed files.
Rank=482 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/605.bin size=56862697472, starting from offset=0.
Rank=482 done reading total 56862697472 bytes from needed files.
Rank=28 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/775.bin size=56862697472, starting from offset=0.
Rank=28 done reading total 56862697472 bytes from needed files.
Rank=769 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/913.bin size=56862697472, starting from offset=0.
Rank=769 done reading total 56862697472 bytes from needed files.
Rank=157 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/507.bin size=56862697472, starting from offset=0.
Rank=157 done reading total 56862697472 bytes from needed files.
Rank=622 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/939.bin size=56862697472, starting from offset=0.
Rank=622 done reading total 56862697472 bytes from needed files.
Rank=958 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/699.bin size=56862697472, starting from offset=0.
Rank=958 done reading total 56862697472 bytes from needed files.
Rank=349 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/713.bin size=56862697472, starting from offset=0.
Rank=349 done reading total 56862697472 bytes from needed files.
Rank=130 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/883.bin size=56862697472, starting from offset=0.
Rank=130 done reading total 56862697472 bytes from needed files.
Rank=976 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/104.bin size=56862697472, starting from offset=0.
Rank=976 done reading total 56862697472 bytes from needed files.
Rank=448 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/131.bin size=56862697472, starting from offset=0.
Rank=448 done reading total 56862697472 bytes from needed files.
Rank=12 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/334.bin size=56862697472, starting from offset=0.
Rank=12 done reading total 56862697472 bytes from needed files.
Rank=171 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/223.bin size=56862697472, starting from offset=0.
Rank=171 done reading total 56862697472 bytes from needed files.
Rank=299 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/739.bin size=56862697472, starting from offset=0.
Rank=299 done reading total 56862697472 bytes from needed files.
Rank=513 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/449.bin size=56862697472, starting from offset=0.
Rank=513 done reading total 56862697472 bytes from needed files.
Rank=393 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/412.bin size=56862697472, starting from offset=0.
Rank=393 done reading total 56862697472 bytes from needed files.
Rank=123 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/631.bin size=56862697472, starting from offset=0.
Rank=123 done reading total 56862697472 bytes from needed files.
Rank=128 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/620.bin size=56862697472, starting from offset=0.
Rank=128 done reading total 56862697472 bytes from needed files.
Rank=1022 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/854.bin size=56862697472, starting from offset=0.
Rank=1022 done reading total 56862697472 bytes from needed files.
Rank=336 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/933.bin size=56862697472, starting from offset=0.
Rank=336 done reading total 56862697472 bytes from needed files.
Rank=174 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/861.bin size=56862697472, starting from offset=0.
Rank=174 done reading total 56862697472 bytes from needed files.
Rank=718 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/118.bin size=56862697472, starting from offset=0.
Rank=718 done reading total 56862697472 bytes from needed files.
Rank=185 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/893.bin size=56862697472, starting from offset=0.
Rank=185 done reading total 56862697472 bytes from needed files.
Rank=507 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/600.bin size=56862697472, starting from offset=0.
Rank=507 done reading total 56862697472 bytes from needed files.
Rank=519 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/393.bin size=56862697472, starting from offset=0.
Rank=519 done reading total 56862697472 bytes from needed files.
Rank=450 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/70.bin size=56862697472, starting from offset=0.
Rank=450 done reading total 56862697472 bytes from needed files.
Rank=713 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/425.bin size=56862697472, starting from offset=0.
Rank=713 done reading total 56862697472 bytes from needed files.
Rank=194 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/437.bin size=56862697472, starting from offset=0.
Rank=194 done reading total 56862697472 bytes from needed files.
Rank=919 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/717.bin size=56862697472, starting from offset=0.
Rank=919 done reading total 56862697472 bytes from needed files.
Rank=589 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1.bin size=56862697472, starting from offset=0.
Rank=589 done reading total 56862697472 bytes from needed files.
Rank=150 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/394.bin size=56862697472, starting from offset=0.
Rank=150 done reading total 56862697472 bytes from needed files.
Rank=846 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/831.bin size=56862697472, starting from offset=0.
Rank=846 done reading total 56862697472 bytes from needed files.
Rank=167 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/69.bin size=56862697472, starting from offset=0.
Rank=167 done reading total 56862697472 bytes from needed files.
Rank=641 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/741.bin size=56862697472, starting from offset=0.
Rank=641 done reading total 56862697472 bytes from needed files.
Rank=543 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/292.bin size=56862697472, starting from offset=0.
Rank=543 done reading total 56862697472 bytes from needed files.
Rank=178 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/476.bin size=56862697472, starting from offset=0.
Rank=178 done reading total 56862697472 bytes from needed files.
Rank=811 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/59.bin size=56862697472, starting from offset=0.
Rank=811 done reading total 56862697472 bytes from needed files.
Rank=348 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/574.bin size=56862697472, starting from offset=0.
Rank=348 done reading total 56862697472 bytes from needed files.
Rank=379 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/898.bin size=56862697472, starting from offset=0.
Rank=379 done reading total 56862697472 bytes from needed files.
Rank=176 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/262.bin size=56862697472, starting from offset=0.
Rank=176 done reading total 56862697472 bytes from needed files.
Rank=244 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/844.bin size=56862697472, starting from offset=0.
Rank=244 done reading total 56862697472 bytes from needed files.
Rank=621 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/261.bin size=56862697472, starting from offset=0.
Rank=621 done reading total 56862697472 bytes from needed files.
Rank=218 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/852.bin size=56862697472, starting from offset=0.
Rank=218 done reading total 56862697472 bytes from needed files.
Rank=626 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/650.bin size=56862697472, starting from offset=0.
Rank=626 done reading total 56862697472 bytes from needed files.
Rank=597 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/651.bin size=56862697472, starting from offset=0.
Rank=597 done reading total 56862697472 bytes from needed files.
Rank=540 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/171.bin size=56862697472, starting from offset=0.
Rank=540 done reading total 56862697472 bytes from needed files.
Rank=853 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/487.bin size=56862697472, starting from offset=0.
Rank=853 done reading total 56862697472 bytes from needed files.
Rank=677 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/322.bin size=56862697472, starting from offset=0.
Rank=677 done reading total 56862697472 bytes from needed files.
Rank=758 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/326.bin size=56862697472, starting from offset=0.
Rank=758 done reading total 56862697472 bytes from needed files.
Rank=567 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/151.bin size=56862697472, starting from offset=0.
Rank=567 done reading total 56862697472 bytes from needed files.
Rank=603 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/961.bin size=56862697472, starting from offset=0.
Rank=603 done reading total 56862697472 bytes from needed files.
Rank=922 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/809.bin size=56862697472, starting from offset=0.
Rank=922 done reading total 56862697472 bytes from needed files.
Rank=456 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/457.bin size=56862697472, starting from offset=0.
Rank=456 done reading total 56862697472 bytes from needed files.
Rank=342 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/825.bin size=56862697472, starting from offset=0.
Rank=342 done reading total 56862697472 bytes from needed files.
Rank=77 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/55.bin size=56862697472, starting from offset=0.
Rank=77 done reading total 56862697472 bytes from needed files.
Rank=383 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/85.bin size=56862697472, starting from offset=0.
Rank=383 done reading total 56862697472 bytes from needed files.
Rank=825 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/580.bin size=56862697472, starting from offset=0.
Rank=825 done reading total 56862697472 bytes from needed files.
Rank=168 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/51.bin size=56862697472, starting from offset=0.
Rank=168 done reading total 56862697472 bytes from needed files.
Rank=957 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/598.bin size=56862697472, starting from offset=0.
Rank=957 done reading total 56862697472 bytes from needed files.
Rank=71 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/626.bin size=56862697472, starting from offset=0.
Rank=71 done reading total 56862697472 bytes from needed files.
Rank=768 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/317.bin size=56862697472, starting from offset=0.
Rank=768 done reading total 56862697472 bytes from needed files.
Rank=459 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/488.bin size=56862697472, starting from offset=0.
Rank=459 done reading total 56862697472 bytes from needed files.
Rank=159 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/934.bin size=56862697472, starting from offset=0.
Rank=159 done reading total 56862697472 bytes from needed files.
Rank=929 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/291.bin size=56862697472, starting from offset=0.
Rank=929 done reading total 56862697472 bytes from needed files.
Rank=737 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/3.bin size=56862697472, starting from offset=0.
Rank=737 done reading total 56862697472 bytes from needed files.
Rank=316 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/501.bin size=56862697472, starting from offset=0.
Rank=316 done reading total 56862697472 bytes from needed files.
Rank=542 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/678.bin size=56862697472, starting from offset=0.
Rank=542 done reading total 56862697472 bytes from needed files.
Rank=11 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/555.bin size=56862697472, starting from offset=0.
Rank=11 done reading total 56862697472 bytes from needed files.
Rank=378 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/480.bin size=56862697472, starting from offset=0.
Rank=378 done reading total 56862697472 bytes from needed files.
Rank=674 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/855.bin size=56862697472, starting from offset=0.
Rank=674 done reading total 56862697472 bytes from needed files.
Rank=787 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/771.bin size=56862697472, starting from offset=0.
Rank=787 done reading total 56862697472 bytes from needed files.
Rank=462 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/416.bin size=56862697472, starting from offset=0.
Rank=462 done reading total 56862697472 bytes from needed files.
Rank=646 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/857.bin size=56862697472, starting from offset=0.
Rank=646 done reading total 56862697472 bytes from needed files.
Rank=409 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/66.bin size=56862697472, starting from offset=0.
Rank=409 done reading total 56862697472 bytes from needed files.
Rank=52 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1016.bin size=56862697472, starting from offset=0.
Rank=52 done reading total 56862697472 bytes from needed files.
Rank=923 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/148.bin size=56862697472, starting from offset=0.
Rank=923 done reading total 56862697472 bytes from needed files.
Rank=860 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/395.bin size=56862697472, starting from offset=0.
Rank=860 done reading total 56862697472 bytes from needed files.
Rank=70 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/629.bin size=56862697472, starting from offset=0.
Rank=70 done reading total 56862697472 bytes from needed files.
Rank=979 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/459.bin size=56862697472, starting from offset=0.
Rank=979 done reading total 56862697472 bytes from needed files.
Rank=246 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/11.bin size=56862697472, starting from offset=0.
Rank=246 done reading total 56862697472 bytes from needed files.
Rank=117 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/141.bin size=56862697472, starting from offset=0.
Rank=117 done reading total 56862697472 bytes from needed files.
Rank=1019 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/763.bin size=56862697472, starting from offset=0.
Rank=1019 done reading total 56862697472 bytes from needed files.
Rank=408 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/363.bin size=56862697472, starting from offset=0.
Rank=408 done reading total 56862697472 bytes from needed files.
Rank=272 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/448.bin size=56862697472, starting from offset=0.
Rank=272 done reading total 56862697472 bytes from needed files.
Rank=750 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/640.bin size=56862697472, starting from offset=0.
Rank=750 done reading total 56862697472 bytes from needed files.
Rank=658 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/908.bin size=56862697472, starting from offset=0.
Rank=658 done reading total 56862697472 bytes from needed files.
Rank=518 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/47.bin size=56862697472, starting from offset=0.
Rank=518 done reading total 56862697472 bytes from needed files.
Rank=967 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/307.bin size=56862697472, starting from offset=0.
Rank=967 done reading total 56862697472 bytes from needed files.
Rank=302 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/749.bin size=56862697472, starting from offset=0.
Rank=302 done reading total 56862697472 bytes from needed files.
Rank=477 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/862.bin size=56862697472, starting from offset=0.
Rank=477 done reading total 56862697472 bytes from needed files.
Rank=451 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/950.bin size=56862697472, starting from offset=0.
Rank=451 done reading total 56862697472 bytes from needed files.
Rank=961 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/16.bin size=56862697472, starting from offset=0.
Rank=961 done reading total 56862697472 bytes from needed files.
Rank=517 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/522.bin size=56862697472, starting from offset=0.
Rank=517 done reading total 56862697472 bytes from needed files.
Rank=759 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/251.bin size=56862697472, starting from offset=0.
Rank=759 done reading total 56862697472 bytes from needed files.
Rank=656 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/5.bin size=56862697472, starting from offset=0.
Rank=656 done reading total 56862697472 bytes from needed files.
Rank=574 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/779.bin size=56862697472, starting from offset=0.
Rank=574 done reading total 56862697472 bytes from needed files.
Rank=1014 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/529.bin size=56862697472, starting from offset=0.
Rank=1014 done reading total 56862697472 bytes from needed files.
Rank=355 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/945.bin size=56862697472, starting from offset=0.
Rank=355 done reading total 56862697472 bytes from needed files.
Rank=591 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/164.bin size=56862697472, starting from offset=0.
Rank=591 done reading total 56862697472 bytes from needed files.
Rank=572 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/314.bin size=56862697472, starting from offset=0.
Rank=572 done reading total 56862697472 bytes from needed files.
Rank=1018 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/192.bin size=56862697472, starting from offset=0.
Rank=1018 done reading total 56862697472 bytes from needed files.
Rank=505 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/329.bin size=56862697472, starting from offset=0.
Rank=505 done reading total 56862697472 bytes from needed files.
Rank=247 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/897.bin size=56862697472, starting from offset=0.
Rank=247 done reading total 56862697472 bytes from needed files.
Rank=392 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/823.bin size=56862697472, starting from offset=0.
Rank=392 done reading total 56862697472 bytes from needed files.
Rank=827 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/526.bin size=56862697472, starting from offset=0.
Rank=827 done reading total 56862697472 bytes from needed files.
Rank=66 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/504.bin size=56862697472, starting from offset=0.
Rank=66 done reading total 56862697472 bytes from needed files.
Rank=747 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/231.bin size=56862697472, starting from offset=0.
Rank=747 done reading total 56862697472 bytes from needed files.
Rank=537 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/952.bin size=56862697472, starting from offset=0.
Rank=537 done reading total 56862697472 bytes from needed files.
Rank=934 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/803.bin size=56862697472, starting from offset=0.
Rank=934 done reading total 56862697472 bytes from needed files.
Rank=935 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/583.bin size=56862697472, starting from offset=0.
Rank=935 done reading total 56862697472 bytes from needed files.
Rank=76 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/233.bin size=56862697472, starting from offset=0.
Rank=76 done reading total 56862697472 bytes from needed files.
Rank=757 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/766.bin size=56862697472, starting from offset=0.
Rank=757 done reading total 56862697472 bytes from needed files.
Rank=866 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/573.bin size=56862697472, starting from offset=0.
Rank=866 done reading total 56862697472 bytes from needed files.
Rank=382 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/593.bin size=56862697472, starting from offset=0.
Rank=382 done reading total 56862697472 bytes from needed files.
Rank=618 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/57.bin size=56862697472, starting from offset=0.
Rank=618 done reading total 56862697472 bytes from needed files.
Rank=365 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/610.bin size=56862697472, starting from offset=0.
Rank=365 done reading total 56862697472 bytes from needed files.
Rank=689 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/234.bin size=56862697472, starting from offset=0.
Rank=689 done reading total 56862697472 bytes from needed files.
Rank=303 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/478.bin size=56862697472, starting from offset=0.
Rank=303 done reading total 56862697472 bytes from needed files.
Rank=322 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/129.bin size=56862697472, starting from offset=0.
Rank=322 done reading total 56862697472 bytes from needed files.
Rank=762 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1018.bin size=56862697472, starting from offset=0.
Rank=762 done reading total 56862697472 bytes from needed files.
Rank=810 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/10.bin size=56862697472, starting from offset=0.
Rank=810 done reading total 56862697472 bytes from needed files.
Rank=817 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/130.bin size=56862697472, starting from offset=0.
Rank=817 done reading total 56862697472 bytes from needed files.
Rank=672 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/7.bin size=56862697472, starting from offset=0.
Rank=672 done reading total 56862697472 bytes from needed files.
Rank=308 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/96.bin size=56862697472, starting from offset=0.
Rank=308 done reading total 56862697472 bytes from needed files.
Rank=364 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/247.bin size=56862697472, starting from offset=0.
Rank=364 done reading total 56862697472 bytes from needed files.
Rank=963 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/499.bin size=56862697472, starting from offset=0.
Rank=963 done reading total 56862697472 bytes from needed files.
Rank=561 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/603.bin size=56862697472, starting from offset=0.
Rank=561 done reading total 56862697472 bytes from needed files.
Rank=601 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/200.bin size=56862697472, starting from offset=0.
Rank=601 done reading total 56862697472 bytes from needed files.
Rank=357 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/937.bin size=56862697472, starting from offset=0.
Rank=357 done reading total 56862697472 bytes from needed files.
Rank=479 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/258.bin size=56862697472, starting from offset=0.
Rank=479 done reading total 56862697472 bytes from needed files.
Rank=536 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/335.bin size=56862697472, starting from offset=0.
Rank=536 done reading total 56862697472 bytes from needed files.
Rank=516 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/794.bin size=56862697472, starting from offset=0.
Rank=516 done reading total 56862697472 bytes from needed files.
Rank=620 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/38.bin size=56862697472, starting from offset=0.
Rank=620 done reading total 56862697472 bytes from needed files.
Rank=310 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/426.bin size=56862697472, starting from offset=0.
Rank=310 done reading total 56862697472 bytes from needed files.
Rank=578 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/833.bin size=56862697472, starting from offset=0.
Rank=578 done reading total 56862697472 bytes from needed files.
Rank=496 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/802.bin size=56862697472, starting from offset=0.
Rank=496 done reading total 56862697472 bytes from needed files.
Rank=798 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/551.bin size=56862697472, starting from offset=0.
Rank=798 done reading total 56862697472 bytes from needed files.
Rank=563 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/691.bin size=56862697472, starting from offset=0.
Rank=563 done reading total 56862697472 bytes from needed files.
Rank=230 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/273.bin size=56862697472, starting from offset=0.
Rank=230 done reading total 56862697472 bytes from needed files.
Rank=685 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/497.bin size=56862697472, starting from offset=0.
Rank=685 done reading total 56862697472 bytes from needed files.
Rank=293 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/539.bin size=56862697472, starting from offset=0.
Rank=293 done reading total 56862697472 bytes from needed files.
Rank=644 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/191.bin size=56862697472, starting from offset=0.
Rank=644 done reading total 56862697472 bytes from needed files.
Rank=491 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/994.bin size=56862697472, starting from offset=0.
Rank=491 done reading total 56862697472 bytes from needed files.
Rank=673 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/35.bin size=56862697472, starting from offset=0.
Rank=673 done reading total 56862697472 bytes from needed files.
Rank=195 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/612.bin size=56862697472, starting from offset=0.
Rank=195 done reading total 56862697472 bytes from needed files.
Rank=749 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/736.bin size=56862697472, starting from offset=0.
Rank=749 done reading total 56862697472 bytes from needed files.
Rank=933 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1014.bin size=56862697472, starting from offset=0.
Rank=933 done reading total 56862697472 bytes from needed files.
Rank=824 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/811.bin size=56862697472, starting from offset=0.
Rank=824 done reading total 56862697472 bytes from needed files.
Rank=600 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/530.bin size=56862697472, starting from offset=0.
Rank=600 done reading total 56862697472 bytes from needed files.
Rank=966 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/905.bin size=56862697472, starting from offset=0.
Rank=966 done reading total 56862697472 bytes from needed files.
Rank=68 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/29.bin size=56862697472, starting from offset=0.
Rank=68 done reading total 56862697472 bytes from needed files.
Rank=330 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/679.bin size=56862697472, starting from offset=0.
Rank=330 done reading total 56862697472 bytes from needed files.
Rank=354 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/590.bin size=56862697472, starting from offset=0.
Rank=354 done reading total 56862697472 bytes from needed files.
Rank=439 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/238.bin size=56862697472, starting from offset=0.
Rank=439 done reading total 56862697472 bytes from needed files.
Rank=111 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/670.bin size=56862697472, starting from offset=0.
Rank=111 done reading total 56862697472 bytes from needed files.
Rank=739 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/235.bin size=56862697472, starting from offset=0.
Rank=739 done reading total 56862697472 bytes from needed files.
Rank=127 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/563.bin size=56862697472, starting from offset=0.
Rank=127 done reading total 56862697472 bytes from needed files.
Rank=595 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/697.bin size=56862697472, starting from offset=0.
Rank=595 done reading total 56862697472 bytes from needed files.
Rank=972 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/572.bin size=56862697472, starting from offset=0.
Rank=972 done reading total 56862697472 bytes from needed files.
Rank=292 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/406.bin size=56862697472, starting from offset=0.
Rank=292 done reading total 56862697472 bytes from needed files.
Rank=647 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/315.bin size=56862697472, starting from offset=0.
Rank=647 done reading total 56862697472 bytes from needed files.
Rank=407 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/716.bin size=56862697472, starting from offset=0.
Rank=407 done reading total 56862697472 bytes from needed files.
Rank=575 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/365.bin size=56862697472, starting from offset=0.
Rank=575 done reading total 56862697472 bytes from needed files.
Rank=918 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/468.bin size=56862697472, starting from offset=0.
Rank=918 done reading total 56862697472 bytes from needed files.
Rank=965 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/797.bin size=56862697472, starting from offset=0.
Rank=965 done reading total 56862697472 bytes from needed files.
Rank=504 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/366.bin size=56862697472, starting from offset=0.
Rank=504 done reading total 56862697472 bytes from needed files.
Rank=964 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/798.bin size=56862697472, starting from offset=0.
Rank=964 done reading total 56862697472 bytes from needed files.
Rank=411 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/78.bin size=56862697472, starting from offset=0.
Rank=411 done reading total 56862697472 bytes from needed files.
Rank=490 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/965.bin size=56862697472, starting from offset=0.
Rank=490 done reading total 56862697472 bytes from needed files.
Rank=328 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/359.bin size=56862697472, starting from offset=0.
Rank=328 done reading total 56862697472 bytes from needed files.
Rank=434 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/550.bin size=56862697472, starting from offset=0.
Rank=434 done reading total 56862697472 bytes from needed files.
Rank=297 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/358.bin size=56862697472, starting from offset=0.
Rank=297 done reading total 56862697472 bytes from needed files.
Rank=975 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/733.bin size=56862697472, starting from offset=0.
Rank=975 done reading total 56862697472 bytes from needed files.
Rank=46 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/343.bin size=56862697472, starting from offset=0.
Rank=46 done reading total 56862697472 bytes from needed files.
Rank=761 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1013.bin size=56862697472, starting from offset=0.
Rank=761 done reading total 56862697472 bytes from needed files.
Rank=498 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/912.bin size=56862697472, starting from offset=0.
Rank=498 done reading total 56862697472 bytes from needed files.
Rank=228 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/781.bin size=56862697472, starting from offset=0.
Rank=228 done reading total 56862697472 bytes from needed files.
Rank=1000 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/472.bin size=56862697472, starting from offset=0.
Rank=1000 done reading total 56862697472 bytes from needed files.
Rank=871 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/756.bin size=56862697472, starting from offset=0.
Rank=871 done reading total 56862697472 bytes from needed files.
Rank=309 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/966.bin size=56862697472, starting from offset=0.
Rank=309 done reading total 56862697472 bytes from needed files.
Rank=959 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/264.bin size=56862697472, starting from offset=0.
Rank=959 done reading total 56862697472 bytes from needed files.
Rank=921 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/380.bin size=56862697472, starting from offset=0.
Rank=921 done reading total 56862697472 bytes from needed files.
Rank=869 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/709.bin size=56862697472, starting from offset=0.
Rank=869 done reading total 56862697472 bytes from needed files.
Rank=449 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/720.bin size=56862697472, starting from offset=0.
Rank=449 done reading total 56862697472 bytes from needed files.
Rank=781 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/901.bin size=56862697472, starting from offset=0.
Rank=781 done reading total 56862697472 bytes from needed files.
Rank=991 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/277.bin size=56862697472, starting from offset=0.
Rank=991 done reading total 56862697472 bytes from needed files.
Rank=54 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/318.bin size=56862697472, starting from offset=0.
Rank=54 done reading total 56862697472 bytes from needed files.
Rank=863 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/477.bin size=56862697472, starting from offset=0.
Rank=863 done reading total 56862697472 bytes from needed files.
Rank=169 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/608.bin size=56862697472, starting from offset=0.
Rank=169 done reading total 56862697472 bytes from needed files.
Rank=116 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/706.bin size=56862697472, starting from offset=0.
Rank=116 done reading total 56862697472 bytes from needed files.
Rank=260 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/721.bin size=56862697472, starting from offset=0.
Rank=260 done reading total 56862697472 bytes from needed files.
Rank=359 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/649.bin size=56862697472, starting from offset=0.
Rank=359 done reading total 56862697472 bytes from needed files.
Rank=659 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/160.bin size=56862697472, starting from offset=0.
Rank=659 done reading total 56862697472 bytes from needed files.
Rank=410 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/900.bin size=56862697472, starting from offset=0.
Rank=410 done reading total 56862697472 bytes from needed files.
Rank=67 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/941.bin size=56862697472, starting from offset=0.
Rank=67 done reading total 56862697472 bytes from needed files.
Rank=780 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/399.bin size=56862697472, starting from offset=0.
Rank=780 done reading total 56862697472 bytes from needed files.
Rank=151 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/422.bin size=56862697472, starting from offset=0.
Rank=151 done reading total 56862697472 bytes from needed files.
Rank=930 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/799.bin size=56862697472, starting from offset=0.
Rank=930 done reading total 56862697472 bytes from needed files.
Rank=679 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/604.bin size=56862697472, starting from offset=0.
Rank=679 done reading total 56862697472 bytes from needed files.
Rank=920 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/537.bin size=56862697472, starting from offset=0.
Rank=920 done reading total 56862697472 bytes from needed files.
Rank=329 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/531.bin size=56862697472, starting from offset=0.
Rank=329 done reading total 56862697472 bytes from needed files.
Rank=179 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/328.bin size=56862697472, starting from offset=0.
Rank=179 done reading total 56862697472 bytes from needed files.
Rank=234 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/209.bin size=56862697472, starting from offset=0.
Rank=234 done reading total 56862697472 bytes from needed files.
Rank=381 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/818.bin size=56862697472, starting from offset=0.
Rank=381 done reading total 56862697472 bytes from needed files.
Rank=277 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/611.bin size=56862697472, starting from offset=0.
Rank=277 done reading total 56862697472 bytes from needed files.
Rank=916 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/379.bin size=56862697472, starting from offset=0.
Rank=916 done reading total 56862697472 bytes from needed files.
Rank=784 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/471.bin size=56862697472, starting from offset=0.
Rank=784 done reading total 56862697472 bytes from needed files.
Rank=645 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/646.bin size=56862697472, starting from offset=0.
Rank=645 done reading total 56862697472 bytes from needed files.
Rank=899 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/824.bin size=56862697472, starting from offset=0.
Rank=899 done reading total 56862697472 bytes from needed files.
Rank=864 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/761.bin size=56862697472, starting from offset=0.
Rank=864 done reading total 56862697472 bytes from needed files.
Rank=917 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/390.bin size=56862697472, starting from offset=0.
Rank=917 done reading total 56862697472 bytes from needed files.
Rank=326 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/49.bin size=56862697472, starting from offset=0.
Rank=326 done reading total 56862697472 bytes from needed files.
Rank=657 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/71.bin size=56862697472, starting from offset=0.
Rank=657 done reading total 56862697472 bytes from needed files.
Rank=472 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/565.bin size=56862697472, starting from offset=0.
Rank=472 done reading total 56862697472 bytes from needed files.
Rank=746 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/743.bin size=56862697472, starting from offset=0.
Rank=746 done reading total 56862697472 bytes from needed files.
Rank=547 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/282.bin size=56862697472, starting from offset=0.
Rank=547 done reading total 56862697472 bytes from needed files.
Rank=261 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1001.bin size=56862697472, starting from offset=0.
Rank=261 done reading total 56862697472 bytes from needed files.
Rank=2 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/94.bin size=56862697472, starting from offset=0.
Rank=2 done reading total 56862697472 bytes from needed files.
Rank=1023 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/185.bin size=56862697472, starting from offset=0.
Rank=1023 done reading total 56862697472 bytes from needed files.
Rank=437 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/93.bin size=56862697472, starting from offset=0.
Rank=437 done reading total 56862697472 bytes from needed files.
Rank=756 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/217.bin size=56862697472, starting from offset=0.
Rank=756 done reading total 56862697472 bytes from needed files.
Rank=683 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/257.bin size=56862697472, starting from offset=0.
Rank=683 done reading total 56862697472 bytes from needed files.
Rank=17 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/510.bin size=56862697472, starting from offset=0.
Rank=17 done reading total 56862697472 bytes from needed files.
Rank=823 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/774.bin size=56862697472, starting from offset=0.
Rank=823 done reading total 56862697472 bytes from needed files.
Rank=275 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/627.bin size=56862697472, starting from offset=0.
Rank=275 done reading total 56862697472 bytes from needed files.
Rank=231 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/170.bin size=56862697472, starting from offset=0.
Rank=231 done reading total 56862697472 bytes from needed files.
Rank=767 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/874.bin size=56862697472, starting from offset=0.
Rank=767 done reading total 56862697472 bytes from needed files.
Rank=0 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/938.bin size=56862697472, starting from offset=0.
Rank=0 done reading total 56862697472 bytes from needed files.
Rank=688 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/207.bin size=56862697472, starting from offset=0.
Rank=688 done reading total 56862697472 bytes from needed files.
Rank=8 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/119.bin size=56862697472, starting from offset=0.
Rank=8 done reading total 56862697472 bytes from needed files.
Rank=367 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/745.bin size=56862697472, starting from offset=0.
Rank=367 done reading total 56862697472 bytes from needed files.
Rank=278 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/871.bin size=56862697472, starting from offset=0.
Rank=278 done reading total 56862697472 bytes from needed files.
Rank=245 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/305.bin size=56862697472, starting from offset=0.
Rank=245 done reading total 56862697472 bytes from needed files.
Rank=684 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/653.bin size=56862697472, starting from offset=0.
Rank=684 done reading total 56862697472 bytes from needed files.
Rank=352 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/147.bin size=56862697472, starting from offset=0.
Rank=352 done reading total 56862697472 bytes from needed files.
Rank=107 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/558.bin size=56862697472, starting from offset=0.
Rank=107 done reading total 56862697472 bytes from needed files.
Rank=198 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/642.bin size=56862697472, starting from offset=0.
Rank=198 done reading total 56862697472 bytes from needed files.
Rank=1020 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/80.bin size=56862697472, starting from offset=0.
Rank=1020 done reading total 56862697472 bytes from needed files.
Rank=208 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/796.bin size=56862697472, starting from offset=0.
Rank=208 done reading total 56862697472 bytes from needed files.
Rank=380 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/668.bin size=56862697472, starting from offset=0.
Rank=380 done reading total 56862697472 bytes from needed files.
Rank=473 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/224.bin size=56862697472, starting from offset=0.
Rank=473 done reading total 56862697472 bytes from needed files.
Rank=1002 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/663.bin size=56862697472, starting from offset=0.
Rank=1002 done reading total 56862697472 bytes from needed files.
Rank=974 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/579.bin size=56862697472, starting from offset=0.
Rank=974 done reading total 56862697472 bytes from needed files.
Rank=989 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/505.bin size=56862697472, starting from offset=0.
Rank=989 done reading total 56862697472 bytes from needed files.
Rank=897 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/829.bin size=56862697472, starting from offset=0.
Rank=897 done reading total 56862697472 bytes from needed files.
Rank=366 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/621.bin size=56862697472, starting from offset=0.
Rank=366 done reading total 56862697472 bytes from needed files.
Rank=311 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/240.bin size=56862697472, starting from offset=0.
Rank=311 done reading total 56862697472 bytes from needed files.
Rank=590 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/43.bin size=56862697472, starting from offset=0.
Rank=590 done reading total 56862697472 bytes from needed files.
Rank=478 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/475.bin size=56862697472, starting from offset=0.
Rank=478 done reading total 56862697472 bytes from needed files.
Rank=399 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/969.bin size=56862697472, starting from offset=0.
Rank=399 done reading total 56862697472 bytes from needed files.
Rank=760 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/655.bin size=56862697472, starting from offset=0.
Rank=760 done reading total 56862697472 bytes from needed files.
Rank=232 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/9.bin size=56862697472, starting from offset=0.
Rank=232 done reading total 56862697472 bytes from needed files.
Rank=458 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/221.bin size=56862697472, starting from offset=0.
Rank=458 done reading total 56862697472 bytes from needed files.
Rank=584 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/375.bin size=56862697472, starting from offset=0.
Rank=584 done reading total 56862697472 bytes from needed files.
Rank=530 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/666.bin size=56862697472, starting from offset=0.
Rank=530 done reading total 56862697472 bytes from needed files.
Rank=1 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/731.bin size=56862697472, starting from offset=0.
Rank=1 done reading total 56862697472 bytes from needed files.
Rank=821 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/198.bin size=56862697472, starting from offset=0.
Rank=821 done reading total 56862697472 bytes from needed files.
Rank=296 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/585.bin size=56862697472, starting from offset=0.
Rank=296 done reading total 56862697472 bytes from needed files.
Rank=751 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/710.bin size=56862697472, starting from offset=0.
Rank=751 done reading total 56862697472 bytes from needed files.
Rank=973 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1009.bin size=56862697472, starting from offset=0.
Rank=973 done reading total 56862697472 bytes from needed files.
Rank=212 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/33.bin size=56862697472, starting from offset=0.
Rank=212 done reading total 56862697472 bytes from needed files.
Rank=1021 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/658.bin size=56862697472, starting from offset=0.
Rank=1021 done reading total 56862697472 bytes from needed files.
Rank=1008 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/189.bin size=56862697472, starting from offset=0.
Rank=1008 done reading total 56862697472 bytes from needed files.
Rank=790 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/920.bin size=56862697472, starting from offset=0.
Rank=790 done reading total 56862697472 bytes from needed files.
Rank=105 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/929.bin size=56862697472, starting from offset=0.
Rank=105 done reading total 56862697472 bytes from needed files.
Rank=47 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/91.bin size=56862697472, starting from offset=0.
Rank=47 done reading total 56862697472 bytes from needed files.
Rank=441 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/187.bin size=56862697472, starting from offset=0.
Rank=441 done reading total 56862697472 bytes from needed files.
Rank=712 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/398.bin size=56862697472, starting from offset=0.
Rank=712 done reading total 56862697472 bytes from needed files.
Rank=868 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/596.bin size=56862697472, starting from offset=0.
Rank=868 done reading total 56862697472 bytes from needed files.
Rank=235 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/946.bin size=56862697472, starting from offset=0.
Rank=235 done reading total 56862697472 bytes from needed files.
Rank=531 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/413.bin size=56862697472, starting from offset=0.
Rank=531 done reading total 56862697472 bytes from needed files.
Rank=460 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/333.bin size=56862697472, starting from offset=0.
Rank=460 done reading total 56862697472 bytes from needed files.
Rank=124 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/673.bin size=56862697472, starting from offset=0.
Rank=124 done reading total 56862697472 bytes from needed files.
Rank=775 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/589.bin size=56862697472, starting from offset=0.
Rank=775 done reading total 56862697472 bytes from needed files.
Rank=276 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/278.bin size=56862697472, starting from offset=0.
Rank=276 done reading total 56862697472 bytes from needed files.
Rank=371 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/806.bin size=56862697472, starting from offset=0.
Rank=371 done reading total 56862697472 bytes from needed files.
Rank=662 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/493.bin size=56862697472, starting from offset=0.
Rank=662 done reading total 56862697472 bytes from needed files.
Rank=210 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/587.bin size=56862697472, starting from offset=0.
Rank=210 done reading total 56862697472 bytes from needed files.
Rank=524 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/976.bin size=56862697472, starting from offset=0.
Rank=524 done reading total 56862697472 bytes from needed files.
Rank=463 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/819.bin size=56862697472, starting from offset=0.
Rank=463 done reading total 56862697472 bytes from needed files.
Rank=385 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/917.bin size=56862697472, starting from offset=0.
Rank=385 done reading total 56862697472 bytes from needed files.
Rank=782 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/503.bin size=56862697472, starting from offset=0.
Rank=782 done reading total 56862697472 bytes from needed files.
Rank=623 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/196.bin size=56862697472, starting from offset=0.
Rank=623 done reading total 56862697472 bytes from needed files.
Rank=845 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/801.bin size=56862697472, starting from offset=0.
Rank=845 done reading total 56862697472 bytes from needed files.
Rank=431 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/84.bin size=56862697472, starting from offset=0.
Rank=431 done reading total 56862697472 bytes from needed files.
Rank=988 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/506.bin size=56862697472, starting from offset=0.
Rank=988 done reading total 56862697472 bytes from needed files.
Rank=331 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/460.bin size=56862697472, starting from offset=0.
Rank=331 done reading total 56862697472 bytes from needed files.
Rank=506 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/441.bin size=56862697472, starting from offset=0.
Rank=506 done reading total 56862697472 bytes from needed files.
Rank=126 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/177.bin size=56862697472, starting from offset=0.
Rank=126 done reading total 56862697472 bytes from needed files.
Rank=199 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/726.bin size=56862697472, starting from offset=0.
Rank=199 done reading total 56862697472 bytes from needed files.
Rank=826 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/245.bin size=56862697472, starting from offset=0.
Rank=826 done reading total 56862697472 bytes from needed files.
Rank=435 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/267.bin size=56862697472, starting from offset=0.
Rank=435 done reading total 56862697472 bytes from needed files.
Rank=125 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/635.bin size=56862697472, starting from offset=0.
Rank=125 done reading total 56862697472 bytes from needed files.
Rank=106 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/944.bin size=56862697472, starting from offset=0.
Rank=106 done reading total 56862697472 bytes from needed files.
Rank=209 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/396.bin size=56862697472, starting from offset=0.
Rank=209 done reading total 56862697472 bytes from needed files.
Rank=895 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/868.bin size=56862697472, starting from offset=0.
Rank=895 done reading total 56862697472 bytes from needed files.
Rank=766 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/768.bin size=56862697472, starting from offset=0.
Rank=766 done reading total 56862697472 bytes from needed files.
Rank=663 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/50.bin size=56862697472, starting from offset=0.
Rank=663 done reading total 56862697472 bytes from needed files.
Rank=298 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/875.bin size=56862697472, starting from offset=0.
Rank=298 done reading total 56862697472 bytes from needed files.
Rank=51 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/325.bin size=56862697472, starting from offset=0.
Rank=51 done reading total 56862697472 bytes from needed files.
Rank=551 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/755.bin size=56862697472, starting from offset=0.
Rank=551 done reading total 56862697472 bytes from needed files.
Rank=865 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/889.bin size=56862697472, starting from offset=0.
Rank=865 done reading total 56862697472 bytes from needed files.
Rank=369 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/174.bin size=56862697472, starting from offset=0.
Rank=369 done reading total 56862697472 bytes from needed files.
Rank=211 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/560.bin size=56862697472, starting from offset=0.
Rank=211 done reading total 56862697472 bytes from needed files.
Rank=687 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/835.bin size=56862697472, starting from offset=0.
Rank=687 done reading total 56862697472 bytes from needed files.
Rank=892 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/982.bin size=56862697472, starting from offset=0.
Rank=892 done reading total 56862697472 bytes from needed files.
Rank=1011 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/980.bin size=56862697472, starting from offset=0.
Rank=1011 done reading total 56862697472 bytes from needed files.
Rank=432 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/922.bin size=56862697472, starting from offset=0.
Rank=432 done reading total 56862697472 bytes from needed files.
Rank=312 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/193.bin size=56862697472, starting from offset=0.
Rank=312 done reading total 56862697472 bytes from needed files.
Rank=476 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/485.bin size=56862697472, starting from offset=0.
Rank=476 done reading total 56862697472 bytes from needed files.
Rank=49 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/306.bin size=56862697472, starting from offset=0.
Rank=49 done reading total 56862697472 bytes from needed files.
Rank=264 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/117.bin size=56862697472, starting from offset=0.
Rank=264 done reading total 56862697472 bytes from needed files.
Rank=807 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/516.bin size=56862697472, starting from offset=0.
Rank=807 done reading total 56862697472 bytes from needed files.
Rank=599 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/828.bin size=56862697472, starting from offset=0.
Rank=599 done reading total 56862697472 bytes from needed files.
Rank=765 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/0.bin size=56862697472, starting from offset=0.
Rank=765 done reading total 56862697472 bytes from needed files.
Rank=893 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1022.bin size=56862697472, starting from offset=0.
Rank=893 done reading total 56862697472 bytes from needed files.
Rank=44 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/373.bin size=56862697472, starting from offset=0.
Rank=44 done reading total 56862697472 bytes from needed files.
Rank=822 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/101.bin size=56862697472, starting from offset=0.
Rank=822 done reading total 56862697472 bytes from needed files.
Rank=681 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/895.bin size=56862697472, starting from offset=0.
Rank=681 done reading total 56862697472 bytes from needed files.
Rank=898 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/30.bin size=56862697472, starting from offset=0.
Rank=898 done reading total 56862697472 bytes from needed files.
Rank=544 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/669.bin size=56862697472, starting from offset=0.
Rank=544 done reading total 56862697472 bytes from needed files.
Rank=283 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/502.bin size=56862697472, starting from offset=0.
Rank=283 done reading total 56862697472 bytes from needed files.
Rank=7 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/220.bin size=56862697472, starting from offset=0.
Rank=7 done reading total 56862697472 bytes from needed files.
Rank=197 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/730.bin size=56862697472, starting from offset=0.
Rank=197 done reading total 56862697472 bytes from needed files.
Rank=196 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1006.bin size=56862697472, starting from offset=0.
Rank=196 done reading total 56862697472 bytes from needed files.
Rank=559 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/972.bin size=56862697472, starting from offset=0.
Rank=559 done reading total 56862697472 bytes from needed files.
Rank=680 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/469.bin size=56862697472, starting from offset=0.
Rank=680 done reading total 56862697472 bytes from needed files.
Rank=847 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/789.bin size=56862697472, starting from offset=0.
Rank=847 done reading total 56862697472 bytes from needed files.
Rank=587 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/392.bin size=56862697472, starting from offset=0.
Rank=587 done reading total 56862697472 bytes from needed files.
Rank=206 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/169.bin size=56862697472, starting from offset=0.
Rank=206 done reading total 56862697472 bytes from needed files.
Rank=33 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/810.bin size=56862697472, starting from offset=0.
Rank=33 done reading total 56862697472 bytes from needed files.
Rank=229 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/32.bin size=56862697472, starting from offset=0.
Rank=229 done reading total 56862697472 bytes from needed files.
Rank=675 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/613.bin size=56862697472, starting from offset=0.
Rank=675 done reading total 56862697472 bytes from needed files.
Rank=34 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/521.bin size=56862697472, starting from offset=0.
Rank=34 done reading total 56862697472 bytes from needed files.
Rank=786 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/216.bin size=56862697472, starting from offset=0.
Rank=786 done reading total 56862697472 bytes from needed files.
Rank=64 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/594.bin size=56862697472, starting from offset=0.
Rank=64 done reading total 56862697472 bytes from needed files.
Rank=39 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/508.bin size=56862697472, starting from offset=0.
Rank=39 done reading total 56862697472 bytes from needed files.
Rank=580 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/795.bin size=56862697472, starting from offset=0.
Rank=580 done reading total 56862697472 bytes from needed files.
Rank=797 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/890.bin size=56862697472, starting from offset=0.
Rank=797 done reading total 56862697472 bytes from needed files.
Rank=527 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/81.bin size=56862697472, starting from offset=0.
Rank=527 done reading total 56862697472 bytes from needed files.
Rank=279 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/949.bin size=56862697472, starting from offset=0.
Rank=279 done reading total 56862697472 bytes from needed files.
Rank=321 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/403.bin size=56862697472, starting from offset=0.
Rank=321 done reading total 56862697472 bytes from needed files.
Rank=461 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/455.bin size=56862697472, starting from offset=0.
Rank=461 done reading total 56862697472 bytes from needed files.
Rank=660 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/958.bin size=56862697472, starting from offset=0.
Rank=660 done reading total 56862697472 bytes from needed files.
Rank=820 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/474.bin size=56862697472, starting from offset=0.
Rank=820 done reading total 56862697472 bytes from needed files.
Rank=215 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/249.bin size=56862697472, starting from offset=0.
Rank=215 done reading total 56862697472 bytes from needed files.
Rank=1012 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/28.bin size=56862697472, starting from offset=0.
Rank=1012 done reading total 56862697472 bytes from needed files.
Rank=525 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/293.bin size=56862697472, starting from offset=0.
Rank=525 done reading total 56862697472 bytes from needed files.
Rank=887 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/814.bin size=56862697472, starting from offset=0.
Rank=887 done reading total 56862697472 bytes from needed files.
Rank=214 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/417.bin size=56862697472, starting from offset=0.
Rank=214 done reading total 56862697472 bytes from needed files.
Rank=436 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/31.bin size=56862697472, starting from offset=0.
Rank=436 done reading total 56862697472 bytes from needed files.
Rank=433 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/832.bin size=56862697472, starting from offset=0.
Rank=433 done reading total 56862697472 bytes from needed files.
Rank=552 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/856.bin size=56862697472, starting from offset=0.
Rank=552 done reading total 56862697472 bytes from needed files.
Rank=549 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/263.bin size=56862697472, starting from offset=0.
Rank=549 done reading total 56862697472 bytes from needed files.
Rank=560 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/383.bin size=56862697472, starting from offset=0.
Rank=560 done reading total 56862697472 bytes from needed files.
Rank=323 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/139.bin size=56862697472, starting from offset=0.
Rank=323 done reading total 56862697472 bytes from needed files.
Rank=763 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/960.bin size=56862697472, starting from offset=0.
Rank=763 done reading total 56862697472 bytes from needed files.
Rank=550 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/975.bin size=56862697472, starting from offset=0.
Rank=550 done reading total 56862697472 bytes from needed files.
Rank=870 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/259.bin size=56862697472, starting from offset=0.
Rank=870 done reading total 56862697472 bytes from needed files.
Rank=403 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/813.bin size=56862697472, starting from offset=0.
Rank=403 done reading total 56862697472 bytes from needed files.
Rank=548 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/72.bin size=56862697472, starting from offset=0.
Rank=548 done reading total 56862697472 bytes from needed files.
Rank=1013 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/214.bin size=56862697472, starting from offset=0.
Rank=1013 done reading total 56862697472 bytes from needed files.
Rank=564 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/345.bin size=56862697472, starting from offset=0.
Rank=564 done reading total 56862697472 bytes from needed files.
Rank=799 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/230.bin size=56862697472, starting from offset=0.
Rank=799 done reading total 56862697472 bytes from needed files.
Rank=579 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/805.bin size=56862697472, starting from offset=0.
Rank=579 done reading total 56862697472 bytes from needed files.
Rank=1015 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/152.bin size=56862697472, starting from offset=0.
Rank=1015 done reading total 56862697472 bytes from needed files.
Rank=661 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/694.bin size=56862697472, starting from offset=0.
Rank=661 done reading total 56862697472 bytes from needed files.
Rank=734 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/617.bin size=56862697472, starting from offset=0.
Rank=734 done reading total 56862697472 bytes from needed files.
Rank=91 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/586.bin size=56862697472, starting from offset=0.
Rank=91 done reading total 56862697472 bytes from needed files.
Rank=764 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/693.bin size=56862697472, starting from offset=0.
Rank=764 done reading total 56862697472 bytes from needed files.
Rank=428 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/999.bin size=56862697472, starting from offset=0.
Rank=428 done reading total 56862697472 bytes from needed files.
Rank=526 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/17.bin size=56862697472, starting from offset=0.
Rank=526 done reading total 56862697472 bytes from needed files.
Rank=529 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/208.bin size=56862697472, starting from offset=0.
Rank=529 done reading total 56862697472 bytes from needed files.
Rank=213 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/466.bin size=56862697472, starting from offset=0.
Rank=213 done reading total 56862697472 bytes from needed files.
Rank=889 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/309.bin size=56862697472, starting from offset=0.
Rank=889 done reading total 56862697472 bytes from needed files.
Rank=135 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/41.bin size=56862697472, starting from offset=0.
Rank=135 done reading total 56862697472 bytes from needed files.
Rank=281 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/197.bin size=56862697472, starting from offset=0.
Rank=281 done reading total 56862697472 bytes from needed files.
Rank=271 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/444.bin size=56862697472, starting from offset=0.
Rank=271 done reading total 56862697472 bytes from needed files.
Rank=528 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/652.bin size=56862697472, starting from offset=0.
Rank=528 done reading total 56862697472 bytes from needed files.
Rank=520 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/570.bin size=56862697472, starting from offset=0.
Rank=520 done reading total 56862697472 bytes from needed files.
Rank=104 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/86.bin size=56862697472, starting from offset=0.
Rank=104 done reading total 56862697472 bytes from needed files.
Rank=269 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/26.bin size=56862697472, starting from offset=0.
Rank=269 done reading total 56862697472 bytes from needed files.
Rank=608 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/60.bin size=56862697472, starting from offset=0.
Rank=608 done reading total 56862697472 bytes from needed files.
Rank=785 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/882.bin size=56862697472, starting from offset=0.
Rank=785 done reading total 56862697472 bytes from needed files.
Rank=990 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/428.bin size=56862697472, starting from offset=0.
Rank=990 done reading total 56862697472 bytes from needed files.
Rank=861 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/372.bin size=56862697472, starting from offset=0.
Rank=861 done reading total 56862697472 bytes from needed files.
Rank=95 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/748.bin size=56862697472, starting from offset=0.
Rank=95 done reading total 56862697472 bytes from needed files.
Rank=3 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/986.bin size=56862697472, starting from offset=0.
Rank=3 done reading total 56862697472 bytes from needed files.
Rank=586 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/226.bin size=56862697472, starting from offset=0.
Rank=586 done reading total 56862697472 bytes from needed files.
Rank=5 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/925.bin size=56862697472, starting from offset=0.
Rank=5 done reading total 56862697472 bytes from needed files.
Rank=488 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/556.bin size=56862697472, starting from offset=0.
Rank=488 done reading total 56862697472 bytes from needed files.
Rank=93 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/599.bin size=56862697472, starting from offset=0.
Rank=93 done reading total 56862697472 bytes from needed files.
Rank=557 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/864.bin size=56862697472, starting from offset=0.
Rank=557 done reading total 56862697472 bytes from needed files.
Rank=924 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/973.bin size=56862697472, starting from offset=0.
Rank=924 done reading total 56862697472 bytes from needed files.
Rank=682 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/695.bin size=56862697472, starting from offset=0.
Rank=682 done reading total 56862697472 bytes from needed files.
Rank=207 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/747.bin size=56862697472, starting from offset=0.
Rank=207 done reading total 56862697472 bytes from needed files.
Rank=859 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/256.bin size=56862697472, starting from offset=0.
Rank=859 done reading total 56862697472 bytes from needed files.
Rank=738 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/909.bin size=56862697472, starting from offset=0.
Rank=738 done reading total 56862697472 bytes from needed files.
Rank=370 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/554.bin size=56862697472, starting from offset=0.
Rank=370 done reading total 56862697472 bytes from needed files.
Rank=202 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/161.bin size=56862697472, starting from offset=0.
Rank=202 done reading total 56862697472 bytes from needed files.
Rank=720 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/654.bin size=56862697472, starting from offset=0.
Rank=720 done reading total 56862697472 bytes from needed files.
Rank=925 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/540.bin size=56862697472, starting from offset=0.
Rank=925 done reading total 56862697472 bytes from needed files.
Rank=927 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/672.bin size=56862697472, starting from offset=0.
Rank=927 done reading total 56862697472 bytes from needed files.
Rank=268 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/618.bin size=56862697472, starting from offset=0.
Rank=268 done reading total 56862697472 bytes from needed files.
Rank=6 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/92.bin size=56862697472, starting from offset=0.
Rank=6 done reading total 56862697472 bytes from needed files.
Rank=397 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/495.bin size=56862697472, starting from offset=0.
Rank=397 done reading total 56862697472 bytes from needed files.
Rank=894 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/479.bin size=56862697472, starting from offset=0.
Rank=894 done reading total 56862697472 bytes from needed files.
Rank=398 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/817.bin size=56862697472, starting from offset=0.
Rank=398 done reading total 56862697472 bytes from needed files.
Rank=270 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/360.bin size=56862697472, starting from offset=0.
Rank=270 done reading total 56862697472 bytes from needed files.
Rank=926 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/202.bin size=56862697472, starting from offset=0.
Rank=926 done reading total 56862697472 bytes from needed files.
Rank=1010 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/665.bin size=56862697472, starting from offset=0.
Rank=1010 done reading total 56862697472 bytes from needed files.
Rank=205 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/924.bin size=56862697472, starting from offset=0.
Rank=205 done reading total 56862697472 bytes from needed files.
Rank=686 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/409.bin size=56862697472, starting from offset=0.
Rank=686 done reading total 56862697472 bytes from needed files.
Rank=862 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/195.bin size=56862697472, starting from offset=0.
Rank=862 done reading total 56862697472 bytes from needed files.
Rank=416 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/156.bin size=56862697472, starting from offset=0.
Rank=416 done reading total 56862697472 bytes from needed files.
Rank=812 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/873.bin size=56862697472, starting from offset=0.
Rank=812 done reading total 56862697472 bytes from needed files.
Rank=426 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/464.bin size=56862697472, starting from offset=0.
Rank=426 done reading total 56862697472 bytes from needed files.
Rank=806 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/887.bin size=56862697472, starting from offset=0.
Rank=806 done reading total 56862697472 bytes from needed files.
Rank=736 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/166.bin size=56862697472, starting from offset=0.
Rank=736 done reading total 56862697472 bytes from needed files.
Rank=606 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/340.bin size=56862697472, starting from offset=0.
Rank=606 done reading total 56862697472 bytes from needed files.
Rank=475 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/888.bin size=56862697472, starting from offset=0.
Rank=475 done reading total 56862697472 bytes from needed files.
Rank=368 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/436.bin size=56862697472, starting from offset=0.
Rank=368 done reading total 56862697472 bytes from needed files.
Rank=45 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/352.bin size=56862697472, starting from offset=0.
Rank=45 done reading total 56862697472 bytes from needed files.
Rank=474 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/67.bin size=56862697472, starting from offset=0.
Rank=474 done reading total 56862697472 bytes from needed files.
Rank=489 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/283.bin size=56862697472, starting from offset=0.
Rank=489 done reading total 56862697472 bytes from needed files.
Rank=815 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/996.bin size=56862697472, starting from offset=0.
Rank=815 done reading total 56862697472 bytes from needed files.
Rank=429 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/659.bin size=56862697472, starting from offset=0.
Rank=429 done reading total 56862697472 bytes from needed files.
Rank=35 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/712.bin size=56862697472, starting from offset=0.
Rank=35 done reading total 56862697472 bytes from needed files.
Rank=1009 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/907.bin size=56862697472, starting from offset=0.
Rank=1009 done reading total 56862697472 bytes from needed files.
Rank=282 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/206.bin size=56862697472, starting from offset=0.
Rank=282 done reading total 56862697472 bytes from needed files.
Rank=419 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/40.bin size=56862697472, starting from offset=0.
Rank=419 done reading total 56862697472 bytes from needed files.
Rank=373 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/630.bin size=56862697472, starting from offset=0.
Rank=373 done reading total 56862697472 bytes from needed files.
Rank=556 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/751.bin size=56862697472, starting from offset=0.
Rank=556 done reading total 56862697472 bytes from needed files.
Rank=446 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/482.bin size=56862697472, starting from offset=0.
Rank=446 done reading total 56862697472 bytes from needed files.
Rank=858 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/800.bin size=56862697472, starting from offset=0.
Rank=858 done reading total 56862697472 bytes from needed files.
Rank=856 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/872.bin size=56862697472, starting from offset=0.
Rank=856 done reading total 56862697472 bytes from needed files.
Rank=993 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/106.bin size=56862697472, starting from offset=0.
Rank=993 done reading total 56862697472 bytes from needed files.
Rank=22 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/681.bin size=56862697472, starting from offset=0.
Rank=22 done reading total 56862697472 bytes from needed files.
Rank=789 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/377.bin size=56862697472, starting from offset=0.
Rank=789 done reading total 56862697472 bytes from needed files.
Rank=418 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/735.bin size=56862697472, starting from offset=0.
Rank=418 done reading total 56862697472 bytes from needed files.
Rank=430 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/158.bin size=56862697472, starting from offset=0.
Rank=430 done reading total 56862697472 bytes from needed files.
Rank=4 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/863.bin size=56862697472, starting from offset=0.
Rank=4 done reading total 56862697472 bytes from needed files.
Rank=492 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1012.bin size=56862697472, starting from offset=0.
Rank=492 done reading total 56862697472 bytes from needed files.
Rank=813 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/660.bin size=56862697472, starting from offset=0.
Rank=813 done reading total 56862697472 bytes from needed files.
Rank=495 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/321.bin size=56862697472, starting from offset=0.
Rank=495 done reading total 56862697472 bytes from needed files.
Rank=21 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/997.bin size=56862697472, starting from offset=0.
Rank=21 done reading total 56862697472 bytes from needed files.
Rank=791 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/63.bin size=56862697472, starting from offset=0.
Rank=791 done reading total 56862697472 bytes from needed files.
Rank=999 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/815.bin size=56862697472, starting from offset=0.
Rank=999 done reading total 56862697472 bytes from needed files.
Rank=634 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1020.bin size=56862697472, starting from offset=0.
Rank=634 done reading total 56862697472 bytes from needed files.
Rank=280 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/692.bin size=56862697472, starting from offset=0.
Rank=280 done reading total 56862697472 bytes from needed files.
Rank=805 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/628.bin size=56862697472, starting from offset=0.
Rank=805 done reading total 56862697472 bytes from needed files.
Rank=203 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/816.bin size=56862697472, starting from offset=0.
Rank=203 done reading total 56862697472 bytes from needed files.
Rank=402 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/46.bin size=56862697472, starting from offset=0.
Rank=402 done reading total 56862697472 bytes from needed files.
Rank=424 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/330.bin size=56862697472, starting from offset=0.
Rank=424 done reading total 56862697472 bytes from needed files.
Rank=401 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/143.bin size=56862697472, starting from offset=0.
Rank=401 done reading total 56862697472 bytes from needed files.
Rank=204 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/728.bin size=56862697472, starting from offset=0.
Rank=204 done reading total 56862697472 bytes from needed files.
Rank=632 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/304.bin size=56862697472, starting from offset=0.
Rank=632 done reading total 56862697472 bytes from needed files.
Rank=607 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/14.bin size=56862697472, starting from offset=0.
Rank=607 done reading total 56862697472 bytes from needed files.
Rank=134 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/839.bin size=56862697472, starting from offset=0.
Rank=134 done reading total 56862697472 bytes from needed files.
Rank=396 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/427.bin size=56862697472, starting from offset=0.
Rank=396 done reading total 56862697472 bytes from needed files.
Rank=726 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/606.bin size=56862697472, starting from offset=0.
Rank=726 done reading total 56862697472 bytes from needed files.
Rank=425 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/705.bin size=56862697472, starting from offset=0.
Rank=425 done reading total 56862697472 bytes from needed files.
Rank=200 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/727.bin size=56862697472, starting from offset=0.
Rank=200 done reading total 56862697472 bytes from needed files.
Rank=447 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/467.bin size=56862697472, starting from offset=0.
Rank=447 done reading total 56862697472 bytes from needed files.
Rank=444 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/489.bin size=56862697472, starting from offset=0.
Rank=444 done reading total 56862697472 bytes from needed files.
Rank=604 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/402.bin size=56862697472, starting from offset=0.
Rank=604 done reading total 56862697472 bytes from needed files.
Rank=494 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/45.bin size=56862697472, starting from offset=0.
Rank=494 done reading total 56862697472 bytes from needed files.
Rank=633 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/684.bin size=56862697472, starting from offset=0.
Rank=633 done reading total 56862697472 bytes from needed files.
Rank=390 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/308.bin size=56862697472, starting from offset=0.
Rank=390 done reading total 56862697472 bytes from needed files.
Rank=161 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/228.bin size=56862697472, starting from offset=0.
Rank=161 done reading total 56862697472 bytes from needed files.
Rank=23 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/310.bin size=56862697472, starting from offset=0.
Rank=23 done reading total 56862697472 bytes from needed files.
Rank=132 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/899.bin size=56862697472, starting from offset=0.
Rank=132 done reading total 56862697472 bytes from needed files.
Rank=997 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/517.bin size=56862697472, starting from offset=0.
Rank=997 done reading total 56862697472 bytes from needed files.
Rank=733 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/27.bin size=56862697472, starting from offset=0.
Rank=733 done reading total 56862697472 bytes from needed files.
Rank=334 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1000.bin size=56862697472, starting from offset=0.
Rank=334 done reading total 56862697472 bytes from needed files.
Rank=391 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/347.bin size=56862697472, starting from offset=0.
Rank=391 done reading total 56862697472 bytes from needed files.
Rank=609 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/614.bin size=56862697472, starting from offset=0.
Rank=609 done reading total 56862697472 bytes from needed files.
Rank=635 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/984.bin size=56862697472, starting from offset=0.
Rank=635 done reading total 56862697472 bytes from needed files.
Rank=388 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/286.bin size=56862697472, starting from offset=0.
Rank=388 done reading total 56862697472 bytes from needed files.
Rank=996 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/682.bin size=56862697472, starting from offset=0.
Rank=996 done reading total 56862697472 bytes from needed files.
Rank=201 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/484.bin size=56862697472, starting from offset=0.
Rank=201 done reading total 56862697472 bytes from needed files.
Rank=804 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/351.bin size=56862697472, starting from offset=0.
Rank=804 done reading total 56862697472 bytes from needed files.
Rank=427 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/371.bin size=56862697472, starting from offset=0.
Rank=427 done reading total 56862697472 bytes from needed files.
Rank=788 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/389.bin size=56862697472, starting from offset=0.
Rank=788 done reading total 56862697472 bytes from needed files.
Rank=92 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/408.bin size=56862697472, starting from offset=0.
Rank=92 done reading total 56862697472 bytes from needed files.
Rank=614 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/337.bin size=56862697472, starting from offset=0.
Rank=614 done reading total 56862697472 bytes from needed files.
Rank=722 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/302.bin size=56862697472, starting from offset=0.
Rank=722 done reading total 56862697472 bytes from needed files.
Rank=335 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/967.bin size=56862697472, starting from offset=0.
Rank=335 done reading total 56862697472 bytes from needed files.
Rank=133 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/764.bin size=56862697472, starting from offset=0.
Rank=133 done reading total 56862697472 bytes from needed files.
Rank=814 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/906.bin size=56862697472, starting from offset=0.
Rank=814 done reading total 56862697472 bytes from needed files.
Rank=165 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/661.bin size=56862697472, starting from offset=0.
Rank=165 done reading total 56862697472 bytes from needed files.
Rank=417 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/157.bin size=56862697472, starting from offset=0.
Rank=417 done reading total 56862697472 bytes from needed files.
Rank=20 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/324.bin size=56862697472, starting from offset=0.
Rank=20 done reading total 56862697472 bytes from needed files.
Rank=605 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/20.bin size=56862697472, starting from offset=0.
Rank=605 done reading total 56862697472 bytes from needed files.
Rank=163 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/902.bin size=56862697472, starting from offset=0.
Rank=163 done reading total 56862697472 bytes from needed files.
Rank=332 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/634.bin size=56862697472, starting from offset=0.
Rank=332 done reading total 56862697472 bytes from needed files.
Rank=611 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/95.bin size=56862697472, starting from offset=0.
Rank=611 done reading total 56862697472 bytes from needed files.
Rank=389 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/744.bin size=56862697472, starting from offset=0.
Rank=389 done reading total 56862697472 bytes from needed files.
Rank=857 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/711.bin size=56862697472, starting from offset=0.
Rank=857 done reading total 56862697472 bytes from needed files.
Rank=160 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/866.bin size=56862697472, starting from offset=0.
Rank=160 done reading total 56862697472 bytes from needed files.
Rank=445 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/114.bin size=56862697472, starting from offset=0.
Rank=445 done reading total 56862697472 bytes from needed files.
Rank=400 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/115.bin size=56862697472, starting from offset=0.
Rank=400 done reading total 56862697472 bytes from needed files.
Rank=493 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/752.bin size=56862697472, starting from offset=0.
Rank=493 done reading total 56862697472 bytes from needed files.
Rank=162 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/370.bin size=56862697472, starting from offset=0.
Rank=162 done reading total 56862697472 bytes from needed files.
Rank=638 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/176.bin size=56862697472, starting from offset=0.
Rank=638 done reading total 56862697472 bytes from needed files.
Rank=881 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/680.bin size=56862697472, starting from offset=0.
Rank=881 done reading total 56862697472 bytes from needed files.
Rank=884 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/571.bin size=56862697472, starting from offset=0.
Rank=884 done reading total 56862697472 bytes from needed files.
Rank=582 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/632.bin size=56862697472, starting from offset=0.
Rank=582 done reading total 56862697472 bytes from needed files.
Rank=724 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/689.bin size=56862697472, starting from offset=0.
Rank=724 done reading total 56862697472 bytes from needed files.
Rank=583 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/298.bin size=56862697472, starting from offset=0.
Rank=583 done reading total 56862697472 bytes from needed files.
Rank=333 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/138.bin size=56862697472, starting from offset=0.
Rank=333 done reading total 56862697472 bytes from needed files.
Rank=998 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/446.bin size=56862697472, starting from offset=0.
Rank=998 done reading total 56862697472 bytes from needed files.
Rank=944 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/124.bin size=56862697472, starting from offset=0.
Rank=944 done reading total 56862697472 bytes from needed files.
Rank=883 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/827.bin size=56862697472, starting from offset=0.
Rank=883 done reading total 56862697472 bytes from needed files.
Rank=610 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/465.bin size=56862697472, starting from offset=0.
Rank=610 done reading total 56862697472 bytes from needed files.
Rank=615 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/229.bin size=56862697472, starting from offset=0.
Rank=615 done reading total 56862697472 bytes from needed files.
Rank=882 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/577.bin size=56862697472, starting from offset=0.
Rank=882 done reading total 56862697472 bytes from needed files.
Rank=725 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/575.bin size=56862697472, starting from offset=0.
Rank=725 done reading total 56862697472 bytes from needed files.
Rank=880 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/948.bin size=56862697472, starting from offset=0.
Rank=880 done reading total 56862697472 bytes from needed files.
Rank=946 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/842.bin size=56862697472, starting from offset=0.
Rank=946 done reading total 56862697472 bytes from needed files.
Rank=613 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/542.bin size=56862697472, starting from offset=0.
Rank=613 done reading total 56862697472 bytes from needed files.
Rank=729 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/430.bin size=56862697472, starting from offset=0.
Rank=729 done reading total 56862697472 bytes from needed files.
Rank=655 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/1019.bin size=56862697472, starting from offset=0.
Rank=655 done reading total 56862697472 bytes from needed files.
Rank=728 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/523.bin size=56862697472, starting from offset=0.
Rank=728 done reading total 56862697472 bytes from needed files.
Rank=730 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/616.bin size=56862697472, starting from offset=0.
Rank=730 done reading total 56862697472 bytes from needed files.
Rank=945 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/914.bin size=56862697472, starting from offset=0.
Rank=945 done reading total 56862697472 bytes from needed files.
Rank=654 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/253.bin size=56862697472, starting from offset=0.
Rank=654 done reading total 56862697472 bytes from needed files.
Rank=948 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/384.bin size=56862697472, starting from offset=0.
Rank=948 done reading total 56862697472 bytes from needed files.
Rank=612 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/447.bin size=56862697472, starting from offset=0.
Rank=612 done reading total 56862697472 bytes from needed files.
Rank=731 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/896.bin size=56862697472, starting from offset=0.
Rank=731 done reading total 56862697472 bytes from needed files.
Rank=947 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/12.bin size=56862697472, starting from offset=0.
Rank=947 done reading total 56862697472 bytes from needed files.
Rank=652 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/765.bin size=56862697472, starting from offset=0.
Rank=652 done reading total 56862697472 bytes from needed files.
Rank=653 done Reading 56862697472 bytes from file /datasets/ogbn_papers100M/wgb/paper/node_feat_1024x.d/68.bin size=56862697472, starting from offset=0.
Rank=653 done reading total 56862697472 bytes from needed files.
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:OGBNPapers100MDataset:created x wg embedding
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
INFO:PyGCuGraphTrainer:getting output features...
[2024-02-18 20:48:08,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004752 closing signal SIGTERM
[2024-02-18 20:48:08,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004753 closing signal SIGTERM
[2024-02-18 20:48:08,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004754 closing signal SIGTERM
[2024-02-18 20:48:08,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004755 closing signal SIGTERM
[2024-02-18 20:48:08,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004756 closing signal SIGTERM
[2024-02-18 20:48:08,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004757 closing signal SIGTERM
[2024-02-18 20:48:08,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3004759 closing signal SIGTERM
[2024-02-18 20:48:13,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610902 closing signal SIGTERM
[2024-02-18 20:48:13,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610903 closing signal SIGTERM
[2024-02-18 20:48:13,251] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610904 closing signal SIGTERM
[2024-02-18 20:48:13,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610905 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610906 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610908 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610909 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396981 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396982 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396983 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396985 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396986 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396987 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396988 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290749 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290750 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290751 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290752 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290753 closing signal SIGTERM
[2024-02-18 20:48:13,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290754 closing signal SIGTERM
[2024-02-18 20:48:13,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290756 closing signal SIGTERM
[2024-02-18 20:48:13,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385871 closing signal SIGTERM
[2024-02-18 20:48:13,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385872 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385873 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385874 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385875 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385877 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385878 closing signal SIGTERM
[2024-02-18 20:48:13,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895570 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895571 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895572 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895573 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895575 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895576 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895577 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541468 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541469 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541470 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541472 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541473 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541474 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541475 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691996 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691998 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691999 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692000 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692001 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692002 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692003 closing signal SIGTERM
[2024-02-18 20:48:13,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878950 closing signal SIGTERM
[2024-02-18 20:48:13,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878951 closing signal SIGTERM
[2024-02-18 20:48:13,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878953 closing signal SIGTERM
[2024-02-18 20:48:13,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878954 closing signal SIGTERM
[2024-02-18 20:48:13,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878955 closing signal SIGTERM
[2024-02-18 20:48:13,208] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878956 closing signal SIGTERM
[2024-02-18 20:48:13,208] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878957 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710864 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710865 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710866 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710867 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710868 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710869 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710870 closing signal SIGTERM
[2024-02-18 20:48:13,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438934 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438935 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438936 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438937 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438939 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438940 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438941 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290290 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290291 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290292 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290293 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290295 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290296 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290297 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755827 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755828 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755829 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755830 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755831 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755832 closing signal SIGTERM
[2024-02-18 20:48:13,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755833 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768658 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768659 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768660 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768661 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768662 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768663 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768664 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561513 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561514 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561515 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561516 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561517 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561518 closing signal SIGTERM
[2024-02-18 20:48:13,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561520 closing signal SIGTERM
[2024-02-18 20:48:13,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075730 closing signal SIGTERM
[2024-02-18 20:48:13,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075732 closing signal SIGTERM
[2024-02-18 20:48:13,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075733 closing signal SIGTERM
[2024-02-18 20:48:13,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075734 closing signal SIGTERM
[2024-02-18 20:48:13,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075735 closing signal SIGTERM
[2024-02-18 20:48:13,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075736 closing signal SIGTERM
[2024-02-18 20:48:13,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075737 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626141 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626142 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626143 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626144 closing signal SIGTERM
[2024-02-18 20:48:13,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626145 closing signal SIGTERM
[2024-02-18 20:48:13,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626146 closing signal SIGTERM
[2024-02-18 20:48:13,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626148 closing signal SIGTERM
[2024-02-18 20:48:13,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905674 closing signal SIGTERM
[2024-02-18 20:48:13,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905675 closing signal SIGTERM
[2024-02-18 20:48:13,250] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905677 closing signal SIGTERM
[2024-02-18 20:48:13,251] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905678 closing signal SIGTERM
[2024-02-18 20:48:13,251] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905679 closing signal SIGTERM
[2024-02-18 20:48:13,251] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905680 closing signal SIGTERM
[2024-02-18 20:48:13,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905681 closing signal SIGTERM
[2024-02-18 20:48:13,213] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629449 closing signal SIGTERM
[2024-02-18 20:48:13,213] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629450 closing signal SIGTERM
[2024-02-18 20:48:13,214] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629451 closing signal SIGTERM
[2024-02-18 20:48:13,214] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629452 closing signal SIGTERM
[2024-02-18 20:48:13,214] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629453 closing signal SIGTERM
[2024-02-18 20:48:13,215] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629454 closing signal SIGTERM
[2024-02-18 20:48:13,215] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629455 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650293 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650295 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650296 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650297 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650298 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650299 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650300 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916059 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916060 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916061 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916062 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916063 closing signal SIGTERM
[2024-02-18 20:48:13,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916065 closing signal SIGTERM
[2024-02-18 20:48:13,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916066 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773095 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773096 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773097 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773098 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773099 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773100 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773101 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654720 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654721 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654723 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654724 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654725 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654726 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654727 closing signal SIGTERM
[2024-02-18 20:48:13,971] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674359 closing signal SIGTERM
[2024-02-18 20:48:13,971] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674360 closing signal SIGTERM
[2024-02-18 20:48:13,971] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674361 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674362 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674363 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674364 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674365 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902780 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902781 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902782 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902783 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902784 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902786 closing signal SIGTERM
[2024-02-18 20:48:13,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902787 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636678 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636679 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636680 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636681 closing signal SIGTERM
[2024-02-18 20:48:13,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636682 closing signal SIGTERM
[2024-02-18 20:48:13,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636683 closing signal SIGTERM
[2024-02-18 20:48:13,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636684 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164700 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164701 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164702 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164703 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164704 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164705 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164706 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277353 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277355 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277356 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277357 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277358 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277359 closing signal SIGTERM
[2024-02-18 20:48:13,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277360 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898513 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898514 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898515 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898517 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898518 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898519 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898520 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913661 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913662 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913663 closing signal SIGTERM
[2024-02-18 20:48:13,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913664 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913665 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913666 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913667 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185486 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185487 closing signal SIGTERM
[2024-02-18 20:48:13,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185488 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185489 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185490 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185492 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185493 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247177 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247178 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247179 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247180 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247181 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247182 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247184 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476947 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476948 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476949 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476950 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476951 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476952 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476953 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715871 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715872 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715873 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715874 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715876 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715877 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715878 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194664 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194665 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194666 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194667 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194669 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194670 closing signal SIGTERM
[2024-02-18 20:48:13,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194671 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745181 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745182 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745183 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745184 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745186 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745187 closing signal SIGTERM
[2024-02-18 20:48:13,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745188 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727883 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727884 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727885 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727886 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727888 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727889 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727890 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852129 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852130 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852131 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852132 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852133 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852134 closing signal SIGTERM
[2024-02-18 20:48:13,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852135 closing signal SIGTERM
[2024-02-18 20:48:13,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706767 closing signal SIGTERM
[2024-02-18 20:48:13,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706768 closing signal SIGTERM
[2024-02-18 20:48:13,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706769 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706770 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706771 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706772 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706774 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796658 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796659 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796660 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796662 closing signal SIGTERM
[2024-02-18 20:48:13,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796663 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796664 closing signal SIGTERM
[2024-02-18 20:48:13,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796665 closing signal SIGTERM
[2024-02-18 20:48:13,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822153 closing signal SIGTERM
[2024-02-18 20:48:13,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822154 closing signal SIGTERM
[2024-02-18 20:48:13,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822155 closing signal SIGTERM
[2024-02-18 20:48:13,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822156 closing signal SIGTERM
[2024-02-18 20:48:13,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822157 closing signal SIGTERM
[2024-02-18 20:48:13,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822158 closing signal SIGTERM
[2024-02-18 20:48:13,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822159 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214445 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214446 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214447 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214448 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214449 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214450 closing signal SIGTERM
[2024-02-18 20:48:13,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214452 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013752 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013754 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013755 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013756 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013757 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013758 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013759 closing signal SIGTERM
[2024-02-18 20:48:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867732 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867733 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867734 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867735 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867736 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867737 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867738 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456750 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456751 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456752 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456754 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456755 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456756 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456757 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824745 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824746 closing signal SIGTERM
[2024-02-18 20:48:13,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824747 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824749 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824750 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824751 closing signal SIGTERM
[2024-02-18 20:48:13,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824752 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922524 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922525 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922526 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922527 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922528 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922529 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922531 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072165 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072167 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072168 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072169 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072170 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072171 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072172 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701941 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701942 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701943 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701944 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701945 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701947 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701948 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605464 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605465 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605466 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605467 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605469 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605470 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605471 closing signal SIGTERM
[2024-02-18 20:48:14,212] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760369 closing signal SIGTERM
[2024-02-18 20:48:14,212] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760370 closing signal SIGTERM
[2024-02-18 20:48:14,212] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760371 closing signal SIGTERM
[2024-02-18 20:48:14,212] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760372 closing signal SIGTERM
[2024-02-18 20:48:14,212] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760373 closing signal SIGTERM
[2024-02-18 20:48:14,213] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760374 closing signal SIGTERM
[2024-02-18 20:48:14,213] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760375 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155423 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155424 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155425 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155426 closing signal SIGTERM
[2024-02-18 20:48:13,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155427 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155428 closing signal SIGTERM
[2024-02-18 20:48:13,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155429 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057873 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057874 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057875 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057876 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057877 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057878 closing signal SIGTERM
[2024-02-18 20:48:13,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057879 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667020 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667021 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667022 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667023 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667024 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667025 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667027 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268770 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268771 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268772 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268773 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268774 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268775 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268776 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754218 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754219 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754220 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754221 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754222 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754223 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754224 closing signal SIGTERM
[2024-02-18 20:48:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042106 closing signal SIGTERM
[2024-02-18 20:48:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042107 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042108 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042109 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042111 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042112 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042113 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897628 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897629 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897630 closing signal SIGTERM
[2024-02-18 20:48:13,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897632 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897633 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897634 closing signal SIGTERM
[2024-02-18 20:48:13,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897635 closing signal SIGTERM
[2024-02-18 20:48:14,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219822 closing signal SIGTERM
[2024-02-18 20:48:14,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219823 closing signal SIGTERM
[2024-02-18 20:48:14,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219824 closing signal SIGTERM
[2024-02-18 20:48:14,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219825 closing signal SIGTERM
[2024-02-18 20:48:14,228] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219827 closing signal SIGTERM
[2024-02-18 20:48:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219828 closing signal SIGTERM
[2024-02-18 20:48:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219829 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605781 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605782 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605783 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605784 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605786 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605787 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605788 closing signal SIGTERM
[2024-02-18 20:48:14,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893217 closing signal SIGTERM
[2024-02-18 20:48:14,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893218 closing signal SIGTERM
[2024-02-18 20:48:14,801] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893219 closing signal SIGTERM
[2024-02-18 20:48:14,802] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893221 closing signal SIGTERM
[2024-02-18 20:48:14,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893222 closing signal SIGTERM
[2024-02-18 20:48:14,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893223 closing signal SIGTERM
[2024-02-18 20:48:14,804] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893224 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686682 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686683 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686684 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686685 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686686 closing signal SIGTERM
[2024-02-18 20:48:14,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686687 closing signal SIGTERM
[2024-02-18 20:48:14,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686688 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916003 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916004 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916005 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916007 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916008 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916009 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916010 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782030 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782031 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782032 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782033 closing signal SIGTERM
[2024-02-18 20:48:18,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782035 closing signal SIGTERM
[2024-02-18 20:48:18,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782036 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782037 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661263 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661264 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661265 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661267 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661268 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661269 closing signal SIGTERM
[2024-02-18 20:48:14,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661270 closing signal SIGTERM
[2024-02-18 20:48:18,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154698 closing signal SIGTERM
[2024-02-18 20:48:18,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154699 closing signal SIGTERM
[2024-02-18 20:48:18,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154700 closing signal SIGTERM
[2024-02-18 20:48:18,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154701 closing signal SIGTERM
[2024-02-18 20:48:18,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154702 closing signal SIGTERM
[2024-02-18 20:48:18,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619804 closing signal SIGTERM
[2024-02-18 20:48:18,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619805 closing signal SIGTERM
[2024-02-18 20:48:18,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619806 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619807 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619808 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619811 closing signal SIGTERM
[2024-02-18 20:48:18,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677918 closing signal SIGTERM
[2024-02-18 20:48:18,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677919 closing signal SIGTERM
[2024-02-18 20:48:18,242] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677920 closing signal SIGTERM
[2024-02-18 20:48:18,243] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677921 closing signal SIGTERM
[2024-02-18 20:48:18,244] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677922 closing signal SIGTERM
[2024-02-18 20:48:18,245] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677923 closing signal SIGTERM
[2024-02-18 20:48:18,245] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677924 closing signal SIGTERM
[2024-02-18 20:48:18,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150073 closing signal SIGTERM
[2024-02-18 20:48:18,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150074 closing signal SIGTERM
[2024-02-18 20:48:18,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150075 closing signal SIGTERM
[2024-02-18 20:48:18,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150076 closing signal SIGTERM
[2024-02-18 20:48:18,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150077 closing signal SIGTERM
[2024-02-18 20:48:18,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150078 closing signal SIGTERM
[2024-02-18 20:48:18,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150080 closing signal SIGTERM
[2024-02-18 20:48:18,257] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706390 closing signal SIGTERM
[2024-02-18 20:48:18,257] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706391 closing signal SIGTERM
[2024-02-18 20:48:18,258] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706392 closing signal SIGTERM
[2024-02-18 20:48:18,258] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706393 closing signal SIGTERM
[2024-02-18 20:48:18,259] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706394 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597619 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597621 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597623 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597624 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597625 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597626 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597628 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272128 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272129 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272130 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272132 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272133 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272134 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272135 closing signal SIGTERM
[2024-02-18 20:48:14,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720892 closing signal SIGTERM
[2024-02-18 20:48:14,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720893 closing signal SIGTERM
[2024-02-18 20:48:14,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720894 closing signal SIGTERM
[2024-02-18 20:48:14,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720895 closing signal SIGTERM
[2024-02-18 20:48:14,799] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720896 closing signal SIGTERM
[2024-02-18 20:48:14,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720897 closing signal SIGTERM
[2024-02-18 20:48:14,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720898 closing signal SIGTERM
[2024-02-18 20:48:14,794] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795859 closing signal SIGTERM
[2024-02-18 20:48:14,794] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795860 closing signal SIGTERM
[2024-02-18 20:48:14,795] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795861 closing signal SIGTERM
[2024-02-18 20:48:14,795] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795862 closing signal SIGTERM
[2024-02-18 20:48:14,795] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795863 closing signal SIGTERM
[2024-02-18 20:48:14,796] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795864 closing signal SIGTERM
[2024-02-18 20:48:14,797] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795866 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495456 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495457 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495458 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495459 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495460 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495461 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495462 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868057 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868058 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868059 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868060 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868061 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868062 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868064 closing signal SIGTERM
[2024-02-18 20:48:18,224] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164196 closing signal SIGTERM
[2024-02-18 20:48:18,224] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164197 closing signal SIGTERM
[2024-02-18 20:48:18,224] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164198 closing signal SIGTERM
[2024-02-18 20:48:18,225] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164199 closing signal SIGTERM
[2024-02-18 20:48:18,225] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164200 closing signal SIGTERM
[2024-02-18 20:48:18,226] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164201 closing signal SIGTERM
[2024-02-18 20:48:18,226] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164202 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041522 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041523 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041524 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041525 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041526 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041527 closing signal SIGTERM
[2024-02-18 20:48:18,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041528 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170968 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170970 closing signal SIGTERM
[2024-02-18 20:48:18,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170971 closing signal SIGTERM
[2024-02-18 20:48:18,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170972 closing signal SIGTERM
[2024-02-18 20:48:18,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170973 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170974 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170975 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362861 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362862 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362863 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362865 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362866 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362867 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362868 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387114 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387115 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387116 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387117 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387119 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387120 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387121 closing signal SIGTERM
[2024-02-18 20:48:18,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250545 closing signal SIGTERM
[2024-02-18 20:48:18,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250546 closing signal SIGTERM
[2024-02-18 20:48:18,277] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250547 closing signal SIGTERM
[2024-02-18 20:48:18,278] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250548 closing signal SIGTERM
[2024-02-18 20:48:18,278] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250549 closing signal SIGTERM
[2024-02-18 20:48:18,279] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250551 closing signal SIGTERM
[2024-02-18 20:48:18,279] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250552 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581812 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581813 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581815 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581816 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581817 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581818 closing signal SIGTERM
[2024-02-18 20:48:18,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581819 closing signal SIGTERM
[2024-02-18 20:48:18,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369310 closing signal SIGTERM
[2024-02-18 20:48:18,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369312 closing signal SIGTERM
[2024-02-18 20:48:18,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369313 closing signal SIGTERM
[2024-02-18 20:48:18,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369314 closing signal SIGTERM
[2024-02-18 20:48:18,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369315 closing signal SIGTERM
[2024-02-18 20:48:18,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369316 closing signal SIGTERM
[2024-02-18 20:48:18,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369317 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781103 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781104 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781106 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781107 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781109 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781110 closing signal SIGTERM
[2024-02-18 20:48:14,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781111 closing signal SIGTERM
[2024-02-18 20:48:18,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860029 closing signal SIGTERM
[2024-02-18 20:48:18,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860030 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860031 closing signal SIGTERM
[2024-02-18 20:48:18,275] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860032 closing signal SIGTERM
[2024-02-18 20:48:18,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860033 closing signal SIGTERM
[2024-02-18 20:48:18,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860035 closing signal SIGTERM
[2024-02-18 20:48:18,276] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860036 closing signal SIGTERM
[2024-02-18 20:48:14,766] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298986 closing signal SIGTERM
[2024-02-18 20:48:14,766] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298987 closing signal SIGTERM
[2024-02-18 20:48:14,766] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298988 closing signal SIGTERM
[2024-02-18 20:48:14,766] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298989 closing signal SIGTERM
[2024-02-18 20:48:14,766] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298990 closing signal SIGTERM
[2024-02-18 20:48:14,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298991 closing signal SIGTERM
[2024-02-18 20:48:14,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298993 closing signal SIGTERM
[2024-02-18 20:48:18,205] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850316 closing signal SIGTERM
[2024-02-18 20:48:18,205] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850317 closing signal SIGTERM
[2024-02-18 20:48:18,206] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850318 closing signal SIGTERM
[2024-02-18 20:48:18,206] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850320 closing signal SIGTERM
[2024-02-18 20:48:18,207] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850321 closing signal SIGTERM
[2024-02-18 20:48:18,208] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850322 closing signal SIGTERM
[2024-02-18 20:48:18,208] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850323 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301637 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301638 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301639 closing signal SIGTERM
[2024-02-18 20:48:18,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301640 closing signal SIGTERM
[2024-02-18 20:48:18,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301641 closing signal SIGTERM
[2024-02-18 20:48:18,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301643 closing signal SIGTERM
[2024-02-18 20:48:18,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301644 closing signal SIGTERM
[2024-02-18 20:48:18,243] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978211 closing signal SIGTERM
[2024-02-18 20:48:18,243] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978212 closing signal SIGTERM
[2024-02-18 20:48:18,244] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978213 closing signal SIGTERM
[2024-02-18 20:48:18,245] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978214 closing signal SIGTERM
[2024-02-18 20:48:18,246] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978215 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525180 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525181 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525182 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525183 closing signal SIGTERM
[2024-02-18 20:48:13,972] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525184 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525185 closing signal SIGTERM
[2024-02-18 20:48:13,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525186 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420621 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420622 closing signal SIGTERM
[2024-02-18 20:48:14,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420623 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420624 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420626 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420627 closing signal SIGTERM
[2024-02-18 20:48:14,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420628 closing signal SIGTERM
[2024-02-18 20:48:18,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154703 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619812 closing signal SIGTERM
[2024-02-18 20:48:18,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706395 closing signal SIGTERM
[2024-02-18 20:48:18,247] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978216 closing signal SIGTERM
[2024-02-18 20:48:18,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154704 closing signal SIGTERM
[2024-02-18 20:48:18,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706396 closing signal SIGTERM
[2024-02-18 20:48:18,247] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978217 closing signal SIGTERM
[2024-02-18 20:48:18,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381118 closing signal SIGTERM
[2024-02-18 20:48:18,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381119 closing signal SIGTERM
[2024-02-18 20:48:18,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381120 closing signal SIGTERM
[2024-02-18 20:48:18,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381121 closing signal SIGTERM
[2024-02-18 20:48:18,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381122 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381123 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381124 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330600 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330602 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330603 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330604 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330605 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330606 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330607 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269820 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269821 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269822 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269823 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269824 closing signal SIGTERM
[2024-02-18 20:48:18,982] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269825 closing signal SIGTERM
[2024-02-18 20:48:18,982] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269827 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577125 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577126 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577127 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577128 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577129 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577130 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577132 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207848 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207849 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207850 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207851 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207852 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207853 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207855 closing signal SIGTERM
[2024-02-18 20:48:18,244] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380176 closing signal SIGTERM
[2024-02-18 20:48:18,244] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380177 closing signal SIGTERM
[2024-02-18 20:48:18,244] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380178 closing signal SIGTERM
[2024-02-18 20:48:18,245] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380179 closing signal SIGTERM
[2024-02-18 20:48:18,245] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380181 closing signal SIGTERM
[2024-02-18 20:48:18,246] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380182 closing signal SIGTERM
[2024-02-18 20:48:18,246] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380183 closing signal SIGTERM
[2024-02-18 20:48:18,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676251 closing signal SIGTERM
[2024-02-18 20:48:18,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676252 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676253 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676254 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676256 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676257 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676258 closing signal SIGTERM
[2024-02-18 20:48:18,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213362 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213364 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213365 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213366 closing signal SIGTERM
[2024-02-18 20:48:18,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213367 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213368 closing signal SIGTERM
[2024-02-18 20:48:18,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213369 closing signal SIGTERM
[2024-02-18 20:48:18,579] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124412 closing signal SIGTERM
[2024-02-18 20:48:18,579] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124414 closing signal SIGTERM
[2024-02-18 20:48:18,580] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124415 closing signal SIGTERM
[2024-02-18 20:48:18,580] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124416 closing signal SIGTERM
[2024-02-18 20:48:18,580] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124417 closing signal SIGTERM
[2024-02-18 20:48:18,581] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124418 closing signal SIGTERM
[2024-02-18 20:48:18,581] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124419 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594095 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594096 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594097 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594099 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594100 closing signal SIGTERM
[2024-02-18 20:48:18,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594101 closing signal SIGTERM
[2024-02-18 20:48:18,275] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594102 closing signal SIGTERM
[2024-02-18 20:48:18,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859141 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859143 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859144 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859145 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859146 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859147 closing signal SIGTERM
[2024-02-18 20:48:18,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859148 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812114 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812116 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812117 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812118 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812119 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812120 closing signal SIGTERM
[2024-02-18 20:48:18,982] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812121 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912788 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912789 closing signal SIGTERM
[2024-02-18 20:48:19,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912790 closing signal SIGTERM
[2024-02-18 20:48:19,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912791 closing signal SIGTERM
[2024-02-18 20:48:19,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912792 closing signal SIGTERM
[2024-02-18 20:48:19,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912793 closing signal SIGTERM
[2024-02-18 20:48:19,242] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912795 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986926 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986927 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986928 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986929 closing signal SIGTERM
[2024-02-18 20:48:19,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986930 closing signal SIGTERM
[2024-02-18 20:48:19,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986931 closing signal SIGTERM
[2024-02-18 20:48:19,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986932 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782048 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782049 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782050 closing signal SIGTERM
[2024-02-18 20:48:19,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782051 closing signal SIGTERM
[2024-02-18 20:48:19,240] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782052 closing signal SIGTERM
[2024-02-18 20:48:19,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782054 closing signal SIGTERM
[2024-02-18 20:48:19,242] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782055 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737237 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737238 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737239 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737240 closing signal SIGTERM
[2024-02-18 20:48:18,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737241 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737243 closing signal SIGTERM
[2024-02-18 20:48:18,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737244 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844389 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844390 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844391 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844392 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844393 closing signal SIGTERM
[2024-02-18 20:48:18,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844394 closing signal SIGTERM
[2024-02-18 20:48:18,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844395 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573135 closing signal SIGTERM
[2024-02-18 20:48:18,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573136 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573137 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573138 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573139 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573141 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573142 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787786 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787787 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787788 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787789 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787790 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787791 closing signal SIGTERM
[2024-02-18 20:48:18,269] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787792 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573063 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573064 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573065 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573066 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573067 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573068 closing signal SIGTERM
[2024-02-18 20:48:18,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573070 closing signal SIGTERM
[2024-02-18 20:48:18,969] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303970 closing signal SIGTERM
[2024-02-18 20:48:18,969] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303971 closing signal SIGTERM
[2024-02-18 20:48:18,969] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303972 closing signal SIGTERM
[2024-02-18 20:48:18,969] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303973 closing signal SIGTERM
[2024-02-18 20:48:18,970] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303974 closing signal SIGTERM
[2024-02-18 20:48:18,970] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303975 closing signal SIGTERM
[2024-02-18 20:48:18,970] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303977 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903418 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903419 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903420 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903421 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903423 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903424 closing signal SIGTERM
[2024-02-18 20:48:19,238] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903425 closing signal SIGTERM
[2024-02-18 20:48:18,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223715 closing signal SIGTERM
[2024-02-18 20:48:18,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223716 closing signal SIGTERM
[2024-02-18 20:48:18,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223717 closing signal SIGTERM
[2024-02-18 20:48:18,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223718 closing signal SIGTERM
[2024-02-18 20:48:18,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223720 closing signal SIGTERM
[2024-02-18 20:48:18,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223721 closing signal SIGTERM
[2024-02-18 20:48:18,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223722 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026519 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026520 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026521 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026522 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026523 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026524 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026526 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539762 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539763 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539765 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539766 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539767 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539768 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539769 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497628 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497629 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497630 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497631 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497633 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497634 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497635 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094899 closing signal SIGTERM
[2024-02-18 20:48:18,978] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094900 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094901 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094902 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094903 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094904 closing signal SIGTERM
[2024-02-18 20:48:18,979] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094905 closing signal SIGTERM
[2024-02-18 20:48:19,235] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885247 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885248 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885250 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885251 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885252 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885253 closing signal SIGTERM
[2024-02-18 20:48:19,237] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885254 closing signal SIGTERM
[2024-02-18 20:48:20,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912890 closing signal SIGTERM
[2024-02-18 20:48:20,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912891 closing signal SIGTERM
[2024-02-18 20:48:20,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912892 closing signal SIGTERM
[2024-02-18 20:48:20,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912893 closing signal SIGTERM
[2024-02-18 20:48:20,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912894 closing signal SIGTERM
[2024-02-18 20:48:20,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912896 closing signal SIGTERM
[2024-02-18 20:48:20,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912897 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066278 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066280 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066281 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066282 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066283 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066284 closing signal SIGTERM
[2024-02-18 20:48:19,236] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066285 closing signal SIGTERM
[2024-02-18 20:48:19,769] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301918 closing signal SIGTERM
[2024-02-18 20:48:19,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301919 closing signal SIGTERM
[2024-02-18 20:48:19,770] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301920 closing signal SIGTERM
[2024-02-18 20:48:19,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301921 closing signal SIGTERM
[2024-02-18 20:48:19,771] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301922 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301924 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301925 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216672 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216673 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216674 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216675 closing signal SIGTERM
[2024-02-18 20:48:19,772] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216676 closing signal SIGTERM
[2024-02-18 20:48:19,773] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216677 closing signal SIGTERM
[2024-02-18 20:48:19,773] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216679 closing signal SIGTERM
[2024-02-18 20:48:19,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075319 closing signal SIGTERM
[2024-02-18 20:48:19,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075320 closing signal SIGTERM
[2024-02-18 20:48:19,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075321 closing signal SIGTERM
[2024-02-18 20:48:19,803] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075322 closing signal SIGTERM
[2024-02-18 20:48:19,804] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075324 closing signal SIGTERM
[2024-02-18 20:48:19,804] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075325 closing signal SIGTERM
[2024-02-18 20:48:19,804] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075326 closing signal SIGTERM
[2024-02-18 20:48:23,715] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614816 closing signal SIGTERM
[2024-02-18 20:48:23,715] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614817 closing signal SIGTERM
[2024-02-18 20:48:23,716] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614818 closing signal SIGTERM
[2024-02-18 20:48:23,717] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614819 closing signal SIGTERM
[2024-02-18 20:48:23,717] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614820 closing signal SIGTERM
[2024-02-18 20:48:23,717] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614821 closing signal SIGTERM
[2024-02-18 20:48:23,718] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614822 closing signal SIGTERM
[2024-02-18 20:48:23,647] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692333 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692334 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692335 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692336 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692337 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692338 closing signal SIGTERM
[2024-02-18 20:48:23,648] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692339 closing signal SIGTERM
[2024-02-18 20:48:23,983] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467544 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467545 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467546 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467547 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467548 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467550 closing signal SIGTERM
[2024-02-18 20:48:23,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467551 closing signal SIGTERM
[2024-02-18 20:48:38,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3004752 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:42,595] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3004753 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:42,826] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3004755 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:42,829] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3004756 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,208] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1878954 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,215] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2629449 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3075734 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,239] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2822153 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,252] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2905674 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3706767 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2824745 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2610902 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,255] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3561513 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,256] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2541468 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,257] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1852129 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2654721 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2456750 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2727883 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2013752 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,261] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2897628 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2773095 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2710864 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,262] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3385871 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2214445 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,263] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2194665 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1768659 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2476947 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,264] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3057873 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2898513 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2715871 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2650295 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2691996 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3290291 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,265] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3247177 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2164701 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,266] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2796658 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,267] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1755827 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3290749 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,268] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3438934 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,270] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2916059 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,271] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2277353 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1895570 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,272] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2913661 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2396981 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,273] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3185486 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2745182 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,274] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2626141 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2701941 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2525180 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2605781 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2667020 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2495456 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2754219 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,973] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2605464 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3268771 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,974] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3072167 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2922524 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2674360 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,975] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2155423 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,976] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2902780 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:43,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2636678 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,213] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2760369 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2219822 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,230] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2042106 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2916003 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,231] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2420621 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1868059 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,232] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2781104 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,233] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2867732 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,241] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3686683 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,654] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2661263 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,767] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2298986 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,797] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2795859 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,800] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2720892 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:44,804] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2893218 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,210] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2396982 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,289] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2396986 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,486] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3185487 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,491] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3185488 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,510] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1755829 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,513] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 3185489 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:46,758] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -9) local_rank: 6 (pid: 3004758) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
/scripts/bench_cugraph_training.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-18_20:48:08
  host      : eos0182.eos.clusters.nvidia.com
  rank      : 142 (local_rank: 6)
  exitcode  : -9 (pid: 3004758)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 3004758
========================================================
srun: error: eos0182: task 17: Exited with exit code 1
srun: Terminating StepId=333377.1
slurmstepd: error: *** STEP 333377.1 ON eos0161 CANCELLED AT 2024-02-18T20:48:47 ***
[2024-02-18 20:48:47,033] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124412 closing signal SIGTERM
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124414 closing signal SIGTERM
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124415 closing signal SIGTERM
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124417 closing signal SIGTERM
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124418 closing signal SIGTERM
[2024-02-18 20:48:47,033] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3124419 closing signal SIGTERM
[2024-02-18 20:48:47,039] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 2476951 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913661 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290291 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290292 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290293 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290295 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913663 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913664 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290296 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913665 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290297 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913666 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2913667 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057873 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057875 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057876 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219822 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219824 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057877 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3057878 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898513 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898514 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978211 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978212 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978213 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610902 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610903 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610904 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610905 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898517 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978214 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219825 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898518 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978216 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2219828 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898519 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2978217 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2898520 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610906 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610908 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2610909 closing signal SIGTERM
[2024-02-18 20:48:47,043] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362861 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362862 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362863 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362865 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362866 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3362867 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782031 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782032 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013752 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013754 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013755 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013756 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013757 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2013759 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782035 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782036 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893218 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782037 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737237 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737238 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737239 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737240 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737243 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893219 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893221 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893222 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2893223 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268771 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268772 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686683 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686684 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912890 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912891 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912893 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686685 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2737244 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686686 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577125 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577126 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686687 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3686688 closing signal SIGTERM
[2024-02-18 20:48:47,044] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3268775 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912894 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912896 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912897 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438934 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438935 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438936 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577127 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075734 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075735 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075736 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577128 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577130 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3577132 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878954 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438937 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075737 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438939 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438940 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3438941 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1878957 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561513 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561514 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561515 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561517 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561518 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3561520 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247177 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247179 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247180 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247181 closing signal SIGTERM
[2024-02-18 20:48:47,045] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3247182 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745182 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745183 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745184 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745186 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745187 closing signal SIGTERM
[2024-02-18 20:48:47,046] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2745188 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654721 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654723 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654724 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654725 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654726 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2654727 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223715 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223716 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223717 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903418 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903419 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903420 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223718 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223720 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2223721 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216672 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216673 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903421 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903423 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2903425 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216674 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3216675 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,047] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922524 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922525 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922526 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922527 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075319 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075320 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692334 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692335 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922528 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2922529 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692337 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692338 closing signal SIGTERM
[2024-02-18 20:48:47,047] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150073 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150075 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692339 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075322 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075324 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3075325 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150076 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150077 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3150078 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154698 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154699 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154700 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154701 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154702 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154703 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2154704 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706390 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706391 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706393 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706394 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706395 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2706396 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272128 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272129 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691996 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691998 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2691999 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692001 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272130 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272132 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207848 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207849 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207850 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207851 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3272135 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912788 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2692002 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912789 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905674 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905677 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905678 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912790 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912791 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912792 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207852 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207853 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2912793 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3207855 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269820 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269821 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269822 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619804 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667020 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667021 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667022 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667023 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619805 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619807 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619808 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597621 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597623 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597624 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269823 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2619811 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269824 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2269827 closing signal SIGTERM
[2024-02-18 20:48:47,048] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844389 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844390 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844391 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905679 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905680 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667024 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2905681 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2667025 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214445 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597625 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214447 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597626 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214448 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2597628 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214449 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2214450 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525180 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525181 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525182 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525183 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155423 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155424 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155425 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525184 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2525185 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301918 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301919 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301920 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301921 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614816 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194665 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194666 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194667 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614817 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844392 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754219 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754220 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754221 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330600 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614819 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844393 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2844394 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754222 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629449 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629450 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629451 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629452 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330602 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859141 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859143 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859145 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614820 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330604 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614821 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754223 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2754224 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330605 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2614822 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330606 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2330607 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194669 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301922 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916003 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916004 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916005 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916007 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2194670 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301924 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636678 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897628 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897632 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897633 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3185489 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605781 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605783 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636679 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859146 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636680 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859147 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636681 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2859148 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164701 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164702 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636682 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916008 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164704 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2636684 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916009 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164705 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916010 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155426 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727883 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727884 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727885 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2897634 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710864 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710865 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710866 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710867 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710868 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155427 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155428 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629453 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626141 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626142 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626143 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626144 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626146 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2155429 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2629455 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2626148 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380177 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380178 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380179 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605784 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2710870 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867732 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867733 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867735 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380181 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2380183 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727886 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385871 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385873 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385874 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715871 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2727888 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715873 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715874 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385875 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715876 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720892 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715877 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720893 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1755829 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385877 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3385878 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2715878 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720895 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720896 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720897 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2720898 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605464 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605466 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605467 closing signal SIGTERM
[2024-02-18 20:48:47,049] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867736 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867737 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2867738 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605469 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026519 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605470 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026520 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2605471 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026521 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170968 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170970 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170971 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026522 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026523 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573063 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573064 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026524 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3026526 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573066 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573067 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298986 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298987 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298988 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298989 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573070 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298990 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677918 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650295 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650296 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650297 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677919 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170972 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2298993 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677920 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170974 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250545 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250546 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250547 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677921 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2170975 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677923 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250548 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2677924 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250551 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795859 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795860 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2250552 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650298 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650299 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2650300 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387114 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387115 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676251 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676252 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676253 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795862 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795863 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2795866 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387116 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387117 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676254 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387119 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676256 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2387120 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2676258 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467544 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467545 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467546 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301637 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301638 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301639 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381118 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381120 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381121 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381122 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2381123 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467548 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768659 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467550 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768660 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 467551 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768661 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301640 closing signal SIGTERM
[2024-02-18 20:48:47,050] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495456 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495457 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495458 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768662 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541468 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541469 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541470 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301643 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3301644 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495459 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495461 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916059 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1768664 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2495462 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916060 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2916061 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860029 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860030 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661263 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661265 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661267 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860031 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868059 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868060 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868062 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860032 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860033 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2396986 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760369 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760370 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760371 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902780 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902781 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902782 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852129 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852130 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852131 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2860036 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541472 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042106 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760373 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541473 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042107 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760374 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541474 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042108 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2760375 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2541475 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042109 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042111 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042112 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661268 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594095 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594097 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573135 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573137 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902784 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852132 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787787 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787789 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787790 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2042113 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661269 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902786 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852133 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2661270 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2902787 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852134 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277353 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277355 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277356 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1868064 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1852135 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787791 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2787792 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420621 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420622 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420623 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420624 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164197 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164198 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277357 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164199 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277359 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164200 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2277360 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594099 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573139 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164201 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594100 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573141 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2164202 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895570 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895571 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594101 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2573142 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2594102 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420626 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066278 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066281 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2420628 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885248 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885251 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066282 closing signal SIGTERM
[2024-02-18 20:48:47,051] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066283 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885252 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895573 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895575 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066284 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885253 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1895577 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3066285 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2885254 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369312 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369313 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369314 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369316 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041523 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2369317 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041524 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041525 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041526 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290749 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290750 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290751 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041527 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2041528 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701941 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701942 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701943 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2476951 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290752 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290754 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3290756 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701944 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812114 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812116 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812117 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812118 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701945 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701947 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2701948 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812119 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213364 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2812121 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213366 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213367 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213368 closing signal SIGTERM
[2024-02-18 20:48:47,052] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2213369 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094899 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094900 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094901 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094904 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094905 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674360 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674361 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674363 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674364 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2674365 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782048 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782049 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456750 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456752 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456754 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456755 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2456756 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782050 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782051 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2782052 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850316 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850317 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850318 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303970 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303971 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303972 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303973 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303974 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781104 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781106 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781107 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850320 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773095 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773096 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773097 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2303977 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850322 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2850323 closing signal SIGTERM
[2024-02-18 20:48:47,053] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706767 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706768 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781109 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781110 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773098 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773099 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706769 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2781111 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2773101 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706770 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706771 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706772 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3706774 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824745 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824747 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824749 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824750 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824751 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2824752 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822153 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822155 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822156 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822157 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2822159 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497628 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497629 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2497630 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072167 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072168 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072169 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072170 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072171 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3072172 closing signal SIGTERM
[2024-02-18 20:48:47,054] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581812 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581813 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581815 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581816 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581818 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2581819 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986926 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986927 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986929 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986930 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539765 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539766 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1986931 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539767 closing signal SIGTERM
[2024-02-18 20:48:47,055] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2539769 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796658 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796659 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796660 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796662 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796663 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796664 closing signal SIGTERM
[2024-02-18 20:48:47,056] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2796665 closing signal SIGTERM
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1754511 got signal: 15
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3184073 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2395627 got signal: 15
srun: error: eos0340: task 96: Exited with exit code 1
srun: error: eos0168: task 5: Exited with exit code 1
srun: error: eos0337: task 93: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2475566 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2743792 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1894234 got signal: 15
srun: error: eos0286: task 71: Exited with exit code 1
srun: error: eos0370: task 116: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2771712 got signal: 15
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2648900 got signal: 15
srun: error: eos0314: task 80: Exited with exit code 1
srun: error: eos0231: task 54: Exited with exit code 1
srun: error: eos0190: task 23: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2163361 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2628096 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2904278 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2726505 got signal: 15
srun: error: eos0295: task 75: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2659894 got signal: 15
srun: error: eos0208: task 38: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2752804 got signal: 15
srun: error: eos0196: task 29: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2624762 got signal: 15
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2795275 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2891785 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2540078 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3289314 got signal: 15
srun: error: eos0179: task 14: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2714443 got signal: 15
srun: error: eos0207: task 37: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2213113 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2276011 got signal: 15
srun: error: eos0184: task 19: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2419254 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2820765 got signal: 15
srun: error: eos0315: task 81: Exited with exit code 1
srun: error: eos0232: task 55: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1850816 got signal: 15
srun: error: eos0379: task 122: Exited with exit code 1
srun: error: eos0193: task 26: Exited with exit code 1
srun: error: eos0263: task 63: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2455414 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2709477 got signal: 15
srun: error: eos0287: task 72: Exited with exit code 1
srun: error: eos0215: task 44: Exited with exit code 1
srun: error: eos0329: task 87: Exited with exit code 1
srun: error: eos0228: task 52: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2901381 got signal: 15
srun: error: eos0326: task 86: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2921144 got signal: 15
srun: error: eos0331: task 89: Exited with exit code 1
srun: error: eos0209: task 39: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2912214 got signal: 15
srun: error: eos0177: task 13: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3056465 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2690612 got signal: 15
srun: error: eos0201: task 33: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3245794 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2665650 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1877603 got signal: 15
srun: error: eos0188: task 21: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2896225 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3384417 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2494068 got signal: 15
srun: error: eos0363: task 111: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1767330 got signal: 15
srun: error: eos0371: task 117: Exited with exit code 1
srun: error: eos0181: task 16: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3074315 got signal: 15
srun: error: eos0192: task 25: Exited with exit code 1
srun: error: eos0452: task 127: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3685228 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2700548 got signal: 15
srun: error: eos0378: task 121: Exited with exit code 1
srun: error: eos0303: task 78: Exited with exit code 1
srun: error: eos0199: task 31: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3560036 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2823359 got signal: 15
srun: error: eos0291: task 73: Exited with exit code 1
srun: error: eos0319: task 83: Exited with exit code 1
srun: error: eos0368: task 115: Exited with exit code 1
srun: error: eos0364: task 112: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3267346 got signal: 15
srun: error: eos0180: task 15: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2914659 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3705360 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3070772 got signal: 15
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2672993 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2604100 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2635320 got signal: 15
srun: error: eos0236: task 58: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3437488 got signal: 15
srun: error: eos0356: task 106: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2523881 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3288862 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2914601 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2193333 got signal: 15
srun: error: eos0358: task 107: Exited with exit code 1
srun: error: eos0238: task 60: Exited with exit code 1
srun: error: eos0171: task 8: Exited with exit code 1
srun: error: eos0227: task 51: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2897119 got signal: 15
srun: error: eos0237: task 59: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2604429 got signal: 15
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2794452 got signal: 15
srun: error: eos0175: task 11: Exited with exit code 1
srun: error: eos0189: task 22: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2012414 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2758953 got signal: 15
srun: error: eos0270: task 68: Exited with exit code 1
srun: error: eos0359: task 108: Exited with exit code 1
srun: error: eos0355: task 105: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2040783 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2866338 got signal: 15
srun: error: eos0195: task 28: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2609500 got signal: 15
srun: error: eos0212: task 42: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2779762 got signal: 15
srun: error: eos0367: task 114: Exited with exit code 1
srun: error: eos0176: task 12: Exited with exit code 1
srun: error: eos0351: task 103: Exited with exit code 1
srun: error: eos0264: task 64: Exited with exit code 1
srun: error: eos0321: task 84: Exited with exit code 1
srun: error: eos0377: task 120: Exited with exit code 1
srun: error: eos0330: task 88: Exited with exit code 1
srun: error: eos0194: task 27: Exited with exit code 1
srun: error: eos0256: task 62: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2154099 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2653354 got signal: 15
srun: error: eos0211: task 41: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1866746 got signal: 15
srun: error: eos0174: task 10: Exited with exit code 1
srun: error: eos0222: task 49: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3270687 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2218522 got signal: 15
srun: error: eos0350: task 102: Exited with exit code 1
srun: error: eos0162: task 1: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2297634 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 725, in _close
    handler.proc.wait()
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1959, in _wait
    (pid, sts) = self._try_wait(0)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1917, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2719519 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2379779 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2976792 got signal: 15
srun: error: eos0318: task 82: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2169641 got signal: 15
srun: error: eos0191: task 24: Exited with exit code 1
srun: error: eos0334: task 91: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2618443 got signal: 15
srun: error: eos0374: task 118: Exited with exit code 1
srun: error: eos0342: task 97: Exited with exit code 1
srun: error: eos0172: task 9: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2780614 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2212030 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2249168 got signal: 15
srun: error: eos0344: task 98: Exited with exit code 1
srun: error: eos0383: task 125: Exited with exit code 1
srun: error: eos0348: task 101: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3148665 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2705013 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3361437 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2676547 got signal: 15
srun: error: eos0183: task 18: Exited with exit code 1
srun: error: eos0164: task 2: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2162862 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2902033 got signal: 15
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2596235 got signal: 15
srun: error: eos0166: task 3: Exited with exit code 1
srun: error: eos0384: task 126: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2848927 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2674889 got signal: 15
srun: error: eos0322: task 85: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2385797 got signal: 15
srun: error: eos0187: task 20: Exited with exit code 1
srun: error: eos0204: task 35: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2378804 got signal: 15
srun: error: eos0235: task 57: Exited with exit code 1
srun: error: eos0167: task 4: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2367924 got signal: 15
srun: error: eos0271: task 69: Exited with exit code 1
srun: error: eos0278: task 70: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3025110 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2858603 got signal: 15
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2222344 got signal: 15
srun: error: eos0267: task 66: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2571736 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2592745 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2843000 got signal: 15
srun: error: eos0198: task 30: Exited with exit code 1
srun: error: eos0216: task 45: Exited with exit code 1
srun: error: eos0339: task 95: Exited with exit code 1
srun: error: eos0332: task 90: Exited with exit code 1
srun: error: eos0205: task 36: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2302594 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2040176 got signal: 15
srun: error: eos0169: task 6: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2735837 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2268503 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2786442 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2810731 got signal: 15
srun: error: eos0268: task 67: Exited with exit code 1
srun: error: eos0302: task 77: Exited with exit code 1
srun: error: eos0380: task 123: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2911389 got signal: 15
srun: error: eos0214: task 43: Exited with exit code 1
srun: error: eos0299: task 76: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2857811 got signal: 15
srun: error: eos0230: task 53: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2153377 got signal: 15
srun: error: eos0200: task 32: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2580502 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2911466 got signal: 15
srun: error: eos0170: task 7: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3575664 got signal: 15
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3093508 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3073917 got signal: 15
srun: error: eos0376: task 119: Exited with exit code 1
srun: error: eos0217: task 46: Exited with exit code 1
srun: error: eos0347: task 100: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3300212 got signal: 15
srun: error: eos0242: task 61: Exited with exit code 1
srun: error: eos0353: task 104: Exited with exit code 1
srun: error: eos0360: task 109: Exited with exit code 1
srun: error: eos0225: task 50: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2571664 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3300483 got signal: 15
srun: error: eos0294: task 74: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2883847 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3123008 got signal: 15
[2024-02-18 20:48:58,681] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0234.eos.clusters.nvidia.com_2780666_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
[2024-02-18 20:48:58,698] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0266.eos.clusters.nvidia.com_2538351_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
[2024-02-18 20:48:58,713] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0311.eos.clusters.nvidia.com_1985576_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
[2024-02-18 20:48:58,722] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0346.eos.clusters.nvidia.com_2496251_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
[2024-02-18 20:48:58,730] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0338.eos.clusters.nvidia.com_3064861_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
[2024-02-18 20:48:58,733] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0221.eos.clusters.nvidia.com_2613464_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
srun: error: eos0202: task 34: Exited with exit code 1
srun: error: eos0336: task 92: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2329263 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3064861 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1985576 got signal: 15
srun: error: eos0161: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2780666 got signal: 15
srun: error: eos0210: task 40: Exited with exit code 1
srun: error: eos0338: task 94: Exited with exit code 1
srun: error: eos0311: task 79: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2496251 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3206436 got signal: 15
srun: error: eos0234: task 56: Exited with exit code 1
srun: error: eos0362: task 110: Exited with exit code 1
srun: error: eos0346: task 99: Exited with exit code 1
[2024-02-18 20:48:59,781] torch.distributed.elastic.rendezvous.dynamic_rendezvous: [WARNING] The node 'eos0366.eos.clusters.nvidia.com_3215232_0' has failed to send a keep-alive heartbeat to the rendezvous '9503' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 3215232 got signal: 15
srun: error: eos0366: task 113: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2538351 got signal: 15
srun: error: eos0266: task 65: Exited with exit code 1
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2690959 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 466313 got signal: 15
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 255, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 736, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 878, in _invoke_run
    run_result = self._monitor_workers(self._worker_group)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 124, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 307, in _monitor_workers
    result = self._pcontext.wait(0)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 288, in wait
    return self._poll()
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 668, in _poll
    self.close()  # terminate all running procs
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 331, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 713, in _close
    handler.proc.wait(time_to_wait)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/opt/conda/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
  File "/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 2613464 got signal: 15
srun: error: eos0382: task 124: Exited with exit code 1
srun: error: eos0220: task 47: Exited with exit code 1
srun: error: eos0221: task 48: Exited with exit code 1
